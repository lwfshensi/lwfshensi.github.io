<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  <title>肆叔的酱油</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta property="og:type" content="website">
<meta property="og:title" content="肆叔的酱油">
<meta property="og:url" content="http://yoursite.com/index.html">
<meta property="og:site_name" content="肆叔的酱油">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="肆叔的酱油">
  
    <link rel="alternate" href="/atom.xml" title="肆叔的酱油" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  
  
    <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <link rel="stylesheet" href="/css/style.css">
  

</head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">肆叔的酱油</a>
      </h1>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="Search"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" results="0" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://yoursite.com"></form>
      </div>
    </div>
  </div>
</header>
      <div class="outer">
        <section id="main">
  
    <article id="post-hello-world" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2024/07/17/hello-world/" class="article-date">
  <time datetime="2024-07-17T13:13:36.139Z" itemprop="datePublished">2024-07-17</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2024/07/17/hello-world/">如何搭建新版本hexo</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>我先是在debian8.6上用官网的方式搭建的，失败～<br>然后用aptget 的方式安装nodejs 再装hexo还是失败～<br>最后我开了一个ubuntu14.04的vm，在里面执行<br>apt-get install nodejs npm<br>apt-get install nodejs-legacy<br>sudo npm install -g hexo-cli</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2024/07/17/hello-world/" data-id="clypv5bij000d6y79hhhmmvz6" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>


  
    <article id="post-Doris-begin" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2024/07/17/Doris-begin/" class="article-date">
  <time datetime="2024-07-17T11:59:10.000Z" itemprop="datePublished">2024-07-17</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2024/07/17/Doris-begin/">Doris使用入门</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>  本文着重介绍doris使用。<br>  按照数据的时效性要求，分为实时报表与准实时报表。<br>  实时报表的实现：<br>  通过apache flink实时同步mysql binlog、mongodDB oplog、kafka的数据通过Flink Doris Connector 插件把数据实时插入到doris中产生实时报表。<br>  这里简单介绍一下Flink CDC：<br>  CDC 是变更数据捕获（Change Data Capture）技术的缩写，它可以将源数据库（Source）的增量变动记录，同步到一个或多个数据目的（Sink）。在同步过程中，还可以对数据进行一定的处理，例如分组（GROUP BY）、多表的关联（JOIN）等。通常来讲，CDC 分为主动查询和事件接收两种技术实现模式。</p>
<p>  对于主动查询而言，用户通常会在数据源表的某个字段中，保存上次更新的时间戳或版本号等信息，然后下游通过不断的查询和与上次的记录做对比，来确定数据是否有变动，是否需要同步。这种方式优点是不涉及数据库底层特性，实现比较通用；缺点是要对业务表做改造，且实时性不高，不能确保跟踪到所有的变更记录，且持续的频繁查询对数据库的压力较大。<br>  事件接收模式可以通过触发器（Trigger）或者日志（例如 Transaction log、Binary log、Write-ahead log 等）来实现。当数据源表发生变动时，会通过附加在表上的触发器或者 binlog 等途径，将操作记录下来。下游可以通过数据库底层的协议，订阅并消费这些事件，然后对数据库变动记录做重放，从而实现同步。这种方式的优点是实时性高，可以精确捕捉上游的各种变动；缺点是部署数据库的事件接收和解析器（例如 Debezium、Canal 等），有一定的学习和运维成本，对一些冷门的数据库支持不够。<br>  综合来看，事件接收模式整体在实时性、吞吐量方面占优，如果数据源是 MySQL、PostgreSQL、MongoDB 等常见的数据库实现，建议使用 Debezium 来实现变更数据的捕获（下图来自 Debezium 官方文档）。如果使用的只有 MySQL，则还可以用 Canal。<br>   以下是debezium的架构：<br>   <img src="/2024/07/17/Doris-begin/debezium.png" alt="debezium" title="debezium"><br>   那么有debezium还是要选择flink的原因有哪些呢：<br>   从上图可以看到，Debezium 官方架构图中，是通过 Kafka Streams 直接实现的 CDC 功能。而我们这里更建议使用 Flink CDC 模块，因为 Flink 相对 Kafka Streams 而言，有如下优势：<br>  Flink 的算子和 SQL 模块更为成熟和易用<br>  Flink 作业可以通过调整算子并行度的方式，轻松扩展处理能力<br>  Flink 支持高级的状态后端（State Backends），允许存取海量的状态数据<br>  Flink 提供更多的 Source 和 Sink 等生态支持<br>  Flink 有更大的用户基数和活跃的支持社群，问题更容易解决<br>Flink 的开源协议允许云厂商进行全托管的深度定制，而 Kafka Streams 只能自行部署和运维。而且 Flink Table / SQL 模块将数据库表和变动记录流（例如 CDC 的数据流）看做是同一事物的两面，因此内部提供的 Upsert 消息结构（+I 表示新增、-U 表示记录更新前的值、+U 表示记录更新后的值，-D 表示删除）可以与 Debezium 等生成的变动记录一一对应。<br>   从上图可以看到，Debezium 官方架构图中，是通过 Kafka Streams 直接实现的 CDC 功能。而我们这里更建议使用 Flink CDC 模块，因为 Flink 相对 Kafka Streams 而言，有如下优势：</p>
<p>Flink 的算子和 SQL 模块更为成熟和易用<br>Flink 作业可以通过调整算子并行度的方式，轻松扩展处理能力<br>Flink 支持高级的状态后端（State Backends），允许存取海量的状态数据<br>Flink 提供更多的 Source 和 Sink 等生态支持<br>Flink 有更大的用户基数和活跃的支持社群，问题更容易解决<br>Flink 的开源协议允许云厂商进行全托管的深度定制，而 Kafka Streams 只能自行部署和运维<br>而且 Flink Table / SQL 模块将数据库表和变动记录流（例如 CDC 的数据流）看做是同一事物的两面，因此内部提供的 Upsert 消息结构（+I 表示新增、-U 表示记录更新前的值、+U 表示记录更新后的值，-D 表示删除）可以与 Debezium 等生成的变动记录一一对应。<br>  准实时报表的实现：<br>  准实时报表的实现最好用海豚调度器，其他调度器比如xxljob或者ejob也不是不可以。</p>
<p>  实时报表的配置例子：<br>  一个标准的实时报表配置包括资源配置、数据源配置、目标数据配置和同步语句配置，配置可以用zeppelin为载体，以用户管理为例子<br>资源配置如下：<br>  <figure class="highlight plain"><figcaption><span>shell config</span></figcaption><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div></pre></td><td class="code"><pre><div class="line">%flink.conf</div><div class="line">  </div><div class="line">jobmanager.memory.process.size 1048m //这几个配置看情况进行调整，一般来说足够</div><div class="line">taskmanager.memory.process.size 1048m</div><div class="line">taskmanager.numberOfTaskSlots 1</div><div class="line">parallelism.default 2  //一般来说足够</div><div class="line">yarn.application.name dwd_auth_user_s //这是别名根据自己业务修改</div><div class="line">execution.checkpointing.interval 120000</div><div class="line">execution.checkpointing.tolerable-failed-checkpoints 5</div><div class="line">execution.checkpointing.externalized-checkpoint-retention RETAIN_ON_CANCELLATION</div><div class="line"></div><div class="line">#flink metric job_name 监控报警用</div><div class="line">metrics.reporter.promgateway.jobName dwd_auth_user_s  //任务名称</div><div class="line">metrics.reporter.promgateway.groupingKey job_name=dwd_auth_user_s;team=auth   //任务名称和团队根据情况修改</div><div class="line"> </div><div class="line">flink.execution.jars hdfs:///zeppelin/jars/doris-flink-1.0.jar,hdfs:///zeppelin/jars/flink-connector-mysql-cdc-2.1.jar</div><div class="line">#连接kafka用 可以不用配置</div><div class="line">flink.execution.packages org.apache.flink:flink-connector-kafka_2.11:1.13.3//</div></pre></td></tr></table></figure></p>
<p>数据源配置：</p>
<figure class="highlight plain"><figcaption><span>shell config</span></figcaption><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div></pre></td><td class="code"><pre><div class="line"></div><div class="line">SQL</div><div class="line">%flink.ssql</div><div class="line"></div><div class="line">CREATE TABLE `mysql_auth_user` (</div><div class="line">  `id` STRING NOT NULL COMMENT &apos;用户id&apos;,</div><div class="line">  `user_name` STRING NOT NULL,</div><div class="line">  `full_name` STRING NOT NULL COMMENT &apos;全名&apos;,</div><div class="line">  `user_type` int COMMENT &apos;用户类型&apos;,</div><div class="line">  `deleted` int,</div><div class="line">  `create_time` timestamp(3) COMMENT &apos;创建时间&apos;,</div><div class="line">  `create_user` STRING  COMMENT &apos;创建用户&apos;,</div><div class="line">  `update_time` timestamp(3) NULL ,</div><div class="line">  `update_user` STRING ,</div><div class="line">  `password` STRING  COMMENT &apos;密码&apos;,</div><div class="line">  PRIMARY KEY (`id`)   NOT ENFORCED</div><div class="line">) WITH (</div><div class="line"> &apos;connector&apos; = &apos;mysql-cdc&apos;,</div><div class="line"> &apos;hostname&apos; = &apos;127.0.0.1&apos;,</div><div class="line"> &apos;port&apos; = &apos;3306&apos;,</div><div class="line"> &apos;username&apos; = &apos;root&apos;,</div><div class="line"> &apos;password&apos; = &apos;123456&apos;,</div><div class="line"> &apos;database-name&apos; = &apos;auth&apos;,//数据库名称</div><div class="line"> &apos;table-name&apos; = &apos;user&apos;,//数据表</div><div class="line"> &apos;scan.startup.mode&apos; = &apos;initial&apos;, //读取全量数据</div><div class="line"> &apos;server-time-zone&apos; = &apos;Asia/Shanghai&apos;</div><div class="line">);</div></pre></td></tr></table></figure>
<p>目标数据源配置：<br><figure class="highlight plain"><figcaption><span>shell config</span></figcaption><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div></pre></td><td class="code"><pre><div class="line"></div><div class="line">SQL</div><div class="line">%flink.ssql</div><div class="line">CREATE TABLE `doris_ods_auth_user_s` (</div><div class="line">SQL</div><div class="line">%flink.ssql</div><div class="line">CREATE TABLE `doris_ods_g2park_barrier_s` (</div><div class="line">`id` STRING NOT NULL COMMENT &apos;用户id&apos;,</div><div class="line">  `user_name` STRING NOT NULL,</div><div class="line">  `full_name` STRING NOT NULL COMMENT &apos;全名&apos;,</div><div class="line">  `user_type` int COMMENT &apos;用户类型&apos;,</div><div class="line">  `deleted` int,</div><div class="line">  `create_time` timestamp(3) COMMENT &apos;创建时间&apos;,</div><div class="line">  `create_user` STRING  COMMENT &apos;创建用户&apos;,</div><div class="line">  `update_time` timestamp(3) NULL ,</div><div class="line">  `update_user` STRING ,</div><div class="line">  `password` STRING  COMMENT &apos;密码&apos;,</div><div class="line">  PRIMARY KEY (`id`)   NOT ENFORCED</div><div class="line">)  WITH (</div><div class="line">    &apos;connector&apos; = &apos;doris&apos;,</div><div class="line">    &apos;fenodes&apos; = &apos;127.0.0.1:8030&apos;,//doris地fe地址</div><div class="line">    &apos;table.identifier&apos; = &apos;auth_db.ods_auth_mysql_user_s&apos;,</div><div class="line">    &apos;sink.batch.size&apos; = &apos;50000&apos;,//批量插入数据</div><div class="line">    &apos;sink.batch.interval&apos;=&apos;6000&apos;,//6s提交一次</div><div class="line">    &apos;username&apos; = &apos;&apos;,//用户名</div><div class="line">    &apos;password&apos; = &apos;&apos;,//密码</div><div class="line">    &apos;sink.properties.column_separator&apos; = &apos;|||&apos;,//文件分隔符</div><div class="line">    &apos;sink.properties.max_filter_ratio&apos;=&apos;0.2&apos;,//r容忍错误率20%</div><div class="line">    &apos;doris.request.connect.timeout.ms&apos;=&apos;3000000&apos;,</div><div class="line">    &apos;doris.request.read.timeout.ms&apos;=&apos;3000000&apos;,</div><div class="line">    &apos;doris.request.query.timeout.s&apos;=&apos;360000&apos;</div><div class="line">);</div></pre></td></tr></table></figure></p>
<p>同步语句设置：<br><figure class="highlight plain"><figcaption><span>shell config</span></figcaption><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"></div><div class="line">SQL</div><div class="line">%flink.ssql(jobName=&quot;mysql_g2park_inout_record&quot;, resumeFromLatestCheckpoint=false)</div><div class="line"></div><div class="line">insert into doris_ods_auth_user_s select * from mysql_auth_user;</div><div class="line"></div><div class="line">resumeFromLatestCheckpoint=false表示重新拉取</div></pre></td></tr></table></figure></p>
<p>kafka数据同步：<br>kafka的数据同步是基于flink进行同步的，当然doris也能从kafka直接抽取数据，但是为了统一管理配置，数据抽取统一采用flink，以下为例子：<br>资源配置：<br><figure class="highlight plain"><figcaption><span>shell config</span></figcaption><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line"></div><div class="line">%flink.conf</div><div class="line">  </div><div class="line">jobmanager.memory.process.size 1048m //这几个配置看情况进行调整，一般来说足够</div><div class="line">taskmanager.memory.process.size 1048m</div><div class="line">taskmanager.numberOfTaskSlots 1</div><div class="line">parallelism.default 2  //一般来说足够</div><div class="line">yarn.application.name kafka_to_doris //这是别名根据自己业务修改</div><div class="line">execution.checkpointing.interval 120000</div><div class="line">execution.checkpointing.tolerable-failed-checkpoints 5</div><div class="line">execution.checkpointing.externalized-checkpoint-retention RETAIN_ON_CANCELLATION</div><div class="line"> </div><div class="line">flink.execution.jars hdfs:///zeppelin/jars/doris-flink-1.0.jar,hdfs:///zeppelin/jars/flink-connector-mysql-cdc-2.1.jar</div><div class="line">flink.execution.packages org.apache.flink:flink-connector-kafka_2.11:1.13.3</div></pre></td></tr></table></figure></p>
<p>数据源配置：<br><figure class="highlight plain"><figcaption><span>shell config</span></figcaption><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div></pre></td><td class="code"><pre><div class="line"></div><div class="line"></div><div class="line">SQL</div><div class="line">%flink.ssql</div><div class="line">CREATE TABLE source_kafka_data</div><div class="line">(</div><div class="line">    siteid INT,</div><div class="line">    citycode INT,</div><div class="line">    username String,</div><div class="line">    pv INT</div><div class="line">)WITH(</div><div class="line">&apos;connector&apos; = &apos;kafka&apos;,</div><div class="line">&apos;topic&apos; = &apos;doris_topic&apos;,</div><div class="line">&apos;properties.bootstrap.servers&apos; = &apos;&apos;,</div><div class="line">&apos;properties.group.id&apos; = &apos;doris_group&apos;,</div><div class="line">&apos;scan.startup.mode&apos; = &apos;latest-offset&apos;, //读取最新数据</div><div class="line">&apos;format&apos; = &apos;json&apos;</div><div class="line">)</div></pre></td></tr></table></figure></p>
<p>目标数据配置：<br><figure class="highlight plain"><figcaption><span>shell config</span></figcaption><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div></pre></td><td class="code"><pre><div class="line"></div><div class="line">SQL</div><div class="line">%flink.ssql</div><div class="line">CREATE TABLE doris_test_kafka</div><div class="line">(</div><div class="line">    siteid INT,</div><div class="line">    citycode INT,</div><div class="line">    username String,</div><div class="line">    pv INT</div><div class="line">) WITH (</div><div class="line">    &apos;connector&apos; = &apos;doris&apos;,</div><div class="line">    &apos;fenodes&apos; = &apos;127.0.0.1:8030&apos;,</div><div class="line">    &apos;table.identifier&apos; = &apos;example_db.test_kafka&apos;,</div><div class="line">    &apos;sink.batch.size&apos; = &apos;50000&apos;,</div><div class="line">    &apos;sink.batch.interval&apos;=&apos;600&apos;,</div><div class="line">    &apos;username&apos; = &apos;&apos;,</div><div class="line">    &apos;password&apos; = &apos;&apos;,</div><div class="line">    &apos;sink.properties.column_separator&apos; = &apos;|||&apos;,</div><div class="line">    &apos;sink.properties.max_filter_ratio&apos;=&apos;0.2&apos;,</div><div class="line">    &apos;doris.request.connect.timeout.ms&apos;=&apos;3000000&apos;,</div><div class="line">    &apos;doris.request.read.timeout.ms&apos;=&apos;3000000&apos;,</div><div class="line">    &apos;doris.request.query.timeout.s&apos;=&apos;360000&apos;</div><div class="line">);</div></pre></td></tr></table></figure></p>
<p>同步语句配置：<br><figure class="highlight plain"><figcaption><span>shell config</span></figcaption><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"></div><div class="line">SQL</div><div class="line">%flink.ssql(jobName=&quot;kafka_to_doris&quot;, resumeFromLatestCheckpoint=false)</div><div class="line"></div><div class="line">insert into doris_test_kafka select * from source_kafka_data;</div></pre></td></tr></table></figure></p>
<p>最后通过zeppelin发布<br>点击每个配置的右上角按钮进行配置，如果报错查看对应日志进行修改</p>
<img src="/2024/07/17/Doris-begin/zeppelin1.png" alt="zeppelin1" title="zeppelin1">
<p>在同步语句配置块点击按钮到flink控制台查看对应执行日志<br><img src="/2024/07/17/Doris-begin/zeppelin2.png" alt="zeppelin2" title="zeppelin2"><br><img src="/2024/07/17/Doris-begin/zeppelin3.png" alt="zeppelin3" title="zeppelin3"></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2024/07/17/Doris-begin/" data-id="clypv5bhy00006y790qk6hg5w" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/bigdata/">bigdata</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-influx-proxy-problemlist" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2021/05/28/influx-proxy-problemlist/" class="article-date">
  <time datetime="2021-05-27T17:08:42.000Z" itemprop="datePublished">2021-05-28</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2021/05/28/influx-proxy-problemlist/">influx-proxy部署遇到的问题</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>环境说明：<br>influxdb 1.8.2<br>influx-proxy采用shell909090<br>系统采用centos7<br>记录过程如下：<br>1、网络问题<br>因为墙的原因，访问不了google的地址，这时启用了科学翻墙工具<br>用http_proxy和https_proxy来设置<br>2、一开始安装的golang 1.11版本<br>安装influx-proxy报错如下：<br>package github.com/influxdata/influxdb/client/v2: cannot find package “github.com/influxdata/influxdb/client/v2” in any of:<br>/usr/lib/go/src/github.com/influxdata/influxdb/client/v2 (from $GOROOT)<br>/home/test/go/src/github.com/influxdata/influxdb/client/v2 (from $GOPATH)<br>尝试独立安装golang的influxdb client<br>参考如下：<br><a href="https://pkg.go.dev/github.com/influxdata/influxdb-client-go/v2#readme-installation" target="_blank" rel="external">https://pkg.go.dev/github.com/influxdata/influxdb-client-go/v2#readme-installation</a><br>采用命令go get github.com/influxdata/influxdb-client-go/v2安装<br>仍然报错<br>这时找到如下资料：<br><a href="https://github.com/influxdata/influxdb-client-go/issues/186" target="_blank" rel="external">https://github.com/influxdata/influxdb-client-go/issues/186</a><br>怀疑是官方代码有问题，不过人家已经在去年8月份修复了<br>这时又查到一篇文章<br><a href="https://github.com/shell909090/influx-proxy/issues/64" target="_blank" rel="external">https://github.com/shell909090/influx-proxy/issues/64</a><br>这里有人提到了可以使用influxdb1-client来代替。<br>“问题还在<br>解决办法：<br>go get - u github.com/influxdata/influxdb1-client （新的依赖地址）<br>go get -u github.com/shell909090/influx-proxy/service<br>报错之后<br>找到（go env）$GOPATH/src/github.com/shell909090/influx-proxy/monitor/metric.go<br>修改为 client “github.com/influxdata/influxdb1-client/v2”<br>到$GOPATH/src/github.com/shell909090/influx-proxy/service目录 go install 然后步骤就顺了”<br>此时却是安装成功了，buginflux-proxy服务起来后从日志（/var/log/influx-proxy.log）上开开始报异常<br>2021/05/27 21:44:42.643833 http.go:99: http error: Get “<a href="http://xxxx:9999/ping" target="_blank" rel="external">http://xxxx:9999/ping</a>“: context deadline exceeded (Client.Timeout exceeded while awaiting headers)<br>2021/05/27 21:44:53.644209 http.go:99: http error: Get “<a href="http://xxxx:9999/ping" target="_blank" rel="external">http://xxxx:9999/ping</a>“: context deadline exceeded (Client.Timeout exceeded while awaiting headers)<br>2021/05/27 21:44:53.644204 http.go:99: http error: Get “<a href="http://xxxx:9999/ping" target="_blank" rel="external">http://xxxx:9999/ping</a>“: context deadline exceeded (Client.Timeout exceeded while awaiting headers)<br>2021/05/27 21:45:04.645219 http.go:99: http error: Get “<a href="http://xxxx:9999/ping" target="_blank" rel="external">http://xxxx:9999/ping</a>“: context deadline exceeded (Client.Timeout exceeded while awaiting headers)<br>2021/05/27 21:45:04.645221 http.go:99: http error: Get “<a href="http://xxx:9999/ping" target="_blank" rel="external">http://xxx:9999/ping</a>“: context deadline exceeded (Client.Timeout exceeded while awaiting headers)<br>也就是连不上我的两个influxdb服务</p>
<p>最后再回过头来再仔细看influxdb-client-go的安装文档<br>发现要求golang版本必须1.13以上并且influxdb1只针对influxdb1.7之前的版本，1.8之后的在v2里<br>此时重新按转golang和influx-proxy，启动仍然报异常，通过curl命令得到如下异常信息：<br> (```[root@abc github.com]# curl <a href="http://127.0.0.1:8086/ping" target="_blank" rel="external">http://127.0.0.1:8086/ping</a></p>
<p><html></html></p>
<p><head><br> <title>500 Internal Privoxy Error</title><br> <link rel="shortcut icon" href="http://config.privoxy.org/error-favicon.ico" type="image/x-icon"></head></p>
<p><body></body></p>
<p></p><h1>500 Internal Privoxy Error</h1><p></p>
<p></p><p>Privoxy encountered an error while processing your request:</p><p></p>
<p></p><p><b>Could not load template file <code>connect-failed</code> or one of its included components.</b></p><p></p>
<p></p><p>Please contact your proxy administrator.</p><p></p>
<p></p><p>If you are the proxy administrator, please put the required file(s)in the <code><i>(confdir)</i>/templates</code> directory.  The location of the <code><i>(confdir)</i></code> directory is specified in the main Privoxy <code>config</code> file.  (It’s typically the Privoxy install directory, or <code>/etc/privoxy/</code>).</p><br><br><br>```)<br>发现是原来设置了http代理的问题，<br>该用goproxy才最终成功，这里值得一提的事golang1.11版本不支持goproxy。<p></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2021/05/28/influx-proxy-problemlist/" data-id="clypv5bis000j6y7983lp8t2d" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/influx-proxy/">influx-proxy</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-build-flume-ha" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2020/04/25/build-flume-ha/" class="article-date">
  <time datetime="2020-04-25T12:55:43.000Z" itemprop="datePublished">2020-04-25</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2020/04/25/build-flume-ha/">构建flume高可用集群</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <img src="/2020/04/25/build-flume-ha/flume-ha.png" alt="flume-ha" title="flume-ha">
<p>服务器端配置如下:</p>
<figure class="highlight plain"><figcaption><span>shell config</span></figcaption><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div></pre></td><td class="code"><pre><div class="line"></div><div class="line">#set Agent name</div><div class="line">agent.sources = r1</div><div class="line">agent.channels = c1</div><div class="line">agent.sinks = k1</div><div class="line">#set channel</div><div class="line">agent.channels.c1.type = memory</div><div class="line">agent.channels.c1.capacity = 1024000</div><div class="line">agent.channels.c1.transactionCapacity = 10000</div><div class="line">agent.channels.c1.byteCapacity=134217728</div><div class="line">agent.channels.c1.byteCapacityBufferPercentage=80</div><div class="line"></div><div class="line"># other node,nna to nns</div><div class="line">agent.sources.r1.type = avro</div><div class="line">agent.sources.r1.bind = &#123;localip&#125;</div><div class="line">agent.sources.r1.port = 52020</div><div class="line">agent.sources.r1.channels = c1</div><div class="line"></div><div class="line">#set sink to hdfs</div><div class="line">agent.sinks.k1.channel = c1</div><div class="line">agent.sinks.k1.type = org.apache.flume.sink.kafka.KafkaSink</div><div class="line">agent.sinks.k1.brokerList = &#123;kafkaIp&#125;:9092</div><div class="line">agent.sinks.k1.topic = flume-test</div><div class="line">agent.sinks.k1.serializer.class = kafka.serializer.StringEncoder</div></pre></td></tr></table></figure>
<p>flume自带load_banlance与fail over<br>先说load_banlance发送端配置如下:<br><figure class="highlight plain"><figcaption><span>shell config</span></figcaption><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div></pre></td><td class="code"><pre><div class="line">#agent1 name</div><div class="line">agent.channels = c1</div><div class="line">agent.sources = r1</div><div class="line">agent.sinks = k1 k2 k3</div><div class="line">#set gruop</div><div class="line">agent.sinkgroups = g1</div><div class="line">#set channel</div><div class="line">agent.channels.c1.type = memory</div><div class="line">agent.channels.c1.capacity = 102400</div><div class="line">agent.channels.c1.transactionCapacity = 1000</div><div class="line">agent.channels.c1.byteCapacity=134217728</div><div class="line">agent.channels.c1.byteCapacityBufferPercentage=80</div><div class="line"></div><div class="line">agent.sources.r1.channels = c1</div><div class="line"></div><div class="line">######## source相关配置 ########</div><div class="line"># source类型</div><div class="line">agent.sources.r1.type = TAILDIR</div><div class="line"># 元数据位置</div><div class="line">agent.sources.r1.positionFile = /root/taildir_position.json</div><div class="line"># 监控的目录</div><div class="line">agent.sources.r1.filegroups = f1</div><div class="line">agent.sources.r1.filegroups.f1=/root/.*log</div><div class="line">agent.sources.r1.fileHeader = true</div><div class="line"></div><div class="line"></div><div class="line"></div><div class="line"># set sink1</div><div class="line">agent.sinks.k1.channel = c1</div><div class="line">agent.sinks.k1.type = avro</div><div class="line">agent.sinks.k1.hostname = &#123;node1&#125;</div><div class="line">agent.sinks.k1.port = 52020</div><div class="line"># set sink2</div><div class="line">agent.sinks.k2.channel = c1</div><div class="line">agent.sinks.k2.type = avro</div><div class="line">agent.sinks.k2.hostname = &#123;node2&#125;</div><div class="line">agent.sinks.k2.port = 52020</div><div class="line"># set sink3</div><div class="line">agent.sinks.k3.channel = c1</div><div class="line">agent.sinks.k3.type = avro</div><div class="line">agent.sinks.k3.hostname = &#123;node3&#125;</div><div class="line">agent.sinks.k3.port = 52020</div><div class="line">#set sink group</div><div class="line">agent.sinkgroups.g1.sinks = k1 k2 k3</div><div class="line">#set load_banlance</div><div class="line">agent.sinkgroups.g1.processor.type = load_balance</div><div class="line">agent.sinkgroups.g1.processor.backoff=true   # 这个里面是个惩罚机制</div><div class="line">agent.sinkgroups.g1.processor.selector=round_robin</div></pre></td></tr></table></figure></p>
<p>load_banlance:<br>1.当三台主机中的一台进程挂掉后，发送端会报错误，但是不影响数据的发送。<br>2.当挂掉的进程回复后，会继续被接受数据。这里面有个惩罚机制，如果开启的失败的接受端会被等待延迟！<br>3.当挂掉的进程回复后，发送端会自动添加会发送列表里，报错消失！<br>4.这个就是一个简单的均衡方式，相当于见了一个sink组，在sink组中的数据被轮询发送！</p>
<p>再说fail over配置:<br>服务器端不变<br>发送端配置改为:<br><figure class="highlight plain"><figcaption><span>shell config</span></figcaption><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div></pre></td><td class="code"><pre><div class="line"></div><div class="line">#agent1 name</div><div class="line">agent.channels = c1</div><div class="line">agent.sources = r1</div><div class="line">agent.sinks = k1 k2 k3</div><div class="line">#set gruop</div><div class="line">agent.sinkgroups = g1</div><div class="line">#set channel</div><div class="line">agent.channels.c1.type = memory</div><div class="line">agent.channels.c1.capacity = 102400</div><div class="line">agent.channels.c1.transactionCapacity = 1000</div><div class="line">agent.channels.c1.byteCapacity=134217728</div><div class="line">agent.channels.c1.byteCapacityBufferPercentage=80</div><div class="line"></div><div class="line">agent.sources.r1.channels = c1</div><div class="line"></div><div class="line">######## source相关配置 ########</div><div class="line"># source类型</div><div class="line">agent.sources.r1.type = TAILDIR</div><div class="line"># 元数据位置</div><div class="line">agent.sources.r1.positionFile = /root/taildir_position.json</div><div class="line"># 监控的目录</div><div class="line">agent.sources.r1.filegroups = f1</div><div class="line">agent.sources.r1.filegroups.f1=/root/.*log</div><div class="line">agent.sources.r1.fileHeader = true</div><div class="line"></div><div class="line"></div><div class="line"></div><div class="line"># set sink1</div><div class="line">agent.sinks.k1.channel = c1</div><div class="line">agent.sinks.k1.type = avro</div><div class="line">agent.sinks.k1.hostname = &#123;node1&#125;</div><div class="line">agent.sinks.k1.port = 52020</div><div class="line"># set sink2</div><div class="line">agent.sinks.k2.channel = c1</div><div class="line">agent.sinks.k2.type = avro</div><div class="line">agent.sinks.k2.hostname = &#123;node2&#125;</div><div class="line">agent.sinks.k2.port = 52020</div><div class="line"># set sink3</div><div class="line">agent.sinks.k3.channel = c1</div><div class="line">agent.sinks.k3.type = avro</div><div class="line">agent.sinks.k3.hostname = &#123;node3&#125;</div><div class="line">agent.sinks.k3.port = 52020</div><div class="line">#set sink group</div><div class="line">agent.sinkgroups.g1.sinks = k1 k2 k3</div><div class="line">#set failover</div><div class="line">agent.sinkgroups.g1.processor.type = failover</div><div class="line">agent.sinkgroups.g1.processor.backoff=true   # 这个里面是个惩罚机制</div><div class="line">agent.sinkgroups.g1.processor.selector=round_robin</div><div class="line">agent.sinkgroups.g1.processor.priority.k1 = 30</div><div class="line">agent.sinkgroups.g1.processor.priority.k2 = 20</div><div class="line">agent.sinkgroups.g1.processor.priority.k3 = 10</div><div class="line">agent.sinkgroups.g1.processor.maxpenalty = 10000</div></pre></td></tr></table></figure></p>
<p>区别在于负载均衡，每台机器都会受到数据处理数据，但是有时候业务需求可能需要一台机器为主要一条业务的数据接收器，另一个台在挂掉后才会被启用，这时候这个就有用了。</p>
<p>还有一种玩法可以在多个flume前面加个nginx<br><img src="/2020/04/25/build-flume-ha/flume-ha2.png" alt="flume-nginx" title="flume-nginx"></p>
<p>ng配置如下:<br>首先在nginx.conf中加入:</p>
<p>include       flume.conf;</p>
<p>加入后配置如下:<br><figure class="highlight plain"><figcaption><span>shell config</span></figcaption><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div></pre></td><td class="code"><pre><div class="line"></div><div class="line">#user  nobody;</div><div class="line">worker_processes  1;</div><div class="line"></div><div class="line">#error_log  logs/error.log;</div><div class="line">#error_log  logs/error.log  notice;</div><div class="line">#error_log  logs/error.log  info;</div><div class="line"></div><div class="line">#pid        logs/nginx.pid;</div><div class="line"></div><div class="line"></div><div class="line">events &#123;</div><div class="line">    worker_connections  1024;</div><div class="line">&#125;</div><div class="line">include       flume.conf;</div><div class="line"></div><div class="line"></div><div class="line">http &#123;</div><div class="line"></div><div class="line"></div><div class="line">...</div><div class="line">&#125;</div><div class="line"></div></pre></td></tr></table></figure></p>
<p>新建flume.conf</p>
<p>配置如下:<br><figure class="highlight plain"><figcaption><span>shell config</span></figcaption><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div></pre></td><td class="code"><pre><div class="line"></div><div class="line">stream &#123;</div><div class="line">      upstream abcdocker &#123;</div><div class="line">        server node1:52020  weight=5 max_fails=3 fail_timeout=30s;</div><div class="line">        server node2:52020  weight=5 max_fails=3 fail_timeout=30s;</div><div class="line">        server node3:52020  weight=5 max_fails=3 fail_timeout=30s;</div><div class="line"></div><div class="line">     &#125;</div><div class="line"></div><div class="line">     server &#123;</div><div class="line">            listen 52021;</div><div class="line">            proxy_pass abcdocker;</div><div class="line">            proxy_connect_timeout 10s;</div><div class="line">            proxy_timeout 24h;</div><div class="line">              &#125;</div><div class="line">  &#125;</div><div class="line"></div></pre></td></tr></table></figure></p>
<p>flume发送端如下:<br><figure class="highlight plain"><figcaption><span>shell config</span></figcaption><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div></pre></td><td class="code"><pre><div class="line"></div><div class="line">#agent1 name</div><div class="line">agent.channels = c1</div><div class="line">agent.sources = r1</div><div class="line">agent.sinks = k1</div><div class="line">#set channel</div><div class="line">agent.channels.c1.type = memory</div><div class="line">agent.channels.c1.capacity = 102400</div><div class="line">agent.channels.c1.transactionCapacity = 1000</div><div class="line">agent.channels.c1.byteCapacity=134217728</div><div class="line">agent.channels.c1.byteCapacityBufferPercentage=80</div><div class="line"></div><div class="line">agent.sources.r1.channels = c1</div><div class="line"></div><div class="line">######## source相关配置 ########</div><div class="line"># source类型</div><div class="line">agent.sources.r1.type = TAILDIR</div><div class="line"># 元数据位置</div><div class="line">agent.sources.r1.positionFile = /root/taildir_position.json</div><div class="line"># 监控的目录</div><div class="line">agent.sources.r1.filegroups = f1</div><div class="line">agent.sources.r1.filegroups.f1=/root/.*log</div><div class="line">agent.sources.r1.fileHeader = true</div><div class="line"></div><div class="line"></div><div class="line"></div><div class="line"># set sink1</div><div class="line">agent.sinks.k1.channel = c1</div><div class="line">agent.sinks.k1.type = avro</div><div class="line">agent.sinks.k1.hostname = &#123;nginx&#125;</div><div class="line">agent.sinks.k1.port = 52021</div><div class="line"></div></pre></td></tr></table></figure></p>
<p>如果要造数据的话可以这样建个sh文件<br><figure class="highlight plain"><figcaption><span>shell config</span></figcaption><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"></div><div class="line">#!/bin/bash</div><div class="line">while true</div><div class="line">do</div><div class="line">        date &gt;&gt; test.log</div><div class="line">        sleep 0.5</div><div class="line">done</div></pre></td></tr></table></figure></p>
<p>执行即可</p>
<p>最后兵无常势水无常形,多思考用最适合自己的方案即可.</p>
<p>参考资料:<br><a href="https://blog.csdn.net/zengxianglei/article/details/89290050" target="_blank" rel="external">https://blog.csdn.net/zengxianglei/article/details/89290050</a><br><a href="https://www.cnblogs.com/chushiyaoyue/articles/6211599.html" target="_blank" rel="external">https://www.cnblogs.com/chushiyaoyue/articles/6211599.html</a></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2020/04/25/build-flume-ha/" data-id="clypv5bie00066y79kqvdcm90" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/flume/">flume</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-firecracker-introduction" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2020/04/14/firecracker-introduction/" class="article-date">
  <time datetime="2020-04-13T18:10:23.000Z" itemprop="datePublished">2020-04-14</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2020/04/14/firecracker-introduction/">firecracker介绍</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>firecracker是aws开源的serverless计算框架.<br>firecracker是基于kvm的micro vm技术,相对于容器隔离醒更好,同时性能更快.<br>仅启动相对较新的 Linux 内核，并且仅启动使用特定配置选项集编译的内核（内核编译配置选项超过 1000 种）。<br>此外，不支持任何类型的图形卡或加速器，不支持硬件透传，不支持（大多数）老旧设备。<br>由于设备模型极简，内核加载过程也简单，可以实现小于 125 ms 的启动时间和更少的内存占用。<br>Firecracker 目前支持 Intel CPU，并将于 2019 年开始支持 AMD 和 ARM，还将与 containerd 等流行的容器运行时集成。</p>
<p>与宿主机的关系:<br><img src="/2020/04/14/firecracker-introduction/host.png" alt="introduce" title="introduce"><br>Firecracker 运行在 Linux 主机上，内核为4.14或更新内核，并且使用 Linux guest OSs (从这一点来说，称为 guest)。 启动该进程后，在发出 instanceart 命令之前，用户与 Firecracker API 交互以配置 microVM。</p>
<p>Firecracker 内部架构<br><img src="/2020/04/14/firecracker-introduction/org.png" alt="introduce" title="introduce"><br>每个 Firecracker 进程封装一个且只有一个 microVM。 该进程运行以下线程: API、 VMM 和 vCPU。 Api 线程负责 Firecracker 的 API 服务器和相关的控制平面。 它永远不会在虚拟机的快速路径上。 Vmm 线程公开机器模型、最小遗留设备模型、 microVM 元数据服务(MMDS)和 VirtIO 设备仿真 Net 和 Block 设备，并提供 i / o 速率限制。 除此之外，还有一个或多个 vCPU 线程(每个客户 CPU 核心一个)。 它们是通过 KVM 创建的，并运行 KVM run 主循环。 它们在设备模型上执行同步 i / o 和存储器映射输入输出操作。</p>
<p>Firecracker如何工作<br><img src="/2020/04/14/firecracker-introduction/work.png" alt="introduce" title="introduce"><br>Firecracker 在用户空间中运行，使用基于 Linux 内核的虚拟机（KVM）来创建 microVM。每个 microVM 的快速启动时间和低内存开销使你可将数千个 microVM 打包到同一台机器上。这意味着每个函数或容器组都可以使用虚拟机屏障进行封装，从而使不同用户的工作负载能在同一台计算机上运行，而无需在安全性和效率之间进行权衡。Firecracker 是 QEMU 的替代品，QEMU 是一个成熟的 VMM，具有通用和广泛的功能集，可以托管各种客户操作系统。</p>
<p>可以通过 RESTful API 控制 Firecracker 进程，RESTful API 可以启用常见操作：例如配置 vCPU 数量或启动计算机。Firecracker 提供内置速率限制器，可精确控制同一台计算机上数千个 microVM 使用的网络和存储资源。你可以通过 Firecracker API 创建和配置速率限制器，并灵活定义速率限制器来支持突发情况或特定带宽 / 操作限制。Firecracker 还提供元数据服务，可在主机和客户机操作系统之间安全地共享配置信息。元数据服务可以使用 Firecracker API 设置。</p>
<p>Firecracker入门实例<br>1、找个linux电脑并且安装依赖<br>sudo apt update<br>sudo apt install qemu qemu-kvm libvirt-bin bridge-utils virt-manager<br>2、开启cpu虚拟化<br>开机进入bios后<br>Configuratio &gt; Intel Virtual Technology &gt; Enabled<br>3、根据官网进行安装<br>latest=$(basename $(curl -fsSLI -o /dev/null -w  %{url_effective}  <a href="https://github.com/firecracker-microvm/firecracker/releases/latest" target="_blank" rel="external">https://github.com/firecracker-microvm/firecracker/releases/latest</a>))<br>curl -LOJ <a href="https://github.com/firecracker-microvm/firecracker/releases/download/${latest}/firecracker-${latest}-$(uname" target="_blank" rel="external">https://github.com/firecracker-microvm/firecracker/releases/download/${latest}/firecracker-${latest}-$(uname</a> -m)<br>mv firecracker-${latest}-$(uname -m) firecracker<br>chmod +x firecracker<br>至此完成安装步骤<br>4、启动服务<br>rm -f /tmp/firecracker.socket<br>./firecracker –api-sock /tmp/firecracker.socket</p>
<p>5、再打开一个窗口进行vm配置工作<br>既然是基于kvm的vm那么一定需要rootfs和kernal.原始文件放在aws的s3存储上但是被墙了..<br>这里找到国内的源如下:<br>wget <a href="https://silenceshell-1255345740.cos.ap-shanghai.myqcloud.com/hello-vmlinux.bin" target="_blank" rel="external">https://silenceshell-1255345740.cos.ap-shanghai.myqcloud.com/hello-vmlinux.bin</a><br>wget <a href="https://silenceshell-1255345740.cos.ap-shanghai.myqcloud.com/hello-rootfs.ext4" target="_blank" rel="external">https://silenceshell-1255345740.cos.ap-shanghai.myqcloud.com/hello-rootfs.ext4</a></p>
<p>通过api配置rootfs与kernal<br>sudo curl –unix-socket /tmp/firecracker.sock -i     -X PUT ‘<a href="http://localhost/boot-source" target="_blank" rel="external">http://localhost/boot-source</a>‘       -H ‘Accept: application/json’               -H ‘Content-Type: application/json’         -d ‘{<br>        “kernel_image_path”: “./hello-vmlinux.bin”,<br>        “boot_args”: “console=ttyS0 reboot=k panic=1 pci=off”<br>    }’</p>
<p> sudo curl –unix-socket /tmp/firecracker.socket -i     -X PUT ‘<a href="http://localhost/drives/rootfs" target="_blank" rel="external">http://localhost/drives/rootfs</a>‘     -H ‘Accept: application/json’               -H ‘Content-Type: application/json’         -d ‘{<br>        “drive_id”: “rootfs”,<br>        “path_on_host”: “./hello-rootfs.ext4”,<br>        “is_root_device”: true,<br>        “is_read_only”: false<br>    }’<br>5、通过api启动vm<br>sudo curl –unix-socket /tmp/firecracker.socket -i     -X PUT ‘<a href="http://localhost/actions" target="_blank" rel="external">http://localhost/actions</a>‘           -H  ‘Accept: application/json’              -H  ‘Content-Type: application/json’        -d ‘{<br>        “action_type”: “InstanceStart”<br>     }’</p>
<p>6、最后切换到启动服务的窗口,这时候应该有了登录窗口了,用户名密码默认是root/root<br><img src="/2020/04/14/firecracker-introduction/login.png" alt="introduce" title="introduce"></p>
<p>7、网络配置<br>首先创建tap设备:<br>sudo ip tuntap add tap0 mode tap</p>
<p>如果要开通外网访问可以用nat转发的方式<br>设置一个网段<br>sudo ip addr add 172.16.0.1/24 dev tap0<br>启用tap0<br>并且初始化iptables规则,这里{networkCardName}指的是自己机器上外网网卡的名称<br>sudo ip link set tap0 up<br>sudo sh -c “echo 1 &gt; /proc/sys/net/ipv4/ip_forward”<br>sudo iptables -t nat -A POSTROUTING -o {networkCardName} -j MASQUERADE<br>sudo iptables -A FORWARD -m conntrack –ctstate RELATED,ESTABLISHED -j ACCEPT<br>sudo iptables -A FORWARD -i tap0 -o {networkCardName} -j ACCEPT<br>启动前进行如下配置:<br>curl –unix-socket /tmp/firecracker.socket -i \<br>  -X PUT ‘<a href="http://localhost/network-interfaces/eth0" target="_blank" rel="external">http://localhost/network-interfaces/eth0</a>‘ \<br>  -H ‘Accept: application/json’ \<br>  -H ‘Content-Type: application/json’ \<br>  -d ‘{<br>      “iface_id”: “eth0”,<br>      “guest_mac”: “AA:FC:00:00:00:01”,<br>      “host_dev_name”: “tap0”<br>    }’</p>
<p>启动实例<br>配置实例网络<br>ip addr add 172.16.0.2/24 dev eth0</p>
<p>ip link set eth0 up<br>ip route add default via 172.16.0.1 dev eth0</p>
<p>最后查验一下<br><img src="/2020/04/14/firecracker-introduction/network.png" alt="introduce" title="introduce"></p>
<p>玩完了就清理一下:<br>sudo ip link del tap0<br>sudo iptables -F<br>sudo sh -c “echo 0 &gt; /proc/sys/net/ipv4/ip_forward”</p>
<p>firecreaker给我们提供了一种容器之外的解决方案,同时具备容器的轻量但实际上是vm.我们可以针对vm创建tap网络设备,而且由于是vm从而使其可以具有状态.<br>在物联网领域可以与规则引擎结合处理告警或者复杂逻辑运算.<br>在容器领域也可以与kata结合让容器运行在micro vm中,增强容器.</p>
<p>参考资料:<br><a href="https://www.tuicool.com/articles/memqyaV" target="_blank" rel="external">https://www.tuicool.com/articles/memqyaV</a><br><a href="https://github.com/firecracker-microvm/firecracker/blob/master/docs/getting-started.md" target="_blank" rel="external">https://github.com/firecracker-microvm/firecracker/blob/master/docs/getting-started.md</a><br><a href="https://blog.csdn.net/wangzan18/article/details/104519333" target="_blank" rel="external">https://blog.csdn.net/wangzan18/article/details/104519333</a><br><a href="https://github.com/kata-containers/documentation/issues/351?spm=a2c4e.10696291.0.0.480319a44pfqoF" target="_blank" rel="external">https://github.com/kata-containers/documentation/issues/351?spm=a2c4e.10696291.0.0.480319a44pfqoF</a><br><a href="https://yq.aliyun.com/articles/692117" target="_blank" rel="external">https://yq.aliyun.com/articles/692117</a></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2020/04/14/firecracker-introduction/" data-id="clypv5big00096y79a4o8krhi" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>


  
    <article id="post-how-to-unregister-service-from-eureka" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2020/04/13/how-to-unregister-service-from-eureka/" class="article-date">
  <time datetime="2020-04-12T17:40:04.000Z" itemprop="datePublished">2020-04-13</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2020/04/13/how-to-unregister-service-from-eureka/">如何安全优雅的把spring微服务从eureka上注销</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>重启服务的方法有很多种.<br>不推荐使用kill -9 ,粗暴的关闭会造成生产数据丢失.<br>这里研究了一下actuator,使用actuator的/pause,可以让服务从eureka上注销但是服务本身还提供服务,<br>也就是可以等30秒等待服务继续把未完成的工作做完,再kill -9 就可以了<br>spring boot1.x与spring 2.x的配置有不一样的地方,举例如下:</p>
<p>spring1.x<br>这里以1.4.1.RELEASE为例子<br>1、在pom文件中加入:</p>
<p> &lt;dependency&gt;<br>            &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;<br>            &lt;artifactId&gt;spring-boot-starter-actuator&lt;/artifactId&gt;<br>            &lt;version&gt;1.4.1.RELEASE&lt;/version&gt;<br>        &lt;/dependency&gt;</p>
<p>2、在application.yml中加入<br>endpoints:</p>
<pre><code># 启用pause端点
pause:
  enabled: true
# 启用restart端点，之所以要启用restart端点，是因为pause端点的启用依赖restart端点的启用
restart:
  enabled: true
</code></pre><p>启动服务后直接调用<br><a href="http://127.0.0.1:{port}/pause" target="_blank" rel="external">http://127.0.0.1:{port}/pause</a><br>从eureka上就是down的状态了</p>
<p>spring2.x:<br>1、在pom文件中加入:<br>       &lt;dependency&gt;<br>            &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;<br>            &lt;artifactId&gt;spring-boot-starter-actuator&lt;/artifactId&gt;<br>            &lt;version&gt;2.1.0.RELEASE&lt;/version&gt;<br>        &lt;/dependency&gt;<br>2、在application.yms中加入:</p>
<p>management:<br>  endpoint:</p>
<pre><code># 启用pause端点
pause:
  enabled: true
# 启用restart端点，之所以要启用restart端点，是因为pause端点的启用依赖restart端点的启用。详见：https://cloud.spring.io/spring-cloud-static/Finchley.SR2/single/spring-cloud.html#_endpoints
restart:
  enabled: true
</code></pre><p>  endpoints:<br>    web:<br>      exposure:<br>        include: pause,restart</p>
<p>启动服务后调用<br><a href="http://127.0.0.1:{port}/actuator/pause" target="_blank" rel="external">http://127.0.0.1:{port}/actuator/pause</a><br>从eureka上就是down的状态了</p>
<p>最后也测试了一下加入kafka consumer不断消费的情况.<br>调用pause后也停止消费了.<br>但是如果实现了org.springframework.cloud.stream.annotation.StreamListener<br>会造成服务注销后再次变为up的情况.<br>说明应该还是有情况会造成程序变为up,需要再研究一下</p>
<hr>
<p>pause有个缺陷,如果开启了healthcheck那么pause会失效.<br>这里最终采用了/service-registry的方式下线应用</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2020/04/13/how-to-unregister-service-from-eureka/" data-id="clypv5biq000g6y79w698alsk" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/spring-eureka/">spring eureka</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-how-to-deploy-logstash-plugin-env" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2020/01/15/how-to-deploy-logstash-plugin-env/" class="article-date">
  <time datetime="2020-01-15T15:21:19.000Z" itemprop="datePublished">2020-01-15</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2020/01/15/how-to-deploy-logstash-plugin-env/">logstash jdbc plugin 优化</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>本文主要讲logstash 插件环境搭建<br>起因是我需要把mysql里一张1000w条数据的表导入到es中,于是采用了logstash数据泵导入到es.<br>logstash是有jdbc插件的,因为数据量巨大,一次性载入内存耗费太大,16g内存机器吃不消.所以采用分页的方式来搞.<br>配置如下:<br>input {<br>jdbc {<br>    jdbc_driver_library =&gt; “/opt/logstash-7.5.1/lib/mysql-connector-java-5.1.44.jar”<br>    jdbc_driver_class =&gt; “com.mysql.jdbc.Driver”<br>    jdbc_connection_string =&gt; “jdbc:mysql://xxxxxxx:33063/xxxxxx?autoReconnect=true&amp;failOverReadOnly=false&amp;maxReconnects=10”<br>    jdbc_user =&gt; “xxxxxx”<br>    jdbc_password =&gt; “xxx”<br>    jdbc_page_size =&gt; 1000<br>    jdbc_validate_connection =&gt; true<br>    jdbc_pool_timeout =&gt; 5<br>    jdbc_validation_timeout =&gt; 10<br>    jdbc_fetch_size =&gt; 1000<br>    connection_retry_attempts_wait_time =&gt; 5<br>    jdbc_paging_enabled =&gt; true<br>    statement =&gt; “SELECT * from a”<br>}<br>}<br>output {<br>  elasticsearch {<br>    index =&gt; “his”<br>    hosts =&gt; [“localhost:9200”]<br>  }<br>  stdout { codec =&gt; rubydebug }<br>}</p>
<p>然后开始报错:<br>[2020-01-09T22:02:19,614][ERROR][logstash.inputs.jdbc     ][main] Java::ComMysqlJdbcExceptionsJdbc4::CommunicationsException: Communications link failure</p>
<p>The last packet successfully received from the server was 60,296 milliseconds ago.  The last packet sent successfully to the server was 60,108 milliseconds ago.: SELECT count(*) AS <code>count</code> FROM (SELECT inout_id,vehicle_no,create_time from a) AS <code>t1</code> LIMIT 1<br>[2020-01-09T22:02:19,666][WARN ][logstash.inputs.jdbc     ][main] Exception when executing JDBC query {:exception=&gt;#&lt;Sequel::DatabaseDisconnectError: Java::ComMysqlJdbcExceptionsJdbc4::CommunicationsException: Communications link failure</p>
<p>The last packet successfully received from the server was 60,296 milliseconds ago.  The last packet sent successfully to the server was 60,108 milliseconds ago.&gt;}<br>[2020-01-09T22:02:20,206][INFO ][logstash.runner          ] Logstash shut down.</p>
<p>一开始以为是参数设置问题,加上各种timeout设置也不起作用.<br>这里有个重点是分页logstash默认走的是子查询查到一个总和,如下所示:<br>select count(<em>) as <code>count</code> from (select </em> from a)<br>但是这要求把全部数据先查一遍.怀疑这里造成性能损耗太大.<br>于是换了个思路,改一下logstash-jdbc源码把……加个专门查总数查询的配置.<br>那因为logstash是juby的,那咱就先装ruby.<br>先安装rvm:<br>curl -sSL <a href="https://get.rvm.io" target="_blank" rel="external">https://get.rvm.io</a> | bash -s stable<br>logstash 必须安装jruby!!<br>安装jruby:<br>rvm install jruby<br>使用rvm的时候要先执行<br>source /etc/profile.d/rvm.sh<br>安装 bundle:<br>gem install bundler<br>这里有至少两个坑:<br>1、安装bundler的时候要注意给/usr/local/rvm以ruby当前用户的权限 chown和chgrp 要不然权限会有问题.<br>另外要随时跟进安装错误日志<br>比如期间遇到过git clone jruby hang住了,那就手动先git clone 到指定目录再执行 安装jruby.</p>
<p>2、网络!<br>装的时候有时需要走ss代理,有时不需要.需要多试试<br>比如步骤1的时候手动clone jruby然后 mvn build的时候,可以用ss代理提速<br>3、安装jruby的时候报错:<br>root@slave-node:~/logstash-input-jdbc# rvm install jruby<br>jruby-9.2.6.0 - #removing src/jruby-9.2.6.0..<br>Searching for binary rubies, this might take some time.<br>Found remote file <a href="https://repo1.maven.org/maven2/org/jruby/jruby-dist/9.2.6.0/jruby-dist-9.2.6.0-bin.tar.gz" target="_blank" rel="external">https://repo1.maven.org/maven2/org/jruby/jruby-dist/9.2.6.0/jruby-dist-9.2.6.0-bin.tar.gz</a><br>Checking requirements for ubuntu.<br>Requirements installation successful.<br>jruby-9.2.6.0 - #configure<br>jruby-9.2.6.0 - #download<br>  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current<br>                                 Dload  Upload   Total   Spent    Left  Speed<br>100 22.9M  100 22.9M    0     0  24821      0  0:16:09  0:16:09 –:–:– 25820<br>No checksum for downloaded archive, recording checksum in user configuration.<br>jruby-9.2.6.0 - #validate archive<br>jruby-9.2.6.0 - #extract<br>jruby-9.2.6.0 - #validate binary<br>jruby-9.2.6.0 - #setup<br>jruby-9.2.6.0 - #gemset created /usr/local/rvm/gems/jruby-9.2.6.0@global<br>jruby-9.2.6.0 - #importing gemset /usr/local/rvm/gemsets/jruby/global.gems.<br>Error running ‘gemset_import_list jruby-openssl jruby-launcher gem-wrappers rubygems-bundler rake rvm bundler’,<br>please read /usr/local/rvm/log/1578889834_jruby-9.2.6.0/gemsets.import.global.log<br>打开本地日志文件如下:<br>java.lang.LayerInstantiationException: Package jdk.internal.jrtfs in both module jrt.fs and module java.base<br>本人用的是jdk13,把jdk13下的jrt-fs.jar删掉后解决<br>4、安装jury的时候,报如下错误:<br>ERROR:  While executing gem … (Gem::Exception)<br>    Failed to find gems [“”] &gt;= 0<br>++ _failed+=(“${_gem} –version ${_version}”)<br>++ read _gem _version _platforms<br>++ ((  1 &gt; 0  ))<br>++ rvm_error ‘\n’\’’command gem pristine –extensions  –version ‘\’’ failed, you need to fix these gems manually.’<br>++ rvm_pretty_print stderr<br>++ case “${rvm_pretty_print_flag:=auto}” in<br>++ case “${TERM:-dumb}” in<br>++ case “$1” in<br>++ [[ -t 2 ]]<br>++ return 1<br>++ printf %b ‘\n’\’’command gem pristine –extensions  –version ‘\’’ failed, you need to fix these gems manually.\n’<br>++ return 1<br>++ return 1</p>
<p>不过我再次输入<br>rvm install jruby的时候提示我已经安装成功了,就没深究这个问题<br>5、删除本地插件的时候要注意执行命令的当前目录<br>必须在 logstash的主目录下!不能在bin下.要不然会报如下错误:<br>ERROR: Operation aborted, cannot remove plugin, message: The path <code>/opt/logstash-7.5.1/bin/logstash-core</code> does not exist.</p>
<p>本地插件编译和安装的步骤如下:<br>1、cd 到jdbc插件的根目录<br>2、切换到ruby的用户,比如我这里是es<br>su -s es<br>4、souce /etc/profile.d/rvm.sh<br>5、项目安装依赖</p>
<p>bundle install<br>这里安装依赖的时候 我忘了是不是要用ss代理了………<br>安装的时候如果有失败的情况会提示你做一些操作,比如:<br>URI::InvalidURIError: can not set user with opaque<br>An error occurred while installing rake (13.0.1), and Bundler<br>cannot continue.<br>Make sure that <code>gem install rake -v &#39;13.0.1&#39; --source &#39;https://rubygems.org/&#39;</code><br>succeeds before bundling.</p>
<p>In Gemfile:<br>  logstash-devutils was resolved to 1.3.6, which depends on<br>    rake</p>
<p>那就执行一下gem install rake -v ‘13.0.1’ –source ‘<a href="https://rubygems.org/" target="_blank" rel="external">https://rubygems.org/</a> 就好了<br>6、构建插件gem (在项目主目录下产生logstash-input-jdbc-4.3.19.gem文件)</p>
<p>gem build logstash-input-jdbc.gemspec</p>
<p>7、logstash卸载logstash-input-jdbc插件</p>
<p>/bin/logstash-plugin uninstall logstash-input-jdbc<br>8、删除<br>rm -rf /opt/logstash-7.5.1/vendor/cache/logstash-filter-geoip-6.0.3-java.gem<br>9、logstash安装本地插件 (先将构建成功的gem文件拷贝至logstash主目录的trade_gem文件夹中)</p>
<p>/bin/logstash-plugin install –no-verify –local ../logstash-input-jdbc-4.3.19.gem</p>
<p>优化后的配置文件如下:<br>input {<br>jdbc {<br>    jdbc_driver_library =&gt; “/opt/logstash-7.5.1/lib/mysql-connector-java-5.1.44.jar”<br>    jdbc_driver_class =&gt; “com.mysql.jdbc.Driver”<br>    jdbc_connection_string =&gt; “jdbc:mysql://xxxxxx:33063/database?autoReconnect=true&amp;failOverReadOnly=false&amp;maxReconnects=10&amp;zeroDateTimeBehavior=convertToNull”<br>    jdbc_user =&gt; “username”<br>    jdbc_password =&gt; “password”<br>    jdbc_page_size =&gt; 1000<br>    jdbc_validate_connection =&gt; true<br>    jdbc_pool_timeout =&gt; 5<br>    jdbc_validation_timeout =&gt; 10<br>    jdbc_fetch_size =&gt; 1000<br>    connection_retry_attempts_wait_time =&gt; 5<br>    jdbc_paging_enabled =&gt; true<br>    subquery_paging_enabled =&gt; true<br>    sum_statement =&gt; “select count(1) as sum from his”<br>    statement =&gt; “SELECT * from his_shadow where id &gt;= (select id from his order by id limit :data_offset,1) limit :jdbc_page_size”<br>}<br>}<br>output {<br>  elasticsearch {<br>    index =&gt; “_his”<br>    hosts =&gt; [“localhost:9200”]<br>  }<br>  stdout { codec =&gt; rubydebug }<br>}</p>
<p>参考资料:</p>
<p><a href="https://www.jianshu.com/p/84e807e1a575" target="_blank" rel="external">https://www.jianshu.com/p/84e807e1a575</a><br>感谢前人无私的分享,新版本的logstash代码接口已经有变化了这里附上新版本的代码如下:<br><a href="/2020/01/15/how-to-deploy-logstash-plugin-env/statement_handler.rb" title="statement_handler.rb">statement_handler.rb</a><br><br>完整工程如下:</p>
<a href="/2020/01/15/how-to-deploy-logstash-plugin-env/logstash-input-jdbc_youhua.zip" title="logstash-input-jdbc_youhua.zip">logstash-input-jdbc_youhua.zip</a>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2020/01/15/how-to-deploy-logstash-plugin-env/" data-id="clypv5bip000f6y794ddblouh" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>


  
    <article id="post-k8s-install" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2019/11/11/k8s-install/" class="article-date">
  <time datetime="2019-11-11T13:43:45.000Z" itemprop="datePublished">2019-11-11</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2019/11/11/k8s-install/">k8s安装</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>今天遇到了k8s 在ubuntu18.04下安装的时候进行kubeadm init的时候拉取image失败的问题，<br>而且我设置了shadowshocket的代理http_proxy和https_proxy依然不行，<br>后来指定镜像库解决。命令如下：<br>kubeadm init –image-repository registry.aliyuncs.com/google_containers –ignore-preflight-errors=Swap<br>收官</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2019/11/11/k8s-install/" data-id="clypv5bit000m6y79lpou7ra1" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/k8s/">k8s</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-tracingSystem" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2019/05/05/tracingSystem/" class="article-date">
  <time datetime="2019-05-05T11:29:34.000Z" itemprop="datePublished">2019-05-05</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2019/05/05/tracingSystem/">链路追踪系统之选型篇</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>Distributed Tracing最早起源于google的一片论文（链接见文末），之后各家都退出了自己的监控系统<br>但是数据格式并不统一，所以发起了open tracing标准，现在skywalking、zipkin和jaeger都遵循这一协议。<br>本篇是对近期链路跟踪系统选型的汇总。<br>主要选型对象有如下几个：zipkin、skywalking、CAT和pinpoint。<br>CAT的代码侵入性最大，并且过于臃肿可以reject，不过CAT的分析系统做的不错，可以借鉴一下。<br>pinpoint是naver出产的，团队规模小，并且需要hbase，韩国出品，所以也reject。<br>这里主要讨论zipkin与skywalking<br>zipkin架构图如下：<br><img src="/2019/05/05/tracingSystem/zipkin_architecture.png" alt="zipkin_overview" title="zipkin_overview"><br>在这里解读一下上图各个部分。<br>首先instrumented client 表示是经过扩展的客户端<br>Non-instrumented client 表示未经过扩展的客户端<br>instrumented client的一个请求叫做Span，一个完整调用链叫trace。<br>上图首先从instrumented client发送一个span到transport然后到collect，最后落盘存储用于ui展示。<br>发送到Non-instrumented client 并不会给collect发送记录存储。</p>
<p>zipkin主要有两个工程：<br><a href="https://github.com/openzipkin/brave" target="_blank" rel="external">https://github.com/openzipkin/brave</a><br><a href="https://github.com/apache/incubator-zipkin" target="_blank" rel="external">https://github.com/apache/incubator-zipkin</a><br>brave主要是zipkin的外围扩展。<br>现在主要支持一下扩展：<br><img src="/2019/05/05/tracingSystem/zipkin_instrument.png" alt="zipkin_instrument" title="zipkin_instrument"></p>
<p>ncubator-zipkin是zipkin的服务端。<br>包含如下部分：<br>server：服务端程序。<br>lens：web ui<br>storage：将数据存盘，现在支持mysql、cassandra和es<br>collect：收集器，包括三种收集模式：rest、kafka和rabbitmq。<br>正常来说应该使用mq异步的collect这样对程序本身性能影响最小。<br>zipkin的扩展是通过有针对性的埋点来实现的。<br>比如对于rest的扩展是通过spring的resttemplate拦截器在header里传traceid和spanid来实现的。<br>对于kafka消息的扩展也是在header里加参数来实现的。<br>所以zipkin本身依赖于第三方目标服务是否具有扩展性。<br>侵入性来说需要添加配置。</p>
<p>下面说说skywalking<br><img src="/2019/05/05/tracingSystem/skywalking.jpeg" alt="skywalking" title="skywalking"><br>首先tracing（跟踪）也好，Metric（指标）也罢，都是通过http或者grpc将采集起来的数据传送到服务器端的。<br>其数据存储包括mysql、es、tidb、h2和sharding<br>因为skywalking是通过AOP的方式动态织入代码的，所以并不受限于api扩展。<br>以下是已有的扩展：<br><img src="/2019/05/05/tracingSystem/skywalking_plugin1.png" alt="skywalking_plu1" title="skywalking_plu1"><br>&amp;<br><img src="/2019/05/05/tracingSystem/skywalking_plugin2.png" alt="skywalking plu2" title="skywalking plu2"><br>而且skywalking对于代码完全没有任何侵入性。<br>缺点是与我司运维人员了解到有两个问题：<br>1、存在es中的数据无法删除<br>2、如果es重启的话，必须所有agent都重启完毕，skywalking的监控上才会有数值。<br>这个要跟一下具体问题。</p>
<p>综合总结一下如下：<br>zipkin是基于api调用的，skywalking是基于AOP的。扩展性上讲skywalking要优于zipkin。<br>代码复杂度上来说，zipkin更容易上手一些，代码量也少。<br>性能上来说skywalking比zipkin要好。</p>
<p>相关zipkin与skywalking的理论基础如下：<br><a href="https://bigbully.github.io/Dapper-translation/" target="_blank" rel="external">https://bigbully.github.io/Dapper-translation/</a></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2019/05/05/tracingSystem/" data-id="clypv5bjq00106y795soya3bn" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/zipkin-skywalking-cat/">zipkin skywalking cat</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-presto-source-code" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2019/01/07/presto-source-code/" class="article-date">
  <time datetime="2019-01-07T14:40:06.000Z" itemprop="datePublished">2019-01-07</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2019/01/07/presto-source-code/">码林啄木之presto阅读理解</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>Day 1:<br>一般来讲，我们要阅读一个项目的源代码，首先要从网上找一下有没有读过该项目，次之去官网上找文档，最后再读细节。<br>我们第一天先讲讲presto的启动命令、架构以及代码架构。<br>首先祭出架构图：摘自官网<br><img src="/2019/01/07/presto-source-code/presto-overview.png" alt="overview" title="overview"><br>也就是说presto主要有如下几个部分组成：<br>CLI：就是命令行，用于提交查询，通俗点的理解就是发起rest请求<br>服务器端有两种节点：coordinators和worker。一下简称C节点和W节点<br>首先来看C节点：<br>C节点主要用于解析语句，生成请求接话，管理W节点。<br>W节点用于执行请求计划，处理数据，W节点从连接器读取数据并彼此交换数据。<br>C节点与W节点通过rest API来通信。<br>以上就是这个overview图的最直白简洁。<br>然鹅还有一些关键概念。<br>一、presto链接数据源的时候需要什么参数？<br>1、Connector连接器，连接器顾名思义，作用相当于jdbc中的driver。<br>2、Catalog，一个数据源就是catalog，可以是mysql，可以是hive等等<br>3、Schema，类似mysql中的database，一个catalog可以有多个schema<br>4、Table，就是表！<br>二、presto的查询模型<br>1、statement<br>语句，就是sql语句。是一个静态的概念。<br>2、query<br>查询，通过语句生成的query，是一个动态的执行。<br>3、stage<br>一个stage代表查询执行计划的一个部分。<br>stage有四种类型：<br>Coordinator_Only:<br>用于执行DDM或者DDL表的创建与更改<br>Single:<br>这种类型用于聚合子Stage的输出数据<br>Fixed:<br>用于接受其子Stage产生的数据并在集群中对这些数据进行分布式的聚合或者分组计算。<br>Source:<br>用于直接链接数据源，从数据源读取数据，在读取数据的时候，该阶段也会根据presto对查询执行计划的优化完成相关的断言下发和条件过滤等<br>Exchange：<br>交换，stage之间用于数据交换用的。场景包括下游从上游读取数据。<br>Task:<br>就是stage切分出来运行在W节点上的具体任务。<br>Split：<br>就是数据集上的子集。下文说道的driver就是在split上做操作<br>Driver：<br>一个task包括一个或者多个Driver。就是一个split的一系列operator集合。<br>Page是presto中处理的最小数据单元。一个Page对象包含多个Block对象，而每个Block对象是一个字节数组，存储一个字段的若干行。多个block横切的一行是真实的一行数据。</p>
<p>以上，就是我们对于presto源码阅读理解的重点喽</p>
<p>再来看看presto的启动命令吧<br>首先是C节点：<br>-Dnode.data-dir=/mnt/disk1/log/presto -Dnode.id=xxxxxxxxxxx -Dnode.environment=production -Dlog.enable-console=false -Dlog.levels-file=/etc/ecm/presto-conf-0.188/log.properties -Dconfig=/etc/ecm/presto-conf-0.188/config.properties com.facebook.presto.server.PrestoServer<br>然后是W节点<br>-Dnode.data-dir=/mnt/disk1/log/presto -Dnode.id=xxxxxxx -Dnode.environment=production -Dlog.enable-console=false -Dlog.levels-file=/etc/ecm/presto-conf-0.188/log.properties -Dconfig=/etc/ecm/presto-conf-0.188/config.properties com.facebook.presto.server.PrestoServer<br>命令是一样滴哦，惊不惊喜，意不意外！<br>差别就在properties里了<br>里面有个coordinator属性为true则为C节点，false为W节点<br>本来想今天把服务跑起来看看的，结果意外了。<br>连续两次意外了。<br>第一次的时候发现需要装antlr插件也就是g4插件。。。。<br>第二次又意外了 jdk版本不够。。。<br>Presto requires Java 8u151+ (found 1.8.0_144)</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2019/01/07/presto-source-code/" data-id="clypv5bji000v6y791lh6bua5" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>


  


  <nav id="page-nav">
    <span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><a class="page-number" href="/page/3/">3</a><a class="extend next" rel="next" href="/page/2/">__('next') &raquo;</a>
  </nav>
</section>
        
          <aside id="sidebar">
  
    

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tags</h3>
    <div class="widget">
      <ul class="tag-list"><li class="tag-list-item"><a class="tag-list-link" href="/tags/bigdata/">bigdata</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/flume/">flume</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/influx-proxy/">influx-proxy</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/k8s/">k8s</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/openstack-neutron/">openstack neutron</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/spark-classload-ClassNotFound/">spark classload ClassNotFound</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/spring/">spring</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/spring-eureka/">spring eureka</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/tensroflow/">tensroflow</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/zipkin-skywalking-cat/">zipkin skywalking cat</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/阿里云-sgoop-presto/">阿里云 sgoop presto</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tag Cloud</h3>
    <div class="widget tagcloud">
      <a href="/tags/bigdata/" style="font-size: 10px;">bigdata</a> <a href="/tags/flume/" style="font-size: 10px;">flume</a> <a href="/tags/influx-proxy/" style="font-size: 10px;">influx-proxy</a> <a href="/tags/k8s/" style="font-size: 10px;">k8s</a> <a href="/tags/openstack-neutron/" style="font-size: 10px;">openstack neutron</a> <a href="/tags/spark-classload-ClassNotFound/" style="font-size: 10px;">spark classload ClassNotFound</a> <a href="/tags/spring/" style="font-size: 20px;">spring</a> <a href="/tags/spring-eureka/" style="font-size: 10px;">spring eureka</a> <a href="/tags/tensroflow/" style="font-size: 10px;">tensroflow</a> <a href="/tags/zipkin-skywalking-cat/" style="font-size: 10px;">zipkin skywalking cat</a> <a href="/tags/阿里云-sgoop-presto/" style="font-size: 10px;">阿里云 sgoop presto</a>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2024/07/">July 2024</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2021/05/">May 2021</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/04/">April 2020</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/01/">January 2020</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/11/">November 2019</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/05/">May 2019</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/01/">January 2019</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/12/">December 2018</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/10/">October 2018</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/09/">September 2018</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/05/">May 2017</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/12/">December 2016</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/11/">November 2016</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2024/07/17/hello-world/">如何搭建新版本hexo</a>
          </li>
        
          <li>
            <a href="/2024/07/17/Doris-begin/">Doris使用入门</a>
          </li>
        
          <li>
            <a href="/2021/05/28/influx-proxy-problemlist/">influx-proxy部署遇到的问题</a>
          </li>
        
          <li>
            <a href="/2020/04/25/build-flume-ha/">构建flume高可用集群</a>
          </li>
        
          <li>
            <a href="/2020/04/14/firecracker-introduction/">firecracker介绍</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2024 wfliu<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>
    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    

<script src="//ajax.googleapis.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>


  <link rel="stylesheet" href="/fancybox/jquery.fancybox.css">
  <script src="/fancybox/jquery.fancybox.pack.js"></script>


<script src="/js/script.js"></script>

  </div>
</body>
</html>