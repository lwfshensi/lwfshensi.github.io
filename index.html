<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  <title>肆叔的酱油</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta property="og:type" content="website">
<meta property="og:title" content="肆叔的酱油">
<meta property="og:url" content="http://yoursite.com/index.html">
<meta property="og:site_name" content="肆叔的酱油">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="肆叔的酱油">
  
    <link rel="alternate" href="/atom.xml" title="肆叔的酱油" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  
  
    <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <link rel="stylesheet" href="/css/style.css">
  

</head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">肆叔的酱油</a>
      </h1>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="Search"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" results="0" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://yoursite.com"></form>
      </div>
    </div>
  </div>
</header>
      <div class="outer">
        <section id="main">
  
    <article id="post-hello-world" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2020/04/25/hello-world/" class="article-date">
  <time datetime="2020-04-25T13:18:53.920Z" itemprop="datePublished">2020-04-25</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2020/04/25/hello-world/">如何搭建新版本hexo</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>我先是在debian8.6上用官网的方式搭建的，失败～<br>然后用aptget 的方式安装nodejs 再装hexo还是失败～<br>最后我开了一个ubuntu14.04的vm，在里面执行<br>apt-get install nodejs npm<br>apt-get install nodejs-legacy<br>sudo npm install -g hexo-cli</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2020/04/25/hello-world/" data-id="ck9fngvdt000efm79adzl9ne5" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>


  
    <article id="post-build-flume-ha" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2020/04/25/build-flume-ha/" class="article-date">
  <time datetime="2020-04-25T12:55:43.000Z" itemprop="datePublished">2020-04-25</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2020/04/25/build-flume-ha/">构建flume高可用集群</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <img src="/2020/04/25/build-flume-ha/flume-ha.png" alt="flume-ha" title="flume-ha">
<p>服务器端配置如下:</p>
<figure class="highlight plain"><figcaption><span>shell config</span></figcaption><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div></pre></td><td class="code"><pre><div class="line"></div><div class="line">#set Agent name</div><div class="line">agent.sources = r1</div><div class="line">agent.channels = c1</div><div class="line">agent.sinks = k1</div><div class="line">#set channel</div><div class="line">agent.channels.c1.type = memory</div><div class="line">agent.channels.c1.capacity = 1024000</div><div class="line">agent.channels.c1.transactionCapacity = 10000</div><div class="line">agent.channels.c1.byteCapacity=134217728</div><div class="line">agent.channels.c1.byteCapacityBufferPercentage=80</div><div class="line"></div><div class="line"># other node,nna to nns</div><div class="line">agent.sources.r1.type = avro</div><div class="line">agent.sources.r1.bind = &#123;localip&#125;</div><div class="line">agent.sources.r1.port = 52020</div><div class="line">agent.sources.r1.channels = c1</div><div class="line"></div><div class="line">#set sink to hdfs</div><div class="line">agent.sinks.k1.channel = c1</div><div class="line">agent.sinks.k1.type = org.apache.flume.sink.kafka.KafkaSink</div><div class="line">agent.sinks.k1.brokerList = &#123;kafkaIp&#125;:9092</div><div class="line">agent.sinks.k1.topic = flume-test</div><div class="line">agent.sinks.k1.serializer.class = kafka.serializer.StringEncoder</div></pre></td></tr></table></figure>
<p>flume自带load_banlance与fail over<br>先说load_banlance发送端配置如下:<br><figure class="highlight plain"><figcaption><span>shell config</span></figcaption><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div></pre></td><td class="code"><pre><div class="line">#agent1 name</div><div class="line">agent.channels = c1</div><div class="line">agent.sources = r1</div><div class="line">agent.sinks = k1 k2 k3</div><div class="line">#set gruop</div><div class="line">agent.sinkgroups = g1</div><div class="line">#set channel</div><div class="line">agent.channels.c1.type = memory</div><div class="line">agent.channels.c1.capacity = 102400</div><div class="line">agent.channels.c1.transactionCapacity = 1000</div><div class="line">agent.channels.c1.byteCapacity=134217728</div><div class="line">agent.channels.c1.byteCapacityBufferPercentage=80</div><div class="line"></div><div class="line">agent.sources.r1.channels = c1</div><div class="line"></div><div class="line">######## source相关配置 ########</div><div class="line"># source类型</div><div class="line">agent.sources.r1.type = TAILDIR</div><div class="line"># 元数据位置</div><div class="line">agent.sources.r1.positionFile = /root/taildir_position.json</div><div class="line"># 监控的目录</div><div class="line">agent.sources.r1.filegroups = f1</div><div class="line">agent.sources.r1.filegroups.f1=/root/.*log</div><div class="line">agent.sources.r1.fileHeader = true</div><div class="line"></div><div class="line"></div><div class="line"></div><div class="line"># set sink1</div><div class="line">agent.sinks.k1.channel = c1</div><div class="line">agent.sinks.k1.type = avro</div><div class="line">agent.sinks.k1.hostname = &#123;node1&#125;</div><div class="line">agent.sinks.k1.port = 52020</div><div class="line"># set sink2</div><div class="line">agent.sinks.k2.channel = c1</div><div class="line">agent.sinks.k2.type = avro</div><div class="line">agent.sinks.k2.hostname = &#123;node2&#125;</div><div class="line">agent.sinks.k2.port = 52020</div><div class="line"># set sink3</div><div class="line">agent.sinks.k3.channel = c1</div><div class="line">agent.sinks.k3.type = avro</div><div class="line">agent.sinks.k3.hostname = &#123;node3&#125;</div><div class="line">agent.sinks.k3.port = 52020</div><div class="line">#set sink group</div><div class="line">agent.sinkgroups.g1.sinks = k1 k2 k3</div><div class="line">#set load_banlance</div><div class="line">agent.sinkgroups.g1.processor.type = load_balance</div><div class="line">agent.sinkgroups.g1.processor.backoff=true   # 这个里面是个惩罚机制</div><div class="line">agent.sinkgroups.g1.processor.selector=round_robin</div></pre></td></tr></table></figure></p>
<p>load_banlance:<br>1.当三台主机中的一台进程挂掉后，发送端会报错误，但是不影响数据的发送。<br>2.当挂掉的进程回复后，会继续被接受数据。这里面有个惩罚机制，如果开启的失败的接受端会被等待延迟！<br>3.当挂掉的进程回复后，发送端会自动添加会发送列表里，报错消失！<br>4.这个就是一个简单的均衡方式，相当于见了一个sink组，在sink组中的数据被轮询发送！</p>
<p>再说fail over配置:<br>服务器端不变<br>发送端配置改为:<br><figure class="highlight plain"><figcaption><span>shell config</span></figcaption><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div></pre></td><td class="code"><pre><div class="line"></div><div class="line">#agent1 name</div><div class="line">agent.channels = c1</div><div class="line">agent.sources = r1</div><div class="line">agent.sinks = k1 k2 k3</div><div class="line">#set gruop</div><div class="line">agent.sinkgroups = g1</div><div class="line">#set channel</div><div class="line">agent.channels.c1.type = memory</div><div class="line">agent.channels.c1.capacity = 102400</div><div class="line">agent.channels.c1.transactionCapacity = 1000</div><div class="line">agent.channels.c1.byteCapacity=134217728</div><div class="line">agent.channels.c1.byteCapacityBufferPercentage=80</div><div class="line"></div><div class="line">agent.sources.r1.channels = c1</div><div class="line"></div><div class="line">######## source相关配置 ########</div><div class="line"># source类型</div><div class="line">agent.sources.r1.type = TAILDIR</div><div class="line"># 元数据位置</div><div class="line">agent.sources.r1.positionFile = /root/taildir_position.json</div><div class="line"># 监控的目录</div><div class="line">agent.sources.r1.filegroups = f1</div><div class="line">agent.sources.r1.filegroups.f1=/root/.*log</div><div class="line">agent.sources.r1.fileHeader = true</div><div class="line"></div><div class="line"></div><div class="line"></div><div class="line"># set sink1</div><div class="line">agent.sinks.k1.channel = c1</div><div class="line">agent.sinks.k1.type = avro</div><div class="line">agent.sinks.k1.hostname = &#123;node1&#125;</div><div class="line">agent.sinks.k1.port = 52020</div><div class="line"># set sink2</div><div class="line">agent.sinks.k2.channel = c1</div><div class="line">agent.sinks.k2.type = avro</div><div class="line">agent.sinks.k2.hostname = &#123;node2&#125;</div><div class="line">agent.sinks.k2.port = 52020</div><div class="line"># set sink3</div><div class="line">agent.sinks.k3.channel = c1</div><div class="line">agent.sinks.k3.type = avro</div><div class="line">agent.sinks.k3.hostname = &#123;node3&#125;</div><div class="line">agent.sinks.k3.port = 52020</div><div class="line">#set sink group</div><div class="line">agent.sinkgroups.g1.sinks = k1 k2 k3</div><div class="line">#set failover</div><div class="line">agent.sinkgroups.g1.processor.type = failover</div><div class="line">agent.sinkgroups.g1.processor.backoff=true   # 这个里面是个惩罚机制</div><div class="line">agent.sinkgroups.g1.processor.selector=round_robin</div><div class="line">agent.sinkgroups.g1.processor.priority.k1 = 30</div><div class="line">agent.sinkgroups.g1.processor.priority.k2 = 20</div><div class="line">agent.sinkgroups.g1.processor.priority.k3 = 10</div><div class="line">agent.sinkgroups.g1.processor.maxpenalty = 10000</div></pre></td></tr></table></figure></p>
<p>区别在于负载均衡，每台机器都会受到数据处理数据，但是有时候业务需求可能需要一台机器为主要一条业务的数据接收器，另一个台在挂掉后才会被启用，这时候这个就有用了。</p>
<p>还有一种玩法可以在多个flume前面加个nginx<br><img src="/2020/04/25/build-flume-ha/flume-ha2.png" alt="flume-nginx" title="flume-nginx"></p>
<p>ng配置如下:<br>首先在nginx.conf中加入:</p>
<p>include       flume.conf;</p>
<p>加入后配置如下:<br><figure class="highlight plain"><figcaption><span>shell config</span></figcaption><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div></pre></td><td class="code"><pre><div class="line"></div><div class="line">#user  nobody;</div><div class="line">worker_processes  1;</div><div class="line"></div><div class="line">#error_log  logs/error.log;</div><div class="line">#error_log  logs/error.log  notice;</div><div class="line">#error_log  logs/error.log  info;</div><div class="line"></div><div class="line">#pid        logs/nginx.pid;</div><div class="line"></div><div class="line"></div><div class="line">events &#123;</div><div class="line">    worker_connections  1024;</div><div class="line">&#125;</div><div class="line">include       flume.conf;</div><div class="line"></div><div class="line"></div><div class="line">http &#123;</div><div class="line"></div><div class="line"></div><div class="line">...</div><div class="line">&#125;</div><div class="line"></div></pre></td></tr></table></figure></p>
<p>新建flume.conf</p>
<p>配置如下:<br><figure class="highlight plain"><figcaption><span>shell config</span></figcaption><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div></pre></td><td class="code"><pre><div class="line"></div><div class="line">stream &#123;</div><div class="line">      upstream abcdocker &#123;</div><div class="line">        server node1:52020  weight=5 max_fails=3 fail_timeout=30s;</div><div class="line">        server node2:52020  weight=5 max_fails=3 fail_timeout=30s;</div><div class="line">        server node3:52020  weight=5 max_fails=3 fail_timeout=30s;</div><div class="line"></div><div class="line">     &#125;</div><div class="line"></div><div class="line">     server &#123;</div><div class="line">            listen 52021;</div><div class="line">            proxy_pass abcdocker;</div><div class="line">            proxy_connect_timeout 10s;</div><div class="line">            proxy_timeout 24h;</div><div class="line">              &#125;</div><div class="line">  &#125;</div><div class="line"></div></pre></td></tr></table></figure></p>
<p>flume发送端如下:<br><figure class="highlight plain"><figcaption><span>shell config</span></figcaption><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div></pre></td><td class="code"><pre><div class="line"></div><div class="line">#agent1 name</div><div class="line">agent.channels = c1</div><div class="line">agent.sources = r1</div><div class="line">agent.sinks = k1</div><div class="line">#set channel</div><div class="line">agent.channels.c1.type = memory</div><div class="line">agent.channels.c1.capacity = 102400</div><div class="line">agent.channels.c1.transactionCapacity = 1000</div><div class="line">agent.channels.c1.byteCapacity=134217728</div><div class="line">agent.channels.c1.byteCapacityBufferPercentage=80</div><div class="line"></div><div class="line">agent.sources.r1.channels = c1</div><div class="line"></div><div class="line">######## source相关配置 ########</div><div class="line"># source类型</div><div class="line">agent.sources.r1.type = TAILDIR</div><div class="line"># 元数据位置</div><div class="line">agent.sources.r1.positionFile = /root/taildir_position.json</div><div class="line"># 监控的目录</div><div class="line">agent.sources.r1.filegroups = f1</div><div class="line">agent.sources.r1.filegroups.f1=/root/.*log</div><div class="line">agent.sources.r1.fileHeader = true</div><div class="line"></div><div class="line"></div><div class="line"></div><div class="line"># set sink1</div><div class="line">agent.sinks.k1.channel = c1</div><div class="line">agent.sinks.k1.type = avro</div><div class="line">agent.sinks.k1.hostname = &#123;nginx&#125;</div><div class="line">agent.sinks.k1.port = 52021</div><div class="line"></div></pre></td></tr></table></figure></p>
<p>如果要造数据的话可以这样建个sh文件<br><figure class="highlight plain"><figcaption><span>shell config</span></figcaption><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"></div><div class="line">#!/bin/bash</div><div class="line">while true</div><div class="line">do</div><div class="line">        date &gt;&gt; test.log</div><div class="line">        sleep 0.5</div><div class="line">done</div></pre></td></tr></table></figure></p>
<p>执行即可</p>
<p>最后兵无常势水无常形,多思考用最适合自己的方案即可.</p>
<p>参考资料:<br><a href="https://blog.csdn.net/zengxianglei/article/details/89290050" target="_blank" rel="external">https://blog.csdn.net/zengxianglei/article/details/89290050</a><br><a href="https://www.cnblogs.com/chushiyaoyue/articles/6211599.html" target="_blank" rel="external">https://www.cnblogs.com/chushiyaoyue/articles/6211599.html</a></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2020/04/25/build-flume-ha/" data-id="ck9fngvcz0003fm79m7yi7ffg" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/flume/">flume</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-firecracker-introduction" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2020/04/14/firecracker-introduction/" class="article-date">
  <time datetime="2020-04-13T18:10:23.000Z" itemprop="datePublished">2020-04-14</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2020/04/14/firecracker-introduction/">firecracker介绍</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>firecracker是aws开源的serverless计算框架.<br>firecracker是基于kvm的micro vm技术,相对于容器隔离醒更好,同时性能更快.<br>仅启动相对较新的 Linux 内核，并且仅启动使用特定配置选项集编译的内核（内核编译配置选项超过 1000 种）。<br>此外，不支持任何类型的图形卡或加速器，不支持硬件透传，不支持（大多数）老旧设备。<br>由于设备模型极简，内核加载过程也简单，可以实现小于 125 ms 的启动时间和更少的内存占用。<br>Firecracker 目前支持 Intel CPU，并将于 2019 年开始支持 AMD 和 ARM，还将与 containerd 等流行的容器运行时集成。</p>
<p>与宿主机的关系:<br><img src="/2020/04/14/firecracker-introduction/host.png" alt="introduce" title="introduce"><br>Firecracker 运行在 Linux 主机上，内核为4.14或更新内核，并且使用 Linux guest OSs (从这一点来说，称为 guest)。 启动该进程后，在发出 instanceart 命令之前，用户与 Firecracker API 交互以配置 microVM。</p>
<p>Firecracker 内部架构<br><img src="/2020/04/14/firecracker-introduction/org.png" alt="introduce" title="introduce"><br>每个 Firecracker 进程封装一个且只有一个 microVM。 该进程运行以下线程: API、 VMM 和 vCPU。 Api 线程负责 Firecracker 的 API 服务器和相关的控制平面。 它永远不会在虚拟机的快速路径上。 Vmm 线程公开机器模型、最小遗留设备模型、 microVM 元数据服务(MMDS)和 VirtIO 设备仿真 Net 和 Block 设备，并提供 i / o 速率限制。 除此之外，还有一个或多个 vCPU 线程(每个客户 CPU 核心一个)。 它们是通过 KVM 创建的，并运行 KVM run 主循环。 它们在设备模型上执行同步 i / o 和存储器映射输入输出操作。</p>
<p>Firecracker如何工作<br><img src="/2020/04/14/firecracker-introduction/work.png" alt="introduce" title="introduce"><br>Firecracker 在用户空间中运行，使用基于 Linux 内核的虚拟机（KVM）来创建 microVM。每个 microVM 的快速启动时间和低内存开销使你可将数千个 microVM 打包到同一台机器上。这意味着每个函数或容器组都可以使用虚拟机屏障进行封装，从而使不同用户的工作负载能在同一台计算机上运行，而无需在安全性和效率之间进行权衡。Firecracker 是 QEMU 的替代品，QEMU 是一个成熟的 VMM，具有通用和广泛的功能集，可以托管各种客户操作系统。</p>
<p>可以通过 RESTful API 控制 Firecracker 进程，RESTful API 可以启用常见操作：例如配置 vCPU 数量或启动计算机。Firecracker 提供内置速率限制器，可精确控制同一台计算机上数千个 microVM 使用的网络和存储资源。你可以通过 Firecracker API 创建和配置速率限制器，并灵活定义速率限制器来支持突发情况或特定带宽 / 操作限制。Firecracker 还提供元数据服务，可在主机和客户机操作系统之间安全地共享配置信息。元数据服务可以使用 Firecracker API 设置。</p>
<p>Firecracker入门实例<br>1、找个linux电脑并且安装依赖<br>sudo apt update<br>sudo apt install qemu qemu-kvm libvirt-bin bridge-utils virt-manager<br>2、开启cpu虚拟化<br>开机进入bios后<br>Configuratio &gt; Intel Virtual Technology &gt; Enabled<br>3、根据官网进行安装<br>latest=$(basename $(curl -fsSLI -o /dev/null -w  %{url_effective}  <a href="https://github.com/firecracker-microvm/firecracker/releases/latest" target="_blank" rel="external">https://github.com/firecracker-microvm/firecracker/releases/latest</a>))<br>curl -LOJ <a href="https://github.com/firecracker-microvm/firecracker/releases/download/${latest}/firecracker-${latest}-$(uname" target="_blank" rel="external">https://github.com/firecracker-microvm/firecracker/releases/download/${latest}/firecracker-${latest}-$(uname</a> -m)<br>mv firecracker-${latest}-$(uname -m) firecracker<br>chmod +x firecracker<br>至此完成安装步骤<br>4、启动服务<br>rm -f /tmp/firecracker.socket<br>./firecracker –api-sock /tmp/firecracker.socket</p>
<p>5、再打开一个窗口进行vm配置工作<br>既然是基于kvm的vm那么一定需要rootfs和kernal.原始文件放在aws的s3存储上但是被墙了..<br>这里找到国内的源如下:<br>wget <a href="https://silenceshell-1255345740.cos.ap-shanghai.myqcloud.com/hello-vmlinux.bin" target="_blank" rel="external">https://silenceshell-1255345740.cos.ap-shanghai.myqcloud.com/hello-vmlinux.bin</a><br>wget <a href="https://silenceshell-1255345740.cos.ap-shanghai.myqcloud.com/hello-rootfs.ext4" target="_blank" rel="external">https://silenceshell-1255345740.cos.ap-shanghai.myqcloud.com/hello-rootfs.ext4</a></p>
<p>通过api配置rootfs与kernal<br>sudo curl –unix-socket /tmp/firecracker.sock -i     -X PUT ‘<a href="http://localhost/boot-source" target="_blank" rel="external">http://localhost/boot-source</a>‘       -H ‘Accept: application/json’               -H ‘Content-Type: application/json’         -d ‘{<br>        “kernel_image_path”: “./hello-vmlinux.bin”,<br>        “boot_args”: “console=ttyS0 reboot=k panic=1 pci=off”<br>    }’</p>
<p> sudo curl –unix-socket /tmp/firecracker.socket -i     -X PUT ‘<a href="http://localhost/drives/rootfs" target="_blank" rel="external">http://localhost/drives/rootfs</a>‘     -H ‘Accept: application/json’               -H ‘Content-Type: application/json’         -d ‘{<br>        “drive_id”: “rootfs”,<br>        “path_on_host”: “./hello-rootfs.ext4”,<br>        “is_root_device”: true,<br>        “is_read_only”: false<br>    }’<br>5、通过api启动vm<br>sudo curl –unix-socket /tmp/firecracker.socket -i     -X PUT ‘<a href="http://localhost/actions" target="_blank" rel="external">http://localhost/actions</a>‘           -H  ‘Accept: application/json’              -H  ‘Content-Type: application/json’        -d ‘{<br>        “action_type”: “InstanceStart”<br>     }’</p>
<p>6、最后切换到启动服务的窗口,这时候应该有了登录窗口了,用户名密码默认是root/root<br><img src="/2020/04/14/firecracker-introduction/login.png" alt="introduce" title="introduce"></p>
<p>7、网络配置<br>首先创建tap设备:<br>sudo ip tuntap add tap0 mode tap</p>
<p>如果要开通外网访问可以用nat转发的方式<br>设置一个网段<br>sudo ip addr add 172.16.0.1/24 dev tap0<br>启用tap0<br>并且初始化iptables规则,这里{networkCardName}指的是自己机器上外网网卡的名称<br>sudo ip link set tap0 up<br>sudo sh -c “echo 1 &gt; /proc/sys/net/ipv4/ip_forward”<br>sudo iptables -t nat -A POSTROUTING -o {networkCardName} -j MASQUERADE<br>sudo iptables -A FORWARD -m conntrack –ctstate RELATED,ESTABLISHED -j ACCEPT<br>sudo iptables -A FORWARD -i tap0 -o {networkCardName} -j ACCEPT<br>启动前进行如下配置:<br>curl –unix-socket /tmp/firecracker.socket -i \<br>  -X PUT ‘<a href="http://localhost/network-interfaces/eth0" target="_blank" rel="external">http://localhost/network-interfaces/eth0</a>‘ \<br>  -H ‘Accept: application/json’ \<br>  -H ‘Content-Type: application/json’ \<br>  -d ‘{<br>      “iface_id”: “eth0”,<br>      “guest_mac”: “AA:FC:00:00:00:01”,<br>      “host_dev_name”: “tap0”<br>    }’</p>
<p>启动实例<br>配置实例网络<br>ip addr add 172.16.0.2/24 dev eth0</p>
<p>ip link set eth0 up<br>ip route add default via 172.16.0.1 dev eth0</p>
<p>最后查验一下<br><img src="/2020/04/14/firecracker-introduction/network.png" alt="introduce" title="introduce"></p>
<p>玩完了就清理一下:<br>sudo ip link del tap0<br>sudo iptables -F<br>sudo sh -c “echo 0 &gt; /proc/sys/net/ipv4/ip_forward”</p>
<p>firecreaker给我们提供了一种容器之外的解决方案,同时具备容器的轻量但实际上是vm.我们可以针对vm创建tap网络设备,而且由于是vm从而使其可以具有状态.<br>在物联网领域可以与规则引擎结合处理告警或者复杂逻辑运算.<br>在容器领域也可以与kata结合让容器运行在micro vm中,增强容器.</p>
<p>参考资料:<br><a href="https://www.tuicool.com/articles/memqyaV" target="_blank" rel="external">https://www.tuicool.com/articles/memqyaV</a><br><a href="https://github.com/firecracker-microvm/firecracker/blob/master/docs/getting-started.md" target="_blank" rel="external">https://github.com/firecracker-microvm/firecracker/blob/master/docs/getting-started.md</a><br><a href="https://blog.csdn.net/wangzan18/article/details/104519333" target="_blank" rel="external">https://blog.csdn.net/wangzan18/article/details/104519333</a><br><a href="https://github.com/kata-containers/documentation/issues/351?spm=a2c4e.10696291.0.0.480319a44pfqoF" target="_blank" rel="external">https://github.com/kata-containers/documentation/issues/351?spm=a2c4e.10696291.0.0.480319a44pfqoF</a><br><a href="https://yq.aliyun.com/articles/692117" target="_blank" rel="external">https://yq.aliyun.com/articles/692117</a></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2020/04/14/firecracker-introduction/" data-id="ck9fngvd80007fm79vvu927a1" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>


  
    <article id="post-how-to-unregister-service-from-eureka" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2020/04/13/how-to-unregister-service-from-eureka/" class="article-date">
  <time datetime="2020-04-12T17:40:04.000Z" itemprop="datePublished">2020-04-13</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2020/04/13/how-to-unregister-service-from-eureka/">如何安全优雅的把spring微服务从eureka上注销</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>重启服务的方法有很多种.<br>不推荐使用kill -9 ,粗暴的关闭会造成生产数据丢失.<br>这里研究了一下actuator,使用actuator的/pause,可以让服务从eureka上注销但是服务本身还提供服务,<br>也就是可以等30秒等待服务继续把未完成的工作做完,再kill -9 就可以了<br>spring boot1.x与spring 2.x的配置有不一样的地方,举例如下:</p>
<p>spring1.x<br>这里以1.4.1.RELEASE为例子<br>1、在pom文件中加入:</p>
<p> &lt;dependency&gt;<br>            &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;<br>            &lt;artifactId&gt;spring-boot-starter-actuator&lt;/artifactId&gt;<br>            &lt;version&gt;1.4.1.RELEASE&lt;/version&gt;<br>        &lt;/dependency&gt;</p>
<p>2、在application.yml中加入<br>endpoints:</p>
<pre><code># 启用pause端点
pause:
  enabled: true
# 启用restart端点，之所以要启用restart端点，是因为pause端点的启用依赖restart端点的启用
restart:
  enabled: true
</code></pre><p>启动服务后直接调用<br><a href="http://127.0.0.1:{port}/pause" target="_blank" rel="external">http://127.0.0.1:{port}/pause</a><br>从eureka上就是down的状态了</p>
<p>spring2.x:<br>1、在pom文件中加入:<br>       &lt;dependency&gt;<br>            &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;<br>            &lt;artifactId&gt;spring-boot-starter-actuator&lt;/artifactId&gt;<br>            &lt;version&gt;2.1.0.RELEASE&lt;/version&gt;<br>        &lt;/dependency&gt;<br>2、在application.yms中加入:</p>
<p>management:<br>  endpoint:</p>
<pre><code># 启用pause端点
pause:
  enabled: true
# 启用restart端点，之所以要启用restart端点，是因为pause端点的启用依赖restart端点的启用。详见：https://cloud.spring.io/spring-cloud-static/Finchley.SR2/single/spring-cloud.html#_endpoints
restart:
  enabled: true
</code></pre><p>  endpoints:<br>    web:<br>      exposure:<br>        include: pause,restart</p>
<p>启动服务后调用<br><a href="http://127.0.0.1:{port}/actuator/pause" target="_blank" rel="external">http://127.0.0.1:{port}/actuator/pause</a><br>从eureka上就是down的状态了</p>
<p>最后也测试了一下加入kafka consumer不断消费的情况.<br>调用pause后也停止消费了.<br>但是如果实现了org.springframework.cloud.stream.annotation.StreamListener<br>会造成服务注销后再次变为up的情况.<br>说明应该还是有情况会造成程序变为up,需要再研究一下</p>
<hr>
<p>pause有个缺陷,如果开启了healthcheck那么pause会失效.<br>这里最终采用了/service-registry的方式下线应用</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2020/04/13/how-to-unregister-service-from-eureka/" data-id="ck9fngvdn000bfm79j1ffmr05" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/spring-eureka/">spring eureka</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-how-to-deploy-logstash-plugin-env" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2020/01/15/how-to-deploy-logstash-plugin-env/" class="article-date">
  <time datetime="2020-01-15T15:21:19.000Z" itemprop="datePublished">2020-01-15</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2020/01/15/how-to-deploy-logstash-plugin-env/">logstash jdbc plugin 优化</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>本文主要讲logstash 插件环境搭建<br>起因是我需要把mysql里一张1000w条数据的表导入到es中,于是采用了logstash数据泵导入到es.<br>logstash是有jdbc插件的,因为数据量巨大,一次性载入内存耗费太大,16g内存机器吃不消.所以采用分页的方式来搞.<br>配置如下:<br>input {<br>jdbc {<br>    jdbc_driver_library =&gt; “/opt/logstash-7.5.1/lib/mysql-connector-java-5.1.44.jar”<br>    jdbc_driver_class =&gt; “com.mysql.jdbc.Driver”<br>    jdbc_connection_string =&gt; “jdbc:mysql://xxxxxxx:33063/xxxxxx?autoReconnect=true&amp;failOverReadOnly=false&amp;maxReconnects=10”<br>    jdbc_user =&gt; “xxxxxx”<br>    jdbc_password =&gt; “xxx”<br>    jdbc_page_size =&gt; 1000<br>    jdbc_validate_connection =&gt; true<br>    jdbc_pool_timeout =&gt; 5<br>    jdbc_validation_timeout =&gt; 10<br>    jdbc_fetch_size =&gt; 1000<br>    connection_retry_attempts_wait_time =&gt; 5<br>    jdbc_paging_enabled =&gt; true<br>    statement =&gt; “SELECT * from a”<br>}<br>}<br>output {<br>  elasticsearch {<br>    index =&gt; “his”<br>    hosts =&gt; [“localhost:9200”]<br>  }<br>  stdout { codec =&gt; rubydebug }<br>}</p>
<p>然后开始报错:<br>[2020-01-09T22:02:19,614][ERROR][logstash.inputs.jdbc     ][main] Java::ComMysqlJdbcExceptionsJdbc4::CommunicationsException: Communications link failure</p>
<p>The last packet successfully received from the server was 60,296 milliseconds ago.  The last packet sent successfully to the server was 60,108 milliseconds ago.: SELECT count(*) AS <code>count</code> FROM (SELECT inout_id,vehicle_no,create_time from a) AS <code>t1</code> LIMIT 1<br>[2020-01-09T22:02:19,666][WARN ][logstash.inputs.jdbc     ][main] Exception when executing JDBC query {:exception=&gt;#&lt;Sequel::DatabaseDisconnectError: Java::ComMysqlJdbcExceptionsJdbc4::CommunicationsException: Communications link failure</p>
<p>The last packet successfully received from the server was 60,296 milliseconds ago.  The last packet sent successfully to the server was 60,108 milliseconds ago.&gt;}<br>[2020-01-09T22:02:20,206][INFO ][logstash.runner          ] Logstash shut down.</p>
<p>一开始以为是参数设置问题,加上各种timeout设置也不起作用.<br>这里有个重点是分页logstash默认走的是子查询查到一个总和,如下所示:<br>select count(<em>) as <code>count</code> from (select </em> from a)<br>但是这要求把全部数据先查一遍.怀疑这里造成性能损耗太大.<br>于是换了个思路,改一下logstash-jdbc源码把……加个专门查总数查询的配置.<br>那因为logstash是juby的,那咱就先装ruby.<br>先安装rvm:<br>curl -sSL <a href="https://get.rvm.io" target="_blank" rel="external">https://get.rvm.io</a> | bash -s stable<br>logstash 必须安装jruby!!<br>安装jruby:<br>rvm install jruby<br>使用rvm的时候要先执行<br>source /etc/profile.d/rvm.sh<br>安装 bundle:<br>gem install bundler<br>这里有至少两个坑:<br>1、安装bundler的时候要注意给/usr/local/rvm以ruby当前用户的权限 chown和chgrp 要不然权限会有问题.<br>另外要随时跟进安装错误日志<br>比如期间遇到过git clone jruby hang住了,那就手动先git clone 到指定目录再执行 安装jruby.</p>
<p>2、网络!<br>装的时候有时需要走ss代理,有时不需要.需要多试试<br>比如步骤1的时候手动clone jruby然后 mvn build的时候,可以用ss代理提速<br>3、安装jruby的时候报错:<br>root@slave-node:~/logstash-input-jdbc# rvm install jruby<br>jruby-9.2.6.0 - #removing src/jruby-9.2.6.0..<br>Searching for binary rubies, this might take some time.<br>Found remote file <a href="https://repo1.maven.org/maven2/org/jruby/jruby-dist/9.2.6.0/jruby-dist-9.2.6.0-bin.tar.gz" target="_blank" rel="external">https://repo1.maven.org/maven2/org/jruby/jruby-dist/9.2.6.0/jruby-dist-9.2.6.0-bin.tar.gz</a><br>Checking requirements for ubuntu.<br>Requirements installation successful.<br>jruby-9.2.6.0 - #configure<br>jruby-9.2.6.0 - #download<br>  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current<br>                                 Dload  Upload   Total   Spent    Left  Speed<br>100 22.9M  100 22.9M    0     0  24821      0  0:16:09  0:16:09 –:–:– 25820<br>No checksum for downloaded archive, recording checksum in user configuration.<br>jruby-9.2.6.0 - #validate archive<br>jruby-9.2.6.0 - #extract<br>jruby-9.2.6.0 - #validate binary<br>jruby-9.2.6.0 - #setup<br>jruby-9.2.6.0 - #gemset created /usr/local/rvm/gems/jruby-9.2.6.0@global<br>jruby-9.2.6.0 - #importing gemset /usr/local/rvm/gemsets/jruby/global.gems.<br>Error running ‘gemset_import_list jruby-openssl jruby-launcher gem-wrappers rubygems-bundler rake rvm bundler’,<br>please read /usr/local/rvm/log/1578889834_jruby-9.2.6.0/gemsets.import.global.log<br>打开本地日志文件如下:<br>java.lang.LayerInstantiationException: Package jdk.internal.jrtfs in both module jrt.fs and module java.base<br>本人用的是jdk13,把jdk13下的jrt-fs.jar删掉后解决<br>4、安装jury的时候,报如下错误:<br>ERROR:  While executing gem … (Gem::Exception)<br>    Failed to find gems [“”] &gt;= 0<br>++ _failed+=(“${_gem} –version ${_version}”)<br>++ read _gem _version _platforms<br>++ ((  1 &gt; 0  ))<br>++ rvm_error ‘\n’\’’command gem pristine –extensions  –version ‘\’’ failed, you need to fix these gems manually.’<br>++ rvm_pretty_print stderr<br>++ case “${rvm_pretty_print_flag:=auto}” in<br>++ case “${TERM:-dumb}” in<br>++ case “$1” in<br>++ [[ -t 2 ]]<br>++ return 1<br>++ printf %b ‘\n’\’’command gem pristine –extensions  –version ‘\’’ failed, you need to fix these gems manually.\n’<br>++ return 1<br>++ return 1</p>
<p>不过我再次输入<br>rvm install jruby的时候提示我已经安装成功了,就没深究这个问题<br>5、删除本地插件的时候要注意执行命令的当前目录<br>必须在 logstash的主目录下!不能在bin下.要不然会报如下错误:<br>ERROR: Operation aborted, cannot remove plugin, message: The path <code>/opt/logstash-7.5.1/bin/logstash-core</code> does not exist.</p>
<p>本地插件编译和安装的步骤如下:<br>1、cd 到jdbc插件的根目录<br>2、切换到ruby的用户,比如我这里是es<br>su -s es<br>4、souce /etc/profile.d/rvm.sh<br>5、项目安装依赖</p>
<p>bundle install<br>这里安装依赖的时候 我忘了是不是要用ss代理了………<br>安装的时候如果有失败的情况会提示你做一些操作,比如:<br>URI::InvalidURIError: can not set user with opaque<br>An error occurred while installing rake (13.0.1), and Bundler<br>cannot continue.<br>Make sure that <code>gem install rake -v &#39;13.0.1&#39; --source &#39;https://rubygems.org/&#39;</code><br>succeeds before bundling.</p>
<p>In Gemfile:<br>  logstash-devutils was resolved to 1.3.6, which depends on<br>    rake</p>
<p>那就执行一下gem install rake -v ‘13.0.1’ –source ‘<a href="https://rubygems.org/" target="_blank" rel="external">https://rubygems.org/</a> 就好了<br>6、构建插件gem (在项目主目录下产生logstash-input-jdbc-4.3.19.gem文件)</p>
<p>gem build logstash-input-jdbc.gemspec</p>
<p>7、logstash卸载logstash-input-jdbc插件</p>
<p>/bin/logstash-plugin uninstall logstash-input-jdbc<br>8、删除<br>rm -rf /opt/logstash-7.5.1/vendor/cache/logstash-filter-geoip-6.0.3-java.gem<br>9、logstash安装本地插件 (先将构建成功的gem文件拷贝至logstash主目录的trade_gem文件夹中)</p>
<p>/bin/logstash-plugin install –no-verify –local ../logstash-input-jdbc-4.3.19.gem</p>
<p>优化后的配置文件如下:<br>input {<br>jdbc {<br>    jdbc_driver_library =&gt; “/opt/logstash-7.5.1/lib/mysql-connector-java-5.1.44.jar”<br>    jdbc_driver_class =&gt; “com.mysql.jdbc.Driver”<br>    jdbc_connection_string =&gt; “jdbc:mysql://xxxxxx:33063/database?autoReconnect=true&amp;failOverReadOnly=false&amp;maxReconnects=10&amp;zeroDateTimeBehavior=convertToNull”<br>    jdbc_user =&gt; “username”<br>    jdbc_password =&gt; “password”<br>    jdbc_page_size =&gt; 1000<br>    jdbc_validate_connection =&gt; true<br>    jdbc_pool_timeout =&gt; 5<br>    jdbc_validation_timeout =&gt; 10<br>    jdbc_fetch_size =&gt; 1000<br>    connection_retry_attempts_wait_time =&gt; 5<br>    jdbc_paging_enabled =&gt; true<br>    subquery_paging_enabled =&gt; true<br>    sum_statement =&gt; “select count(1) as sum from his”<br>    statement =&gt; “SELECT * from his_shadow where id &gt;= (select id from his order by id limit :data_offset,1) limit :jdbc_page_size”<br>}<br>}<br>output {<br>  elasticsearch {<br>    index =&gt; “_his”<br>    hosts =&gt; [“localhost:9200”]<br>  }<br>  stdout { codec =&gt; rubydebug }<br>}</p>
<p>参考资料:</p>
<p><a href="https://www.jianshu.com/p/84e807e1a575" target="_blank" rel="external">https://www.jianshu.com/p/84e807e1a575</a><br>感谢前人无私的分享,新版本的logstash代码接口已经有变化了这里附上新版本的代码如下:<br><a href="/2020/01/15/how-to-deploy-logstash-plugin-env/statement_handler.rb" title="statement_handler.rb">statement_handler.rb</a><br><br>完整工程如下:</p>
<a href="/2020/01/15/how-to-deploy-logstash-plugin-env/logstash-input-jdbc_youhua.zip" title="logstash-input-jdbc_youhua.zip">logstash-input-jdbc_youhua.zip</a>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2020/01/15/how-to-deploy-logstash-plugin-env/" data-id="ck9fngvdx000ffm79av2g0bf9" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>


  
    <article id="post-k8s-install" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2019/11/11/k8s-install/" class="article-date">
  <time datetime="2019-11-11T13:43:45.000Z" itemprop="datePublished">2019-11-11</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2019/11/11/k8s-install/">k8s安装</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>今天遇到了k8s 在ubuntu18.04下安装的时候进行kubeadm init的时候拉取image失败的问题，<br>而且我设置了shadowshocket的代理http_proxy和https_proxy依然不行，<br>后来指定镜像库解决。命令如下：<br>kubeadm init –image-repository registry.aliyuncs.com/google_containers –ignore-preflight-errors=Swap<br>收官</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2019/11/11/k8s-install/" data-id="ck9fngved000jfm792ii61qbi" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/k8s/">k8s</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-tracingSystem" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2019/05/05/tracingSystem/" class="article-date">
  <time datetime="2019-05-05T11:29:34.000Z" itemprop="datePublished">2019-05-05</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2019/05/05/tracingSystem/">链路追踪系统之选型篇</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>Distributed Tracing最早起源于google的一片论文（链接见文末），之后各家都退出了自己的监控系统<br>但是数据格式并不统一，所以发起了open tracing标准，现在skywalking、zipkin和jaeger都遵循这一协议。<br>本篇是对近期链路跟踪系统选型的汇总。<br>主要选型对象有如下几个：zipkin、skywalking、CAT和pinpoint。<br>CAT的代码侵入性最大，并且过于臃肿可以reject，不过CAT的分析系统做的不错，可以借鉴一下。<br>pinpoint是naver出产的，团队规模小，并且需要hbase，韩国出品，所以也reject。<br>这里主要讨论zipkin与skywalking<br>zipkin架构图如下：<br><img src="/2019/05/05/tracingSystem/zipkin_architecture.png" alt="zipkin_overview" title="zipkin_overview"><br>在这里解读一下上图各个部分。<br>首先instrumented client 表示是经过扩展的客户端<br>Non-instrumented client 表示未经过扩展的客户端<br>instrumented client的一个请求叫做Span，一个完整调用链叫trace。<br>上图首先从instrumented client发送一个span到transport然后到collect，最后落盘存储用于ui展示。<br>发送到Non-instrumented client 并不会给collect发送记录存储。</p>
<p>zipkin主要有两个工程：<br><a href="https://github.com/openzipkin/brave" target="_blank" rel="external">https://github.com/openzipkin/brave</a><br><a href="https://github.com/apache/incubator-zipkin" target="_blank" rel="external">https://github.com/apache/incubator-zipkin</a><br>brave主要是zipkin的外围扩展。<br>现在主要支持一下扩展：<br><img src="/2019/05/05/tracingSystem/zipkin_instrument.png" alt="zipkin_instrument" title="zipkin_instrument"></p>
<p>ncubator-zipkin是zipkin的服务端。<br>包含如下部分：<br>server：服务端程序。<br>lens：web ui<br>storage：将数据存盘，现在支持mysql、cassandra和es<br>collect：收集器，包括三种收集模式：rest、kafka和rabbitmq。<br>正常来说应该使用mq异步的collect这样对程序本身性能影响最小。<br>zipkin的扩展是通过有针对性的埋点来实现的。<br>比如对于rest的扩展是通过spring的resttemplate拦截器在header里传traceid和spanid来实现的。<br>对于kafka消息的扩展也是在header里加参数来实现的。<br>所以zipkin本身依赖于第三方目标服务是否具有扩展性。<br>侵入性来说需要添加配置。</p>
<p>下面说说skywalking<br><img src="/2019/05/05/tracingSystem/skywalking.jpeg" alt="skywalking" title="skywalking"><br>首先tracing（跟踪）也好，Metric（指标）也罢，都是通过http或者grpc将采集起来的数据传送到服务器端的。<br>其数据存储包括mysql、es、tidb、h2和sharding<br>因为skywalking是通过AOP的方式动态织入代码的，所以并不受限于api扩展。<br>以下是已有的扩展：<br><img src="/2019/05/05/tracingSystem/skywalking_plugin1.png" alt="skywalking_plu1" title="skywalking_plu1"><br>&amp;<br><img src="/2019/05/05/tracingSystem/skywalking_plugin2.png" alt="skywalking plu2" title="skywalking plu2"><br>而且skywalking对于代码完全没有任何侵入性。<br>缺点是与我司运维人员了解到有两个问题：<br>1、存在es中的数据无法删除<br>2、如果es重启的话，必须所有agent都重启完毕，skywalking的监控上才会有数值。<br>这个要跟一下具体问题。</p>
<p>综合总结一下如下：<br>zipkin是基于api调用的，skywalking是基于AOP的。扩展性上讲skywalking要优于zipkin。<br>代码复杂度上来说，zipkin更容易上手一些，代码量也少。<br>性能上来说skywalking比zipkin要好。</p>
<p>相关zipkin与skywalking的理论基础如下：<br><a href="https://bigbully.github.io/Dapper-translation/" target="_blank" rel="external">https://bigbully.github.io/Dapper-translation/</a></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2019/05/05/tracingSystem/" data-id="ck9fngvhs0010fm792gs4jt6v" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/zipkin-skywalking-cat/">zipkin skywalking cat</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-presto-source-code" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2019/01/07/presto-source-code/" class="article-date">
  <time datetime="2019-01-07T14:40:06.000Z" itemprop="datePublished">2019-01-07</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2019/01/07/presto-source-code/">码林啄木之presto阅读理解</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>Day 1:<br>一般来讲，我们要阅读一个项目的源代码，首先要从网上找一下有没有读过该项目，次之去官网上找文档，最后再读细节。<br>我们第一天先讲讲presto的启动命令、架构以及代码架构。<br>首先祭出架构图：摘自官网<br><img src="/2019/01/07/presto-source-code/presto-overview.png" alt="overview" title="overview"><br>也就是说presto主要有如下几个部分组成：<br>CLI：就是命令行，用于提交查询，通俗点的理解就是发起rest请求<br>服务器端有两种节点：coordinators和worker。一下简称C节点和W节点<br>首先来看C节点：<br>C节点主要用于解析语句，生成请求接话，管理W节点。<br>W节点用于执行请求计划，处理数据，W节点从连接器读取数据并彼此交换数据。<br>C节点与W节点通过rest API来通信。<br>以上就是这个overview图的最直白简洁。<br>然鹅还有一些关键概念。<br>一、presto链接数据源的时候需要什么参数？<br>1、Connector连接器，连接器顾名思义，作用相当于jdbc中的driver。<br>2、Catalog，一个数据源就是catalog，可以是mysql，可以是hive等等<br>3、Schema，类似mysql中的database，一个catalog可以有多个schema<br>4、Table，就是表！<br>二、presto的查询模型<br>1、statement<br>语句，就是sql语句。是一个静态的概念。<br>2、query<br>查询，通过语句生成的query，是一个动态的执行。<br>3、stage<br>一个stage代表查询执行计划的一个部分。<br>stage有四种类型：<br>Coordinator_Only:<br>用于执行DDM或者DDL表的创建与更改<br>Single:<br>这种类型用于聚合子Stage的输出数据<br>Fixed:<br>用于接受其子Stage产生的数据并在集群中对这些数据进行分布式的聚合或者分组计算。<br>Source:<br>用于直接链接数据源，从数据源读取数据，在读取数据的时候，该阶段也会根据presto对查询执行计划的优化完成相关的断言下发和条件过滤等<br>Exchange：<br>交换，stage之间用于数据交换用的。场景包括下游从上游读取数据。<br>Task:<br>就是stage切分出来运行在W节点上的具体任务。<br>Split：<br>就是数据集上的子集。下文说道的driver就是在split上做操作<br>Driver：<br>一个task包括一个或者多个Driver。就是一个split的一系列operator集合。<br>Page是presto中处理的最小数据单元。一个Page对象包含多个Block对象，而每个Block对象是一个字节数组，存储一个字段的若干行。多个block横切的一行是真实的一行数据。</p>
<p>以上，就是我们对于presto源码阅读理解的重点喽</p>
<p>再来看看presto的启动命令吧<br>首先是C节点：<br>-Dnode.data-dir=/mnt/disk1/log/presto -Dnode.id=xxxxxxxxxxx -Dnode.environment=production -Dlog.enable-console=false -Dlog.levels-file=/etc/ecm/presto-conf-0.188/log.properties -Dconfig=/etc/ecm/presto-conf-0.188/config.properties com.facebook.presto.server.PrestoServer<br>然后是W节点<br>-Dnode.data-dir=/mnt/disk1/log/presto -Dnode.id=xxxxxxx -Dnode.environment=production -Dlog.enable-console=false -Dlog.levels-file=/etc/ecm/presto-conf-0.188/log.properties -Dconfig=/etc/ecm/presto-conf-0.188/config.properties com.facebook.presto.server.PrestoServer<br>命令是一样滴哦，惊不惊喜，意不意外！<br>差别就在properties里了<br>里面有个coordinator属性为true则为C节点，false为W节点<br>本来想今天把服务跑起来看看的，结果意外了。<br>连续两次意外了。<br>第一次的时候发现需要装antlr插件也就是g4插件。。。。<br>第二次又意外了 jdk版本不够。。。<br>Presto requires Java 8u151+ (found 1.8.0_144)</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2019/01/07/presto-source-code/" data-id="ck9fngvev000pfm79v5f1nv3g" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>


  
    <article id="post-sqoop-huizong" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2018/12/20/sqoop-huizong/" class="article-date">
  <time datetime="2018-12-20T15:38:06.000Z" itemprop="datePublished">2018-12-20</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2018/12/20/sqoop-huizong/">sqoop问题汇总</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>sqoop最近遇到的问题在这里汇总一下</p>
<p>先说环境<br>阿里emr-3.12.0<br>sqoop 1.4.7<br>1、字段类型问题<br>首先遇到的一个问题是通过sqoop从mysql导入到hive的时候，如果不指定列的类型，到hive默认都是字符串。<br>这样做日期计算的时候会稍微麻烦一些。<br>解决办法：<br>通过–map-column-java 和–map-column-hive 来指定类型<br>参考资料：<br><a href="https://blog.csdn.net/xianjie0318/article/details/79300435" target="_blank" rel="external">https://blog.csdn.net/xianjie0318/article/details/79300435</a><br>2、mysql导入hive报GC overhead limit exceeded<br>解决方法：<br>mysql connection连接字符串加上：<br>?dontTrackOpenResources=true&amp;defaultFetchSize=1000&amp;useCursorFetch=true<br>参考资料：<br><a href="https://www.cnblogs.com/chwilliam85/p/9693276.html" target="_blank" rel="external">https://www.cnblogs.com/chwilliam85/p/9693276.html</a><br>3、通过sqoop增量地从mysql导入到hive<br>这个我一开始用–append 模式导入的，然后报错如下：<br>Append mode for hive imports is not  yet supported. Please remove the parameter –append-mode<br>我提了一个工单给阿里客服，<br>提供了一个参考链接：<br><a href="https://community.hortonworks.com/questions/11373/sqoop-incremental-import-in-hive-i-get-error-messa.html" target="_blank" rel="external">https://community.hortonworks.com/questions/11373/sqoop-incremental-import-in-hive-i-get-error-messa.html</a><br>这种方式是通过创建外部表，然后增量写入到hdfs来实现的。<br>这里有个注意的地方是要配合-check-column来使用，而这个字段可以是数值型的id，也可以是时间戳！<br>参考资料如下：<br><a href="https://stackoverflow.com/questions/41238887/sqoop-incremental-import-error-using-date-column" target="_blank" rel="external">https://stackoverflow.com/questions/41238887/sqoop-incremental-import-error-using-date-column</a><br>以时间戳为增量的sqoop基本设计流程如下：<br>首先创建外部表：<br>CREATE EXTERNAL TABLE test (id string,create_time timestamp,reservation1 string,reservation2 string,reservation3 string,reservation4 string,reservation5 string,reservation6 string,reservation7 string,reservation8 string,reservation9 string,reservation10 string) ROW FORMAT DELIMITED FIELDS TERMINATED BY ‘,’ STORED AS TEXTFILE location ‘/user/wfliu/test’ ;<br>然后全量导入mysql数据库表，如下：</p>
<p>sqoop import -Dorg.apache.sqoop.splitter.allow_text_splitter=true –connect ‘jdbc:mysql://host:3306/test?useUnicode=true&amp;characterEncoding=utf-8&amp;zeroDateTimeBehavior=convertToNull&amp;dontTrackOpenResources=true&amp;defaultFetchSize=1000&amp;useCursorFetch=true’ –driver com.mysql.jdbc.Driver –username test –password xxx –incremental append –table test –target-dir /user/wfliu/test -check-column update_time -m 1 –hive-drop-import-delims –verbose -m 1<br>最后定一个定时执行增量的sqoop任务：<br>sqoop import -Dorg.apache.sqoop.splitter.allow_text_splitter=true –connect ‘jdbc:mysql://host:3306/test?useUnicode=true&amp;characterEncoding=utf-8&amp;zeroDateTimeBehavior=convertToNull&amp;dontTrackOpenResources=true&amp;defaultFetchSize=1000&amp;useCursorFetch=true’ –driver com.mysql.jdbc.Driver –username test –password xxx –table test –target-dir /user/wfliu/test –incremental lastmodified -check-column create_time -m 1 –last-value “${lastvalue}” –append –hive-drop-import-delims –verbose -m 1<br>其中lastvalue是emr中配置的参数可以为${yyyy-MM-dd 00:00:00-1d}。为增量导入截止到每天凌晨12点的数据。<br>4、新版“数据开发”功能支持参数化<br>以前用老版的“作业调度”sqoop并不支持参数<br>用新版的“数据开发”<br><img src="/2018/12/20/sqoop-huizong/1.png" alt="参数化" title="参数化"><br>5、如何删除外部表中的数据<br>今天遇到的问题是因为导入数据有错误想清空外部表的数据<br>一开始用<br>delete from xxx的方式，不行<br>然后用trancat table xxx 也不行<br>这两种都不能删除外部表的数据<br>那怎么办呢<br>答案：先把外部表变成内部表再删除<br>ALTER TABLE xxx SET TBLPROPERTIES(‘EXTERNAL’=’False’);<br>然后<br>truncate table xxx;<br>这样就可以啦！<br>参考资料：<br><a href="https://blog.csdn.net/tengxing007/article/details/81211500" target="_blank" rel="external">https://blog.csdn.net/tengxing007/article/details/81211500</a></p>
<p>另外sqoop也是可以直接直接创建hive表的<br>sqoop create-hive-table<br>可以指定–hive-overwrite来覆盖表中原有数据<br>参考资料：<br><a href="https://segmentfault.com/a/1190000002532293" target="_blank" rel="external">https://segmentfault.com/a/1190000002532293</a></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2018/12/20/sqoop-huizong/" data-id="ck9fngvfz000wfm798zrhqqyf" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>


  
    <article id="post-how-to-add-col-spark-dataframe" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2018/12/17/how-to-add-col-spark-dataframe/" class="article-date">
  <time datetime="2018-12-17T15:22:06.000Z" itemprop="datePublished">2018-12-17</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2018/12/17/how-to-add-col-spark-dataframe/">如何给spark表添加一列</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>刚开始接触spark的时候遇到一个特别中二的问题。<br>我从两张表里查数据，这两张表里都只有一行，如何把这两张表合并起来呢？？？？<br>然后我特缺心眼的从网上搜，spark添加列的方式。<br>好吧。。。前情交代完毕。<br>说说过程<br>解决方式特简单，用join就tm可以了。<br>好了<br>话题说到这里就结束哦了吗？？？？<br>那我还写个鸡毛！<br>之后我翻阅了一下我搜到的如何增加一列的问题！<br>首先先看如何增加一个常数列<br>代码如下：<br>        val tempDataFrame = spark.createDataFrame(Seq(<br>            (1, “asf”),<br>            (2, “2143”),<br>            (3, “rfds”)<br>        )).toDF(“id”, “content”)<br>        val addColDataframe = tempDataFrame.withColumn(“col”, tempDataFrame(“id”)*2)<br>        addColDataframe.show(10,false)</p>
<p>+—+——-+—+<br>|id |content|col|<br>+—+——-+—+<br>|1  |asf    |2  |<br>|2  |2143   |4  |<br>|3  |rfds   |6  |<br>+—+——-+—+</p>
<p>然后用udf来实现一个复杂一些的：<br>        import scala.util.Try<br>        import org.apache.spark.sql.functions._<br>        val tempDataFrame = spark.createDataFrame(Seq(<br>            (“1”, “2”),<br>            (“2”, “3”),<br>            (“3”, “1”)<br>        )).toDF(“content1”, “content2”)</p>
<pre><code>val code = (arg1: String, arg2: String) =&gt; {
    Try(if (arg1.toInt &gt; arg2.toInt) &quot;arg1&gt;arg2&quot; else &quot;arg1&lt;=arg2&quot;).getOrElse(&quot;error&quot;)
}
val compareUdf = udf(code)

val addColDataframe = tempDataFrame.withColumn(&quot;compare&quot;, compareUdf(tempDataFrame(&quot;content1&quot;),tempDataFrame(&quot;content2&quot;)))
addColDataframe.show(10, false)
</code></pre><p>+——–+——–+———-+<br>|content1|content2|compare   |<br>+——–+——–+———-+<br>|1       |2       |arg1&lt;=arg2|<br>|2       |3       |arg1&lt;=arg2|<br>|3       |1       |arg1&gt;arg2 |<br>+——–+——–+———-+</p>
<p>这样添加的一列就可以是比较复杂的了。<br>参考资料如下：<br><a href="http://www.cnblogs.com/TTyb/p/7169148.html" target="_blank" rel="external">http://www.cnblogs.com/TTyb/p/7169148.html</a></p>
<p>让我们再深入一些！<br>如何添加唯一的一列，对！就是类似主键的那样<br>两种方式：<br>第一种 使用zipWithUniqueId获取id 并重建 DataFrame.<br>        import spark.implicits._<br>        import org.apache.spark.sql.Row<br>        import org.apache.spark.sql.types.{StructType, StructField, LongType}</p>
<pre><code>val df =Seq((&quot;a&quot;, -1.0), (&quot;b&quot;, -2.0), (&quot;c&quot;, -3.0)).toDF(&quot;foo&quot;, &quot;bar&quot;)
// 获取df 的表头
val s = df.schema

// 将原表转换成带有rdd,
//再转换成带有id的rdd,
//再展开成Seq方便转化成 Dataframe
val rows = df.rdd.zipWithUniqueId.map{case (r: Row, id: Long) =&gt; Row.fromSeq(id +: r.toSeq)}

// 再由 row 根据原表头进行转换
val dfWithPK = spark.createDataFrame( rows, StructType(StructField(&quot;id&quot;, LongType, false) +: s.fields))
dfWithPK.show
</code></pre><p>+—+—+—-+<br>| id|foo| bar|<br>+—+—+—-+<br>|  0|  a|-1.0|<br>|  1|  b|-2.0|<br>|  2|  c|-3.0|<br>+—+—+—-+</p>
<p>以上可以实现自增列。<br>第二种：直接使用spark 自己的api,monotonicallyIncreasingId<br>        val df =Seq((“a”, -1.0), (“b”, -2.0), (“c”, -3.0)).toDF(“foo”, “bar”)<br>        import org.apache.spark.sql.functions.monotonically_increasing_id<br>        df.withColumn(“id”, monotonically_increasing_id).show()</p>
<p>+—+—-+—+<br>|foo| bar| id|<br>+—+—-+—+<br>|  a|-1.0|  0|<br>|  b|-2.0|  1|<br>|  c|-3.0|  2|<br>+—+—-+—+<br>注意这里也是自增列</p>
<p>还有第三种：使用createDateFrame<br>        val input =Seq((“a”, 100.0), (“b”, -2.0), (“c”, -3.0)).toDF(“foo”, “bar”)<br>        var critValueR = 0<br>        var critValueL = -10</p>
<pre><code>val trdd = input.select(&quot;bar&quot;).rdd.map(x=&gt;{
    if (x.get(0).toString().toDouble &gt; critValueR || x.get(0).toString().toDouble &lt; critValueL)
        Row(x.get(0).toString().toDouble,&quot;F&quot;)
    else Row(x.get(0).toString().toDouble,&quot;T&quot;)
})
val schema = input.select(&quot;bar&quot;).schema.add(&quot;flag&quot;, StringType, true)
val sample3 = spark.createDataFrame(trdd, schema).distinct().withColumnRenamed(&quot;flag&quot;, &quot;idx&quot;)
sample3.show
</code></pre><p>+—–+—+<br>|  bar|idx|<br>+—–+—+<br>| -3.0|  T|<br>|100.0|  F|<br>| -2.0|  T|<br>+—–+—+</p>
<p>觉得比较直观且开放性大的还是实用udf。</p>
<p>参考资料：<br><a href="https://www.jianshu.com/p/7e6e406dd15b" target="_blank" rel="external">https://www.jianshu.com/p/7e6e406dd15b</a></p>
<p>。。。。。。。。。。你以为这就完了？。。。。。。<br>那我还写个毛线。。。。。。<br>后来又发现了对列的拆分和合并</p>
<p>1、如何合并列：<br>        val df =Seq((“Ming”,20,”15552211521”), (“hong”,19,”13287994007”), (“zhi”,21,”15552211523”)).toDF(“name”,”age”, “phone”)<br>        val separator = “,”<br>        df.map(<em>.toSeq.foldLeft(“”)(</em> + separator + _).substring(1)).show()<br>+——————-+<br>|              value|<br>+——————-+<br>|Ming,20,15552211521|<br>|hong,19,13287994007|<br>| zhi,21,15552211523|<br>+——————-+<br>也可以使用concat<em>ws:<br>        import org.apache.spark.sql.functions.</em><br>        df.select(concat_ws(separator, $”name”, $”age”, $”phone”).cast(StringType).as(“value”)).show()</p>
<p>结果一样<br>第三种实用udf！<br>       def mergeCols(row: Row): String = {<br>            row.toSeq.foldLeft(“”)(<em> + separator + </em>).substring(1)<br>        }</p>
<pre><code>val mergeColsUDF = udf(mergeCols _)
df.select(mergeColsUDF(struct($&quot;name&quot;, $&quot;age&quot;, $&quot;phone&quot;)).as(&quot;value&quot;)).show()
</code></pre><p>结果也是同上</p>
<p>2、如何拆分列呢？<br>a、实用split<br>    val df =Seq((“Ming,20,15552211521”), (“hong,19,13287994007”), (“zhi,21,15552211523”)).toDF(“value”)<br>    val separator = “,”<br>    lazy val first = df.first()</p>
<pre><code>val numAttrs = first.toString().split(separator).length
val attrs = Array.tabulate(numAttrs)(n =&gt; &quot;col_&quot; + n)
//按指定分隔符拆分value列，生成splitCols列
var newDF = df.withColumn(&quot;splitCols&quot;, split($&quot;value&quot;, separator))
attrs.zipWithIndex.foreach(x =&gt; {
  newDF = newDF.withColumn(x._1, $&quot;splitCols&quot;.getItem(x._2))
})
newDF.show()
</code></pre><p>+——————-+——————–+—–+—–+———–+<br>|              value|           splitCols|col_0|col_1|      col_2|<br>+——————-+——————–+—–+—–+———–+<br>|Ming,20,15552211521|[Ming, 20, 155522…| Ming|   20|15552211521|<br>|hong,19,13287994007|[hong, 19, 132879…| hong|   19|13287994007|<br>| zhi,21,15552211523|[zhi, 21, 1555221…|  zhi|   21|15552211523|<br>+——————-+——————–+—–+—–+———–+</p>
<p>b、实用udf：<br>        import org.apache.spark.ml.attribute.{Attribute, NumericAttribute}</p>
<pre><code>val df =Seq((&quot;Ming,20,15552211521&quot;), (&quot;hong,19,13287994007&quot;), (&quot;zhi,21,15552211523&quot;)).toDF(&quot;value&quot;)
val separator = &quot;,&quot;
lazy val first = df.first()
val attributes: Array[Attribute] = {
    val numAttrs = first.toString().split(separator).length
    //生成attributes
    Array.tabulate(numAttrs)(i =&gt; NumericAttribute.defaultAttr.withName(&quot;value&quot; + &quot;_&quot; + i))
}
//创建多列数据
val fieldCols = attributes.zipWithIndex.map(x =&gt; {
    val assembleFunc = udf {
        str: String =&gt;
            str.split(separator)(x._2)
    }
    assembleFunc(df(&quot;value&quot;).cast(StringType)).as(x._1.name.get, x._1.toMetadata())
})
//合并数据
df.select(col(&quot;*&quot;) +: fieldCols: _*).show()
</code></pre><p>+——————-+——————–+—–+—–+———–+<br>|              value|           splitCols|col_0|col_1|      col_2|<br>+——————-+——————–+—–+—–+———–+<br>|Ming,20,15552211521|[Ming, 20, 155522…| Ming|   20|15552211521|<br>|hong,19,13287994007|[hong, 19, 132879…| hong|   19|13287994007|<br>| zhi,21,15552211523|[zhi, 21, 1555221…|  zhi|   21|15552211523|<br>+——————-+——————–+—–+—–+———–+<br>参考资料：<br><a href="http://www.cnblogs.com/itboys/p/9813934.html" target="_blank" rel="external">http://www.cnblogs.com/itboys/p/9813934.html</a></p>
<p>综上所述，udf还是很牛逼的。</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2018/12/17/how-to-add-col-spark-dataframe/" data-id="ck9fngvd60006fm79up6g4xyl" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>


  


  <nav id="page-nav">
    <span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><a class="page-number" href="/page/3/">3</a><a class="extend next" rel="next" href="/page/2/">__('next') &raquo;</a>
  </nav>
</section>
        
          <aside id="sidebar">
  
    

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tags</h3>
    <div class="widget">
      <ul class="tag-list"><li class="tag-list-item"><a class="tag-list-link" href="/tags/flume/">flume</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/k8s/">k8s</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/openstack-neutron/">openstack neutron</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/spark-classload-ClassNotFound/">spark classload ClassNotFound</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/spring/">spring</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/spring-eureka/">spring eureka</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/tensroflow/">tensroflow</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/zipkin-skywalking-cat/">zipkin skywalking cat</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/阿里云-sgoop-presto/">阿里云 sgoop presto</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tag Cloud</h3>
    <div class="widget tagcloud">
      <a href="/tags/flume/" style="font-size: 10px;">flume</a> <a href="/tags/k8s/" style="font-size: 10px;">k8s</a> <a href="/tags/openstack-neutron/" style="font-size: 10px;">openstack neutron</a> <a href="/tags/spark-classload-ClassNotFound/" style="font-size: 10px;">spark classload ClassNotFound</a> <a href="/tags/spring/" style="font-size: 20px;">spring</a> <a href="/tags/spring-eureka/" style="font-size: 10px;">spring eureka</a> <a href="/tags/tensroflow/" style="font-size: 10px;">tensroflow</a> <a href="/tags/zipkin-skywalking-cat/" style="font-size: 10px;">zipkin skywalking cat</a> <a href="/tags/阿里云-sgoop-presto/" style="font-size: 10px;">阿里云 sgoop presto</a>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/04/">April 2020</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/01/">January 2020</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/11/">November 2019</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/05/">May 2019</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/01/">January 2019</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/12/">December 2018</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/10/">October 2018</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/09/">September 2018</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/05/">May 2017</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/12/">December 2016</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/11/">November 2016</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2020/04/25/hello-world/">如何搭建新版本hexo</a>
          </li>
        
          <li>
            <a href="/2020/04/25/build-flume-ha/">构建flume高可用集群</a>
          </li>
        
          <li>
            <a href="/2020/04/14/firecracker-introduction/">firecracker介绍</a>
          </li>
        
          <li>
            <a href="/2020/04/13/how-to-unregister-service-from-eureka/">如何安全优雅的把spring微服务从eureka上注销</a>
          </li>
        
          <li>
            <a href="/2020/01/15/how-to-deploy-logstash-plugin-env/">logstash jdbc plugin 优化</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2020 wfliu<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>
    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    

<script src="//ajax.googleapis.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>


  <link rel="stylesheet" href="/fancybox/jquery.fancybox.css">
  <script src="/fancybox/jquery.fancybox.pack.js"></script>


<script src="/js/script.js"></script>

  </div>
</body>
</html>