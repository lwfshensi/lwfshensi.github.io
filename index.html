<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  <title>肆叔的酱油</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta property="og:type" content="website">
<meta property="og:title" content="肆叔的酱油">
<meta property="og:url" content="http://yoursite.com/index.html">
<meta property="og:site_name" content="肆叔的酱油">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="肆叔的酱油">
  
    <link rel="alternate" href="/atom.xml" title="肆叔的酱油" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  
  
    <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <link rel="stylesheet" href="/css/style.css">
  

</head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">肆叔的酱油</a>
      </h1>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="Search"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" results="0" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://yoursite.com"></form>
      </div>
    </div>
  </div>
</header>
      <div class="outer">
        <section id="main">
  
    <article id="post-hello-world" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2020/04/13/hello-world/" class="article-date">
  <time datetime="2020-04-12T18:29:15.110Z" itemprop="datePublished">2020-04-13</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2020/04/13/hello-world/">如何搭建新版本hexo</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>我先是在debian8.6上用官网的方式搭建的，失败～<br>然后用aptget 的方式安装nodejs 再装hexo还是失败～<br>最后我开了一个ubuntu14.04的vm，在里面执行<br>apt-get install nodejs npm<br>apt-get install nodejs-legacy<br>sudo npm install -g hexo-cli</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2020/04/13/hello-world/" data-id="ck8xdtwx20007bq79u6vddtq8" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>


  
    <article id="post-how-to-unregister-service-from-eureka" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2020/04/13/how-to-unregister-service-from-eureka/" class="article-date">
  <time datetime="2020-04-12T17:40:04.000Z" itemprop="datePublished">2020-04-13</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2020/04/13/how-to-unregister-service-from-eureka/">如何安全优雅的把spring微服务从eureka上注销</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>重启服务的方法有很多种.<br>不推荐使用kill -9 ,粗暴的关闭会造成生产数据丢失.<br>这里研究了一下actuator,使用actuator的/pause,可以让服务从eureka上注销但是服务本身还提供服务,<br>也就是可以等30秒等待服务继续把未完成的工作做完,再kill -9 就可以了<br>spring boot1.x与spring 2.x的配置有不一样的地方,举例如下:</p>
<p>spring1.x<br>这里以1.4.1.RELEASE为例子<br>1、在pom文件中加入:</p>
<p> &lt;dependency&gt;<br>            &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;<br>            &lt;artifactId&gt;spring-boot-starter-actuator&lt;/artifactId&gt;<br>            &lt;version&gt;1.4.1.RELEASE&lt;/version&gt;<br>        &lt;/dependency&gt;</p>
<p>2、在application.yml中加入<br>endpoints:</p>
<pre><code># 启用pause端点
pause:
  enabled: true
# 启用restart端点，之所以要启用restart端点，是因为pause端点的启用依赖restart端点的启用
restart:
  enabled: true
</code></pre><p>启动服务后直接调用<br><a href="http://127.0.0.1:{port}/pause" target="_blank" rel="external">http://127.0.0.1:{port}/pause</a><br>从eureka上就是down的状态了</p>
<p>spring2.x:<br>1、在pom文件中加入:<br>       &lt;dependency&gt;<br>            &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;<br>            &lt;artifactId&gt;spring-boot-starter-actuator&lt;/artifactId&gt;<br>            &lt;version&gt;2.1.0.RELEASE&lt;/version&gt;<br>        &lt;/dependency&gt;<br>2、在application.yms中加入:</p>
<p>management:<br>  endpoint:</p>
<pre><code># 启用pause端点
pause:
  enabled: true
# 启用restart端点，之所以要启用restart端点，是因为pause端点的启用依赖restart端点的启用。详见：https://cloud.spring.io/spring-cloud-static/Finchley.SR2/single/spring-cloud.html#_endpoints
restart:
  enabled: true
</code></pre><p>  endpoints:<br>    web:<br>      exposure:<br>        include: pause,restart</p>
<p>启动服务后调用<br><a href="http://127.0.0.1:{port}/actuator/pause" target="_blank" rel="external">http://127.0.0.1:{port}/actuator/pause</a><br>从eureka上就是down的状态了</p>
<p>最后也测试了一下加入kafka consumer不断消费的情况.<br>调用pause后也停止消费了.<br>但是如果实现了org.springframework.cloud.stream.annotation.StreamListener<br>会造成服务注销后再次变为up的情况.<br>说明应该还是有情况会造成程序变为up,需要再研究一下</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2020/04/13/how-to-unregister-service-from-eureka/" data-id="ck8xdtwxb0008bq79ilae58tq" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/spring-eureka/">spring eureka</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-how-to-deploy-logstash-plugin-env" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2020/01/15/how-to-deploy-logstash-plugin-env/" class="article-date">
  <time datetime="2020-01-15T15:21:19.000Z" itemprop="datePublished">2020-01-15</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2020/01/15/how-to-deploy-logstash-plugin-env/">logstash jdbc plugin 优化</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>本文主要讲logstash 插件环境搭建<br>起因是我需要把mysql里一张1000w条数据的表导入到es中,于是采用了logstash数据泵导入到es.<br>logstash是有jdbc插件的,因为数据量巨大,一次性载入内存耗费太大,16g内存机器吃不消.所以采用分页的方式来搞.<br>配置如下:<br>input {<br>jdbc {<br>    jdbc_driver_library =&gt; “/opt/logstash-7.5.1/lib/mysql-connector-java-5.1.44.jar”<br>    jdbc_driver_class =&gt; “com.mysql.jdbc.Driver”<br>    jdbc_connection_string =&gt; “jdbc:mysql://xxxxxxx:33063/xxxxxx?autoReconnect=true&amp;failOverReadOnly=false&amp;maxReconnects=10”<br>    jdbc_user =&gt; “xxxxxx”<br>    jdbc_password =&gt; “xxx”<br>    jdbc_page_size =&gt; 1000<br>    jdbc_validate_connection =&gt; true<br>    jdbc_pool_timeout =&gt; 5<br>    jdbc_validation_timeout =&gt; 10<br>    jdbc_fetch_size =&gt; 1000<br>    connection_retry_attempts_wait_time =&gt; 5<br>    jdbc_paging_enabled =&gt; true<br>    statement =&gt; “SELECT * from a”<br>}<br>}<br>output {<br>  elasticsearch {<br>    index =&gt; “g2park_inout_record_his”<br>    hosts =&gt; [“localhost:9200”]<br>  }<br>  stdout { codec =&gt; rubydebug }<br>}</p>
<p>然后开始报错:<br>[2020-01-09T22:02:19,614][ERROR][logstash.inputs.jdbc     ][main] Java::ComMysqlJdbcExceptionsJdbc4::CommunicationsException: Communications link failure</p>
<p>The last packet successfully received from the server was 60,296 milliseconds ago.  The last packet sent successfully to the server was 60,108 milliseconds ago.: SELECT count(*) AS <code>count</code> FROM (SELECT inout_id,vehicle_no,create_time from a) AS <code>t1</code> LIMIT 1<br>[2020-01-09T22:02:19,666][WARN ][logstash.inputs.jdbc     ][main] Exception when executing JDBC query {:exception=&gt;#&lt;Sequel::DatabaseDisconnectError: Java::ComMysqlJdbcExceptionsJdbc4::CommunicationsException: Communications link failure</p>
<p>The last packet successfully received from the server was 60,296 milliseconds ago.  The last packet sent successfully to the server was 60,108 milliseconds ago.&gt;}<br>[2020-01-09T22:02:20,206][INFO ][logstash.runner          ] Logstash shut down.</p>
<p>一开始以为是参数设置问题,加上各种timeout设置也不起作用.<br>这里有个重点是分页logstash默认走的是子查询查到一个总和,如下所示:<br>select count(<em>) as <code>count</code> from (select </em> from a)<br>但是这要求把全部数据先查一遍.怀疑这里造成性能损耗太大.<br>于是换了个思路,改一下logstash-jdbc源码把……加个专门查总数查询的配置.<br>那因为logstash是juby的,那咱就先装ruby.<br>先安装rvm:<br>curl -sSL <a href="https://get.rvm.io" target="_blank" rel="external">https://get.rvm.io</a> | bash -s stable<br>logstash 必须安装jruby!!<br>安装jruby:<br>rvm install jruby<br>使用rvm的时候要先执行<br>source /etc/profile.d/rvm.sh<br>安装 bundle:<br>gem install bundler<br>这里有至少两个坑:<br>1、安装bundler的时候要注意给/usr/local/rvm以ruby当前用户的权限 chown和chgrp 要不然权限会有问题.<br>另外要随时跟进安装错误日志<br>比如期间遇到过git clone jruby hang住了,那就手动先git clone 到指定目录再执行 安装jruby.</p>
<p>2、网络!<br>装的时候有时需要走ss代理,有时不需要.需要多试试<br>比如步骤1的时候手动clone jruby然后 mvn build的时候,可以用ss代理提速<br>3、安装jruby的时候报错:<br>root@slave-node:~/logstash-input-jdbc# rvm install jruby<br>jruby-9.2.6.0 - #removing src/jruby-9.2.6.0..<br>Searching for binary rubies, this might take some time.<br>Found remote file <a href="https://repo1.maven.org/maven2/org/jruby/jruby-dist/9.2.6.0/jruby-dist-9.2.6.0-bin.tar.gz" target="_blank" rel="external">https://repo1.maven.org/maven2/org/jruby/jruby-dist/9.2.6.0/jruby-dist-9.2.6.0-bin.tar.gz</a><br>Checking requirements for ubuntu.<br>Requirements installation successful.<br>jruby-9.2.6.0 - #configure<br>jruby-9.2.6.0 - #download<br>  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current<br>                                 Dload  Upload   Total   Spent    Left  Speed<br>100 22.9M  100 22.9M    0     0  24821      0  0:16:09  0:16:09 –:–:– 25820<br>No checksum for downloaded archive, recording checksum in user configuration.<br>jruby-9.2.6.0 - #validate archive<br>jruby-9.2.6.0 - #extract<br>jruby-9.2.6.0 - #validate binary<br>jruby-9.2.6.0 - #setup<br>jruby-9.2.6.0 - #gemset created /usr/local/rvm/gems/jruby-9.2.6.0@global<br>jruby-9.2.6.0 - #importing gemset /usr/local/rvm/gemsets/jruby/global.gems.<br>Error running ‘gemset_import_list jruby-openssl jruby-launcher gem-wrappers rubygems-bundler rake rvm bundler’,<br>please read /usr/local/rvm/log/1578889834_jruby-9.2.6.0/gemsets.import.global.log<br>打开本地日志文件如下:<br>java.lang.LayerInstantiationException: Package jdk.internal.jrtfs in both module jrt.fs and module java.base<br>本人用的是jdk13,把jdk13下的jrt-fs.jar删掉后解决<br>4、安装jury的时候,报如下错误:<br>ERROR:  While executing gem … (Gem::Exception)<br>    Failed to find gems [“”] &gt;= 0<br>++ _failed+=(“${_gem} –version ${_version}”)<br>++ read _gem _version _platforms<br>++ ((  1 &gt; 0  ))<br>++ rvm_error ‘\n’\’’command gem pristine –extensions  –version ‘\’’ failed, you need to fix these gems manually.’<br>++ rvm_pretty_print stderr<br>++ case “${rvm_pretty_print_flag:=auto}” in<br>++ case “${TERM:-dumb}” in<br>++ case “$1” in<br>++ [[ -t 2 ]]<br>++ return 1<br>++ printf %b ‘\n’\’’command gem pristine –extensions  –version ‘\’’ failed, you need to fix these gems manually.\n’<br>++ return 1<br>++ return 1</p>
<p>不过我再次输入<br>rvm install jruby的时候提示我已经安装成功了,就没深究这个问题<br>5、删除本地插件的时候要注意执行命令的当前目录<br>必须在 logstash的主目录下!不能在bin下.要不然会报如下错误:<br>ERROR: Operation aborted, cannot remove plugin, message: The path <code>/opt/logstash-7.5.1/bin/logstash-core</code> does not exist.</p>
<p>本地插件编译和安装的步骤如下:<br>1、cd 到jdbc插件的根目录<br>2、切换到ruby的用户,比如我这里是es<br>su -s es<br>4、souce /etc/profile.d/rvm.sh<br>5、项目安装依赖</p>
<p>bundle install<br>这里安装依赖的时候 我忘了是不是要用ss代理了………<br>安装的时候如果有失败的情况会提示你做一些操作,比如:<br>URI::InvalidURIError: can not set user with opaque<br>An error occurred while installing rake (13.0.1), and Bundler<br>cannot continue.<br>Make sure that <code>gem install rake -v &#39;13.0.1&#39; --source &#39;https://rubygems.org/&#39;</code><br>succeeds before bundling.</p>
<p>In Gemfile:<br>  logstash-devutils was resolved to 1.3.6, which depends on<br>    rake</p>
<p>那就执行一下gem install rake -v ‘13.0.1’ –source ‘<a href="https://rubygems.org/" target="_blank" rel="external">https://rubygems.org/</a> 就好了<br>6、构建插件gem (在项目主目录下产生logstash-input-jdbc-4.3.19.gem文件)</p>
<p>gem build logstash-input-jdbc.gemspec</p>
<p>7、logstash卸载logstash-input-jdbc插件</p>
<p>/bin/logstash-plugin uninstall logstash-input-jdbc<br>8、删除</p>
<p>9、logstash安装本地插件 (先将构建成功的gem文件拷贝至logstash主目录的trade_gem文件夹中)</p>
<p>/bin/logstash-plugin install –no-verify –local ../logstash-input-jdbc-4.3.19.gem</p>
<p>参考资料:</p>
<p><a href="https://www.jianshu.com/p/84e807e1a575" target="_blank" rel="external">https://www.jianshu.com/p/84e807e1a575</a><br>感谢前人无私的分享,新版本的logstash代码接口已经有变化了这里附上新版本的代码如下:<br><a href="/2020/01/15/how-to-deploy-logstash-plugin-env/statement_handler.rb" title="statement_handler.rb">statement_handler.rb</a><br><br>完整工程如下:</p>
<a href="/2020/01/15/how-to-deploy-logstash-plugin-env/logstash-input-jdbc.zip" title="logstash-input-jdbc.zip">logstash-input-jdbc.zip</a>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2020/01/15/how-to-deploy-logstash-plugin-env/" data-id="ck8xdtwxg000bbq79j9ax1au5" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>


  
    <article id="post-k8s-install" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2019/11/11/k8s-install/" class="article-date">
  <time datetime="2019-11-11T13:43:45.000Z" itemprop="datePublished">2019-11-11</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2019/11/11/k8s-install/">k8s安装</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>今天遇到了k8s 在ubuntu18.04下安装的时候进行kubeadm init的时候拉取image失败的问题，<br>而且我设置了shadowshocket的代理http_proxy和https_proxy依然不行，<br>后来指定镜像库解决。命令如下：<br>kubeadm init –image-repository registry.aliyuncs.com/google_containers –ignore-preflight-errors=Swap<br>收官</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2019/11/11/k8s-install/" data-id="ck8xdtwxj000cbq79mxxm8mps" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/k8s/">k8s</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-tracingSystem" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2019/05/05/tracingSystem/" class="article-date">
  <time datetime="2019-05-05T11:29:34.000Z" itemprop="datePublished">2019-05-05</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2019/05/05/tracingSystem/">链路追踪系统之选型篇</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>Distributed Tracing最早起源于google的一片论文（链接见文末），之后各家都退出了自己的监控系统<br>但是数据格式并不统一，所以发起了open tracing标准，现在skywalking、zipkin和jaeger都遵循这一协议。<br>本篇是对近期链路跟踪系统选型的汇总。<br>主要选型对象有如下几个：zipkin、skywalking、CAT和pinpoint。<br>CAT的代码侵入性最大，并且过于臃肿可以reject，不过CAT的分析系统做的不错，可以借鉴一下。<br>pinpoint是naver出产的，团队规模小，并且需要hbase，韩国出品，所以也reject。<br>这里主要讨论zipkin与skywalking<br>zipkin架构图如下：<br><img src="/2019/05/05/tracingSystem/zipkin_architecture.png" alt="zipkin_overview" title="zipkin_overview"><br>在这里解读一下上图各个部分。<br>首先instrumented client 表示是经过扩展的客户端<br>Non-instrumented client 表示未经过扩展的客户端<br>instrumented client的一个请求叫做Span，一个完整调用链叫trace。<br>上图首先从instrumented client发送一个span到transport然后到collect，最后落盘存储用于ui展示。<br>发送到Non-instrumented client 并不会给collect发送记录存储。</p>
<p>zipkin主要有两个工程：<br><a href="https://github.com/openzipkin/brave" target="_blank" rel="external">https://github.com/openzipkin/brave</a><br><a href="https://github.com/apache/incubator-zipkin" target="_blank" rel="external">https://github.com/apache/incubator-zipkin</a><br>brave主要是zipkin的外围扩展。<br>现在主要支持一下扩展：<br><img src="/2019/05/05/tracingSystem/zipkin_instrument.png" alt="zipkin_instrument" title="zipkin_instrument"></p>
<p>ncubator-zipkin是zipkin的服务端。<br>包含如下部分：<br>server：服务端程序。<br>lens：web ui<br>storage：将数据存盘，现在支持mysql、cassandra和es<br>collect：收集器，包括三种收集模式：rest、kafka和rabbitmq。<br>正常来说应该使用mq异步的collect这样对程序本身性能影响最小。<br>zipkin的扩展是通过有针对性的埋点来实现的。<br>比如对于rest的扩展是通过spring的resttemplate拦截器在header里传traceid和spanid来实现的。<br>对于kafka消息的扩展也是在header里加参数来实现的。<br>所以zipkin本身依赖于第三方目标服务是否具有扩展性。<br>侵入性来说需要添加配置。</p>
<p>下面说说skywalking<br><img src="/2019/05/05/tracingSystem/skywalking.jpeg" alt="skywalking" title="skywalking"><br>首先tracing（跟踪）也好，Metric（指标）也罢，都是通过http或者grpc将采集起来的数据传送到服务器端的。<br>其数据存储包括mysql、es、tidb、h2和sharding<br>因为skywalking是通过AOP的方式动态织入代码的，所以并不受限于api扩展。<br>以下是已有的扩展：<br><img src="/2019/05/05/tracingSystem/skywalking_plugin1.png" alt="skywalking_plu1" title="skywalking_plu1"><br>&amp;<br><img src="/2019/05/05/tracingSystem/skywalking_plugin2.png" alt="skywalking plu2" title="skywalking plu2"><br>而且skywalking对于代码完全没有任何侵入性。<br>缺点是与我司运维人员了解到有两个问题：<br>1、存在es中的数据无法删除<br>2、如果es重启的话，必须所有agent都重启完毕，skywalking的监控上才会有数值。<br>这个要跟一下具体问题。</p>
<p>综合总结一下如下：<br>zipkin是基于api调用的，skywalking是基于AOP的。扩展性上讲skywalking要优于zipkin。<br>代码复杂度上来说，zipkin更容易上手一些，代码量也少。<br>性能上来说skywalking比zipkin要好。</p>
<p>相关zipkin与skywalking的理论基础如下：<br><a href="https://bigbully.github.io/Dapper-translation/" target="_blank" rel="external">https://bigbully.github.io/Dapper-translation/</a></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2019/05/05/tracingSystem/" data-id="ck8xdtx0v000wbq794ebqxh0t" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/zipkin-skywalking-cat/">zipkin skywalking cat</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-presto-source-code" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2019/01/07/presto-source-code/" class="article-date">
  <time datetime="2019-01-07T14:40:06.000Z" itemprop="datePublished">2019-01-07</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2019/01/07/presto-source-code/">码林啄木之presto阅读理解</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>Day 1:<br>一般来讲，我们要阅读一个项目的源代码，首先要从网上找一下有没有读过该项目，次之去官网上找文档，最后再读细节。<br>我们第一天先讲讲presto的启动命令、架构以及代码架构。<br>首先祭出架构图：摘自官网<br><img src="/2019/01/07/presto-source-code/presto-overview.png" alt="overview" title="overview"><br>也就是说presto主要有如下几个部分组成：<br>CLI：就是命令行，用于提交查询，通俗点的理解就是发起rest请求<br>服务器端有两种节点：coordinators和worker。一下简称C节点和W节点<br>首先来看C节点：<br>C节点主要用于解析语句，生成请求接话，管理W节点。<br>W节点用于执行请求计划，处理数据，W节点从连接器读取数据并彼此交换数据。<br>C节点与W节点通过rest API来通信。<br>以上就是这个overview图的最直白简洁。<br>然鹅还有一些关键概念。<br>一、presto链接数据源的时候需要什么参数？<br>1、Connector连接器，连接器顾名思义，作用相当于jdbc中的driver。<br>2、Catalog，一个数据源就是catalog，可以是mysql，可以是hive等等<br>3、Schema，类似mysql中的database，一个catalog可以有多个schema<br>4、Table，就是表！<br>二、presto的查询模型<br>1、statement<br>语句，就是sql语句。是一个静态的概念。<br>2、query<br>查询，通过语句生成的query，是一个动态的执行。<br>3、stage<br>一个stage代表查询执行计划的一个部分。<br>stage有四种类型：<br>Coordinator_Only:<br>用于执行DDM或者DDL表的创建与更改<br>Single:<br>这种类型用于聚合子Stage的输出数据<br>Fixed:<br>用于接受其子Stage产生的数据并在集群中对这些数据进行分布式的聚合或者分组计算。<br>Source:<br>用于直接链接数据源，从数据源读取数据，在读取数据的时候，该阶段也会根据presto对查询执行计划的优化完成相关的断言下发和条件过滤等<br>Exchange：<br>交换，stage之间用于数据交换用的。场景包括下游从上游读取数据。<br>Task:<br>就是stage切分出来运行在W节点上的具体任务。<br>Split：<br>就是数据集上的子集。下文说道的driver就是在split上做操作<br>Driver：<br>一个task包括一个或者多个Driver。就是一个split的一系列operator集合。<br>Page是presto中处理的最小数据单元。一个Page对象包含多个Block对象，而每个Block对象是一个字节数组，存储一个字段的若干行。多个block横切的一行是真实的一行数据。</p>
<p>以上，就是我们对于presto源码阅读理解的重点喽</p>
<p>再来看看presto的启动命令吧<br>首先是C节点：<br>-Dnode.data-dir=/mnt/disk1/log/presto -Dnode.id=xxxxxxxxxxx -Dnode.environment=production -Dlog.enable-console=false -Dlog.levels-file=/etc/ecm/presto-conf-0.188/log.properties -Dconfig=/etc/ecm/presto-conf-0.188/config.properties com.facebook.presto.server.PrestoServer<br>然后是W节点<br>-Dnode.data-dir=/mnt/disk1/log/presto -Dnode.id=xxxxxxx -Dnode.environment=production -Dlog.enable-console=false -Dlog.levels-file=/etc/ecm/presto-conf-0.188/log.properties -Dconfig=/etc/ecm/presto-conf-0.188/config.properties com.facebook.presto.server.PrestoServer<br>命令是一样滴哦，惊不惊喜，意不意外！<br>差别就在properties里了<br>里面有个coordinator属性为true则为C节点，false为W节点<br>本来想今天把服务跑起来看看的，结果意外了。<br>连续两次意外了。<br>第一次的时候发现需要装antlr插件也就是g4插件。。。。<br>第二次又意外了 jdk版本不够。。。<br>Presto requires Java 8u151+ (found 1.8.0_144)</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2019/01/07/presto-source-code/" data-id="ck8xdtwyh000lbq79ozyzogfo" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>


  
    <article id="post-sqoop-huizong" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2018/12/20/sqoop-huizong/" class="article-date">
  <time datetime="2018-12-20T15:38:06.000Z" itemprop="datePublished">2018-12-20</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2018/12/20/sqoop-huizong/">sqoop问题汇总</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>sqoop最近遇到的问题在这里汇总一下</p>
<p>先说环境<br>阿里emr-3.12.0<br>sqoop 1.4.7<br>1、字段类型问题<br>首先遇到的一个问题是通过sqoop从mysql导入到hive的时候，如果不指定列的类型，到hive默认都是字符串。<br>这样做日期计算的时候会稍微麻烦一些。<br>解决办法：<br>通过–map-column-java 和–map-column-hive 来指定类型<br>参考资料：<br><a href="https://blog.csdn.net/xianjie0318/article/details/79300435" target="_blank" rel="external">https://blog.csdn.net/xianjie0318/article/details/79300435</a><br>2、mysql导入hive报GC overhead limit exceeded<br>解决方法：<br>mysql connection连接字符串加上：<br>?dontTrackOpenResources=true&amp;defaultFetchSize=1000&amp;useCursorFetch=true<br>参考资料：<br><a href="https://www.cnblogs.com/chwilliam85/p/9693276.html" target="_blank" rel="external">https://www.cnblogs.com/chwilliam85/p/9693276.html</a><br>3、通过sqoop增量地从mysql导入到hive<br>这个我一开始用–append 模式导入的，然后报错如下：<br>Append mode for hive imports is not  yet supported. Please remove the parameter –append-mode<br>我提了一个工单给阿里客服，<br>提供了一个参考链接：<br><a href="https://community.hortonworks.com/questions/11373/sqoop-incremental-import-in-hive-i-get-error-messa.html" target="_blank" rel="external">https://community.hortonworks.com/questions/11373/sqoop-incremental-import-in-hive-i-get-error-messa.html</a><br>这种方式是通过创建外部表，然后增量写入到hdfs来实现的。<br>这里有个注意的地方是要配合-check-column来使用，而这个字段可以是数值型的id，也可以是时间戳！<br>参考资料如下：<br><a href="https://stackoverflow.com/questions/41238887/sqoop-incremental-import-error-using-date-column" target="_blank" rel="external">https://stackoverflow.com/questions/41238887/sqoop-incremental-import-error-using-date-column</a><br>以时间戳为增量的sqoop基本设计流程如下：<br>首先创建外部表：<br>CREATE EXTERNAL TABLE test (id string,create_time timestamp,reservation1 string,reservation2 string,reservation3 string,reservation4 string,reservation5 string,reservation6 string,reservation7 string,reservation8 string,reservation9 string,reservation10 string) ROW FORMAT DELIMITED FIELDS TERMINATED BY ‘,’ STORED AS TEXTFILE location ‘/user/wfliu/test’ ;<br>然后全量导入mysql数据库表，如下：</p>
<p>sqoop import -Dorg.apache.sqoop.splitter.allow_text_splitter=true –connect ‘jdbc:mysql://host:3306/test?useUnicode=true&amp;characterEncoding=utf-8&amp;zeroDateTimeBehavior=convertToNull&amp;dontTrackOpenResources=true&amp;defaultFetchSize=1000&amp;useCursorFetch=true’ –driver com.mysql.jdbc.Driver –username test –password xxx –incremental append –table test –target-dir /user/wfliu/test -check-column update_time -m 1 –hive-drop-import-delims –verbose -m 1<br>最后定一个定时执行增量的sqoop任务：<br>sqoop import -Dorg.apache.sqoop.splitter.allow_text_splitter=true –connect ‘jdbc:mysql://host:3306/test?useUnicode=true&amp;characterEncoding=utf-8&amp;zeroDateTimeBehavior=convertToNull&amp;dontTrackOpenResources=true&amp;defaultFetchSize=1000&amp;useCursorFetch=true’ –driver com.mysql.jdbc.Driver –username test –password xxx –table test –target-dir /user/wfliu/test –incremental lastmodified -check-column create_time -m 1 –last-value “${lastvalue}” –append –hive-drop-import-delims –verbose -m 1<br>其中lastvalue是emr中配置的参数可以为${yyyy-MM-dd 00:00:00-1d}。为增量导入截止到每天凌晨12点的数据。<br>4、新版“数据开发”功能支持参数化<br>以前用老版的“作业调度”sqoop并不支持参数<br>用新版的“数据开发”<br><img src="/2018/12/20/sqoop-huizong/1.png" alt="参数化" title="参数化"><br>5、如何删除外部表中的数据<br>今天遇到的问题是因为导入数据有错误想清空外部表的数据<br>一开始用<br>delete from xxx的方式，不行<br>然后用trancat table xxx 也不行<br>这两种都不能删除外部表的数据<br>那怎么办呢<br>答案：先把外部表变成内部表再删除<br>ALTER TABLE xxx SET TBLPROPERTIES(‘EXTERNAL’=’False’);<br>然后<br>truncate table xxx;<br>这样就可以啦！<br>参考资料：<br><a href="https://blog.csdn.net/tengxing007/article/details/81211500" target="_blank" rel="external">https://blog.csdn.net/tengxing007/article/details/81211500</a></p>
<p>另外sqoop也是可以直接直接创建hive表的<br>sqoop create-hive-table<br>可以指定–hive-overwrite来覆盖表中原有数据<br>参考资料：<br><a href="https://segmentfault.com/a/1190000002532293" target="_blank" rel="external">https://segmentfault.com/a/1190000002532293</a></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2018/12/20/sqoop-huizong/" data-id="ck8xdtwzf000sbq791jzti85d" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>


  
    <article id="post-how-to-add-col-spark-dataframe" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2018/12/17/how-to-add-col-spark-dataframe/" class="article-date">
  <time datetime="2018-12-17T15:22:06.000Z" itemprop="datePublished">2018-12-17</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2018/12/17/how-to-add-col-spark-dataframe/">如何给spark表添加一列</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>刚开始接触spark的时候遇到一个特别中二的问题。<br>我从两张表里查数据，这两张表里都只有一行，如何把这两张表合并起来呢？？？？<br>然后我特缺心眼的从网上搜，spark添加列的方式。<br>好吧。。。前情交代完毕。<br>说说过程<br>解决方式特简单，用join就tm可以了。<br>好了<br>话题说到这里就结束哦了吗？？？？<br>那我还写个鸡毛！<br>之后我翻阅了一下我搜到的如何增加一列的问题！<br>首先先看如何增加一个常数列<br>代码如下：<br>        val tempDataFrame = spark.createDataFrame(Seq(<br>            (1, “asf”),<br>            (2, “2143”),<br>            (3, “rfds”)<br>        )).toDF(“id”, “content”)<br>        val addColDataframe = tempDataFrame.withColumn(“col”, tempDataFrame(“id”)*2)<br>        addColDataframe.show(10,false)</p>
<p>+—+——-+—+<br>|id |content|col|<br>+—+——-+—+<br>|1  |asf    |2  |<br>|2  |2143   |4  |<br>|3  |rfds   |6  |<br>+—+——-+—+</p>
<p>然后用udf来实现一个复杂一些的：<br>        import scala.util.Try<br>        import org.apache.spark.sql.functions._<br>        val tempDataFrame = spark.createDataFrame(Seq(<br>            (“1”, “2”),<br>            (“2”, “3”),<br>            (“3”, “1”)<br>        )).toDF(“content1”, “content2”)</p>
<pre><code>val code = (arg1: String, arg2: String) =&gt; {
    Try(if (arg1.toInt &gt; arg2.toInt) &quot;arg1&gt;arg2&quot; else &quot;arg1&lt;=arg2&quot;).getOrElse(&quot;error&quot;)
}
val compareUdf = udf(code)

val addColDataframe = tempDataFrame.withColumn(&quot;compare&quot;, compareUdf(tempDataFrame(&quot;content1&quot;),tempDataFrame(&quot;content2&quot;)))
addColDataframe.show(10, false)
</code></pre><p>+——–+——–+———-+<br>|content1|content2|compare   |<br>+——–+——–+———-+<br>|1       |2       |arg1&lt;=arg2|<br>|2       |3       |arg1&lt;=arg2|<br>|3       |1       |arg1&gt;arg2 |<br>+——–+——–+———-+</p>
<p>这样添加的一列就可以是比较复杂的了。<br>参考资料如下：<br><a href="http://www.cnblogs.com/TTyb/p/7169148.html" target="_blank" rel="external">http://www.cnblogs.com/TTyb/p/7169148.html</a></p>
<p>让我们再深入一些！<br>如何添加唯一的一列，对！就是类似主键的那样<br>两种方式：<br>第一种 使用zipWithUniqueId获取id 并重建 DataFrame.<br>        import spark.implicits._<br>        import org.apache.spark.sql.Row<br>        import org.apache.spark.sql.types.{StructType, StructField, LongType}</p>
<pre><code>val df =Seq((&quot;a&quot;, -1.0), (&quot;b&quot;, -2.0), (&quot;c&quot;, -3.0)).toDF(&quot;foo&quot;, &quot;bar&quot;)
// 获取df 的表头
val s = df.schema

// 将原表转换成带有rdd,
//再转换成带有id的rdd,
//再展开成Seq方便转化成 Dataframe
val rows = df.rdd.zipWithUniqueId.map{case (r: Row, id: Long) =&gt; Row.fromSeq(id +: r.toSeq)}

// 再由 row 根据原表头进行转换
val dfWithPK = spark.createDataFrame( rows, StructType(StructField(&quot;id&quot;, LongType, false) +: s.fields))
dfWithPK.show
</code></pre><p>+—+—+—-+<br>| id|foo| bar|<br>+—+—+—-+<br>|  0|  a|-1.0|<br>|  1|  b|-2.0|<br>|  2|  c|-3.0|<br>+—+—+—-+</p>
<p>以上可以实现自增列。<br>第二种：直接使用spark 自己的api,monotonicallyIncreasingId<br>        val df =Seq((“a”, -1.0), (“b”, -2.0), (“c”, -3.0)).toDF(“foo”, “bar”)<br>        import org.apache.spark.sql.functions.monotonically_increasing_id<br>        df.withColumn(“id”, monotonically_increasing_id).show()</p>
<p>+—+—-+—+<br>|foo| bar| id|<br>+—+—-+—+<br>|  a|-1.0|  0|<br>|  b|-2.0|  1|<br>|  c|-3.0|  2|<br>+—+—-+—+<br>注意这里也是自增列</p>
<p>还有第三种：使用createDateFrame<br>        val input =Seq((“a”, 100.0), (“b”, -2.0), (“c”, -3.0)).toDF(“foo”, “bar”)<br>        var critValueR = 0<br>        var critValueL = -10</p>
<pre><code>val trdd = input.select(&quot;bar&quot;).rdd.map(x=&gt;{
    if (x.get(0).toString().toDouble &gt; critValueR || x.get(0).toString().toDouble &lt; critValueL)
        Row(x.get(0).toString().toDouble,&quot;F&quot;)
    else Row(x.get(0).toString().toDouble,&quot;T&quot;)
})
val schema = input.select(&quot;bar&quot;).schema.add(&quot;flag&quot;, StringType, true)
val sample3 = spark.createDataFrame(trdd, schema).distinct().withColumnRenamed(&quot;flag&quot;, &quot;idx&quot;)
sample3.show
</code></pre><p>+—–+—+<br>|  bar|idx|<br>+—–+—+<br>| -3.0|  T|<br>|100.0|  F|<br>| -2.0|  T|<br>+—–+—+</p>
<p>觉得比较直观且开放性大的还是实用udf。</p>
<p>参考资料：<br><a href="https://www.jianshu.com/p/7e6e406dd15b" target="_blank" rel="external">https://www.jianshu.com/p/7e6e406dd15b</a></p>
<p>。。。。。。。。。。你以为这就完了？。。。。。。<br>那我还写个毛线。。。。。。<br>后来又发现了对列的拆分和合并</p>
<p>1、如何合并列：<br>        val df =Seq((“Ming”,20,”15552211521”), (“hong”,19,”13287994007”), (“zhi”,21,”15552211523”)).toDF(“name”,”age”, “phone”)<br>        val separator = “,”<br>        df.map(<em>.toSeq.foldLeft(“”)(</em> + separator + _).substring(1)).show()<br>+——————-+<br>|              value|<br>+——————-+<br>|Ming,20,15552211521|<br>|hong,19,13287994007|<br>| zhi,21,15552211523|<br>+——————-+<br>也可以使用concat<em>ws:<br>        import org.apache.spark.sql.functions.</em><br>        df.select(concat_ws(separator, $”name”, $”age”, $”phone”).cast(StringType).as(“value”)).show()</p>
<p>结果一样<br>第三种实用udf！<br>       def mergeCols(row: Row): String = {<br>            row.toSeq.foldLeft(“”)(<em> + separator + </em>).substring(1)<br>        }</p>
<pre><code>val mergeColsUDF = udf(mergeCols _)
df.select(mergeColsUDF(struct($&quot;name&quot;, $&quot;age&quot;, $&quot;phone&quot;)).as(&quot;value&quot;)).show()
</code></pre><p>结果也是同上</p>
<p>2、如何拆分列呢？<br>a、实用split<br>    val df =Seq((“Ming,20,15552211521”), (“hong,19,13287994007”), (“zhi,21,15552211523”)).toDF(“value”)<br>    val separator = “,”<br>    lazy val first = df.first()</p>
<pre><code>val numAttrs = first.toString().split(separator).length
val attrs = Array.tabulate(numAttrs)(n =&gt; &quot;col_&quot; + n)
//按指定分隔符拆分value列，生成splitCols列
var newDF = df.withColumn(&quot;splitCols&quot;, split($&quot;value&quot;, separator))
attrs.zipWithIndex.foreach(x =&gt; {
  newDF = newDF.withColumn(x._1, $&quot;splitCols&quot;.getItem(x._2))
})
newDF.show()
</code></pre><p>+——————-+——————–+—–+—–+———–+<br>|              value|           splitCols|col_0|col_1|      col_2|<br>+——————-+——————–+—–+—–+———–+<br>|Ming,20,15552211521|[Ming, 20, 155522…| Ming|   20|15552211521|<br>|hong,19,13287994007|[hong, 19, 132879…| hong|   19|13287994007|<br>| zhi,21,15552211523|[zhi, 21, 1555221…|  zhi|   21|15552211523|<br>+——————-+——————–+—–+—–+———–+</p>
<p>b、实用udf：<br>        import org.apache.spark.ml.attribute.{Attribute, NumericAttribute}</p>
<pre><code>val df =Seq((&quot;Ming,20,15552211521&quot;), (&quot;hong,19,13287994007&quot;), (&quot;zhi,21,15552211523&quot;)).toDF(&quot;value&quot;)
val separator = &quot;,&quot;
lazy val first = df.first()
val attributes: Array[Attribute] = {
    val numAttrs = first.toString().split(separator).length
    //生成attributes
    Array.tabulate(numAttrs)(i =&gt; NumericAttribute.defaultAttr.withName(&quot;value&quot; + &quot;_&quot; + i))
}
//创建多列数据
val fieldCols = attributes.zipWithIndex.map(x =&gt; {
    val assembleFunc = udf {
        str: String =&gt;
            str.split(separator)(x._2)
    }
    assembleFunc(df(&quot;value&quot;).cast(StringType)).as(x._1.name.get, x._1.toMetadata())
})
//合并数据
df.select(col(&quot;*&quot;) +: fieldCols: _*).show()
</code></pre><p>+——————-+——————–+—–+—–+———–+<br>|              value|           splitCols|col_0|col_1|      col_2|<br>+——————-+——————–+—–+—–+———–+<br>|Ming,20,15552211521|[Ming, 20, 155522…| Ming|   20|15552211521|<br>|hong,19,13287994007|[hong, 19, 132879…| hong|   19|13287994007|<br>| zhi,21,15552211523|[zhi, 21, 1555221…|  zhi|   21|15552211523|<br>+——————-+——————–+—–+—–+———–+<br>参考资料：<br><a href="http://www.cnblogs.com/itboys/p/9813934.html" target="_blank" rel="external">http://www.cnblogs.com/itboys/p/9813934.html</a></p>
<p>综上所述，udf还是很牛逼的。</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2018/12/17/how-to-add-col-spark-dataframe/" data-id="ck8xdtww40000bq79zr5x1nqx" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>


  
    <article id="post-mysql-HOUR-OF-DAY-2-3" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2018/12/17/mysql-HOUR-OF-DAY-2-3/" class="article-date">
  <time datetime="2018-12-17T15:19:48.000Z" itemprop="datePublished">2018-12-17</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2018/12/17/mysql-HOUR-OF-DAY-2-3/">mysql-HOUR_OF_DAY-2-3</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>今天开发spark应用遇到了一个问题：<br>java.lang.IllegalArgumentException: HOUR_OF_DAY: 2 -&gt; 3</p>
<p>在数据库配置文件中加上 &amp;serverTimezone=Asia/Shanghai 即可~</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2018/12/17/mysql-HOUR-OF-DAY-2-3/" data-id="ck8xdtwxo000fbq79eua6i71m" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>


  
    <article id="post-spark-sql-count-as-issue" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2018/12/13/spark-sql-count-as-issue/" class="article-date">
  <time datetime="2018-12-13T15:25:33.000Z" itemprop="datePublished">2018-12-13</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2018/12/13/spark-sql-count-as-issue/">spark-sql-count-as-issue</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>今天遇到了一个奇怪的问题，就是用as的时候，如果别名全部为数字，那么会报错，具体说明如下：<br>准备的数据如下：<br><img src="/2018/12/13/spark-sql-count-as-issue/4.png" alt="data" title="data"><br>如果设置成2或者2-2，那么会报如下错误。如果是’2’效果一样<br><img src="/2018/12/13/spark-sql-count-as-issue/2.png" alt="wrong" title="wrong"><br><img src="/2018/12/13/spark-sql-count-as-issue/3.png" alt="exception" title="exception"><br>如果设置成2_2，则查询正常<br><img src="/2018/12/13/spark-sql-count-as-issue/1.png" alt="right" title="right"></p>
<p>用的spark和spark sql为2.3.1，pom如下：</p>
<pre><code>&lt;dependency&gt;
    &lt;groupId&gt;org.apache.spark&lt;/groupId&gt;
    &lt;artifactId&gt;spark-core_2.11&lt;/artifactId&gt;
    &lt;version&gt;2.3.1&lt;/version&gt;
    &lt;scope&gt;${spark.scope}&lt;/scope&gt;
&lt;/dependency&gt;

&lt;dependency&gt;
    &lt;groupId&gt;org.apache.spark&lt;/groupId&gt;
    &lt;artifactId&gt;spark-sql_2.11&lt;/artifactId&gt;
    &lt;version&gt;2.3.1&lt;/version&gt;
    &lt;scope&gt;${spark.scope}&lt;/scope&gt;
&lt;/dependency&gt;
</code></pre><p>暂时先用’_’，还没有具体的解决办法</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2018/12/13/spark-sql-count-as-issue/" data-id="ck8xdtwyu000pbq79bksnphwx" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>


  


  <nav id="page-nav">
    <span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><a class="extend next" rel="next" href="/page/2/">__('next') &raquo;</a>
  </nav>
</section>
        
          <aside id="sidebar">
  
    

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tags</h3>
    <div class="widget">
      <ul class="tag-list"><li class="tag-list-item"><a class="tag-list-link" href="/tags/k8s/">k8s</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/openstack-neutron/">openstack neutron</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/spark-classload-ClassNotFound/">spark classload ClassNotFound</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/spring/">spring</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/spring-eureka/">spring eureka</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/tensroflow/">tensroflow</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/zipkin-skywalking-cat/">zipkin skywalking cat</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/阿里云-sgoop-presto/">阿里云 sgoop presto</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tag Cloud</h3>
    <div class="widget tagcloud">
      <a href="/tags/k8s/" style="font-size: 10px;">k8s</a> <a href="/tags/openstack-neutron/" style="font-size: 10px;">openstack neutron</a> <a href="/tags/spark-classload-ClassNotFound/" style="font-size: 10px;">spark classload ClassNotFound</a> <a href="/tags/spring/" style="font-size: 20px;">spring</a> <a href="/tags/spring-eureka/" style="font-size: 10px;">spring eureka</a> <a href="/tags/tensroflow/" style="font-size: 10px;">tensroflow</a> <a href="/tags/zipkin-skywalking-cat/" style="font-size: 10px;">zipkin skywalking cat</a> <a href="/tags/阿里云-sgoop-presto/" style="font-size: 10px;">阿里云 sgoop presto</a>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/04/">April 2020</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/01/">January 2020</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/11/">November 2019</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/05/">May 2019</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/01/">January 2019</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/12/">December 2018</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/10/">October 2018</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/09/">September 2018</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/05/">May 2017</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/12/">December 2016</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/11/">November 2016</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2020/04/13/hello-world/">如何搭建新版本hexo</a>
          </li>
        
          <li>
            <a href="/2020/04/13/how-to-unregister-service-from-eureka/">如何安全优雅的把spring微服务从eureka上注销</a>
          </li>
        
          <li>
            <a href="/2020/01/15/how-to-deploy-logstash-plugin-env/">logstash jdbc plugin 优化</a>
          </li>
        
          <li>
            <a href="/2019/11/11/k8s-install/">k8s安装</a>
          </li>
        
          <li>
            <a href="/2019/05/05/tracingSystem/">链路追踪系统之选型篇</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2020 wfliu<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>
    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    

<script src="//ajax.googleapis.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>


  <link rel="stylesheet" href="/fancybox/jquery.fancybox.css">
  <script src="/fancybox/jquery.fancybox.pack.js"></script>


<script src="/js/script.js"></script>

  </div>
</body>
</html>