<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  <title>肆叔的酱油</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta property="og:type" content="website">
<meta property="og:title" content="肆叔的酱油">
<meta property="og:url" content="http://yoursite.com/index.html">
<meta property="og:site_name" content="肆叔的酱油">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="肆叔的酱油">
  
    <link rel="alternate" href="/atom.xml" title="肆叔的酱油" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  
  
    <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <link rel="stylesheet" href="/css/style.css">
  

</head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">肆叔的酱油</a>
      </h1>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="Search"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" results="0" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://yoursite.com"></form>
      </div>
    </div>
  </div>
</header>
      <div class="outer">
        <section id="main">
  
    <article id="post-hello-world" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2018/12/20/hello-world/" class="article-date">
  <time datetime="2018-12-20T15:55:46.855Z" itemprop="datePublished">2018-12-20</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2018/12/20/hello-world/">如何搭建新版本hexo</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>我先是在debian8.6上用官网的方式搭建的，失败～<br>然后用aptget 的方式安装nodejs 再装hexo还是失败～<br>最后我开了一个ubuntu14.04的vm，在里面执行<br>apt-get install nodejs npm<br>apt-get install nodejs-legacy<br>sudo npm install -g hexo-cli</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2018/12/20/hello-world/" data-id="cjpwsfig8000397798s1cw7ek" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>


  
    <article id="post-sqoop-huizong" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2018/12/20/sqoop-huizong/" class="article-date">
  <time datetime="2018-12-20T15:38:06.000Z" itemprop="datePublished">2018-12-20</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2018/12/20/sqoop-huizong/">sqoop问题汇总</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>sqoop最近遇到的问题在这里汇总一下</p>
<p>先说环境<br>阿里emr-3.12.0<br>sqoop 1.4.7<br>1、字段类型问题<br>首先遇到的一个问题是通过sqoop从mysql导入到hive的时候，如果不指定列的类型，到hive默认都是字符串。<br>这样做日期计算的时候会稍微麻烦一些。<br>解决办法：<br>通过–map-column-java 和–map-column-hive 来指定类型<br>参考资料：<br><a href="https://blog.csdn.net/xianjie0318/article/details/79300435" target="_blank" rel="external">https://blog.csdn.net/xianjie0318/article/details/79300435</a><br>2、mysql导入hive报GC overhead limit exceeded<br>解决方法：<br>mysql connection连接字符串加上：<br>?dontTrackOpenResources=true&amp;defaultFetchSize=1000&amp;useCursorFetch=true<br>参考资料：<br><a href="https://www.cnblogs.com/chwilliam85/p/9693276.html" target="_blank" rel="external">https://www.cnblogs.com/chwilliam85/p/9693276.html</a><br>3、通过sqoop增量地从mysql导入到hive<br>这个我一开始用–append 模式导入的，然后报错如下：<br>Append mode for hive imports is not  yet supported. Please remove the parameter –append-mode<br>我提了一个工单给阿里客服，<br>提供了一个参考链接：<br><a href="https://community.hortonworks.com/questions/11373/sqoop-incremental-import-in-hive-i-get-error-messa.html" target="_blank" rel="external">https://community.hortonworks.com/questions/11373/sqoop-incremental-import-in-hive-i-get-error-messa.html</a><br>这种方式是通过创建外部表，然后增量写入到hdfs来实现的。<br>这里有个注意的地方是要配合-check-column来使用，而这个字段可以是数值型的id，也可以是时间戳！<br>参考资料如下：<br><a href="https://stackoverflow.com/questions/41238887/sqoop-incremental-import-error-using-date-column" target="_blank" rel="external">https://stackoverflow.com/questions/41238887/sqoop-incremental-import-error-using-date-column</a><br>以时间戳为增量的sqoop基本设计流程如下：<br>首先创建外部表：<br>CREATE EXTERNAL TABLE test (id string,create_time timestamp,reservation1 string,reservation2 string,reservation3 string,reservation4 string,reservation5 string,reservation6 string,reservation7 string,reservation8 string,reservation9 string,reservation10 string) ROW FORMAT DELIMITED FIELDS TERMINATED BY ‘,’ STORED AS TEXTFILE location ‘/user/wfliu/test’ ;<br>然后全量导入mysql数据库表，如下：</p>
<p>sqoop import -Dorg.apache.sqoop.splitter.allow_text_splitter=true –connect ‘jdbc:mysql://host:3306/test?useUnicode=true&amp;characterEncoding=utf-8&amp;zeroDateTimeBehavior=convertToNull&amp;dontTrackOpenResources=true&amp;defaultFetchSize=1000&amp;useCursorFetch=true’ –driver com.mysql.jdbc.Driver –username test –password xxx –incremental append –table test –target-dir /user/wfliu/test -check-column update_time -m 1 –hive-drop-import-delims –verbose -m 1<br>最后定一个定时执行增量的sqoop任务：<br>sqoop import -Dorg.apache.sqoop.splitter.allow_text_splitter=true –connect ‘jdbc:mysql://host:3306/test?useUnicode=true&amp;characterEncoding=utf-8&amp;zeroDateTimeBehavior=convertToNull&amp;dontTrackOpenResources=true&amp;defaultFetchSize=1000&amp;useCursorFetch=true’ –driver com.mysql.jdbc.Driver –username test –password xxx –table test –target-dir /user/wfliu/test –incremental lastmodified -check-column create_time -m 1 –last-value “${lastvalue}” –append –hive-drop-import-delims –verbose -m 1<br>其中lastvalue是emr中配置的参数可以为${yyyy-MM-dd 00:00:00-1d}。为增量导入截止到每天凌晨12点的数据。<br>4、新版“数据开发”功能支持参数化<br>以前用老版的“作业调度”sqoop并不支持参数<br>用新版的“数据开发”<br><img src="/2018/12/20/sqoop-huizong/1.png" alt="参数化" title="参数化"><br>5、如何删除外部表中的数据<br>今天遇到的问题是因为导入数据有错误想清空外部表的数据<br>一开始用<br>delete from xxx的方式，不行<br>然后用trancat table xxx 也不行<br>这两种都不能删除外部表的数据<br>那怎么办呢<br>答案：先把外部表变成内部表再删除<br>ALTER TABLE xxx SET TBLPROPERTIES(‘EXTERNAL’=’False’);<br>然后<br>truncate table xxx;<br>这样就可以啦！<br>参考资料：<br><a href="https://blog.csdn.net/tengxing007/article/details/81211500" target="_blank" rel="external">https://blog.csdn.net/tengxing007/article/details/81211500</a></p>
<p>另外sqoop也是可以直接直接创建hive表的<br>sqoop create-hive-table<br>可以指定–hive-overwrite来覆盖表中原有数据<br>参考资料：<br><a href="https://segmentfault.com/a/1190000002532293" target="_blank" rel="external">https://segmentfault.com/a/1190000002532293</a></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2018/12/20/sqoop-huizong/" data-id="cjpwsfirb000h9779icpmau40" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>


  
    <article id="post-how-to-add-col-spark-dataframe" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2018/12/17/how-to-add-col-spark-dataframe/" class="article-date">
  <time datetime="2018-12-17T15:22:06.000Z" itemprop="datePublished">2018-12-17</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2018/12/17/how-to-add-col-spark-dataframe/">如何给spark表添加一列</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>刚开始接触spark的时候遇到一个特别中二的问题。<br>我从两张表里查数据，这两张表里都只有一行，如何把这两张表合并起来呢？？？？<br>然后我特缺心眼的从网上搜，spark添加列的方式。<br>好吧。。。前情交代完毕。<br>说说过程<br>解决方式特简单，用join就tm可以了。<br>好了<br>话题说到这里就结束哦了吗？？？？<br>那我还写个鸡毛！<br>之后我翻阅了一下我搜到的如何增加一列的问题！<br>首先先看如何增加一个常数列<br>代码如下：<br>        val tempDataFrame = spark.createDataFrame(Seq(<br>            (1, “asf”),<br>            (2, “2143”),<br>            (3, “rfds”)<br>        )).toDF(“id”, “content”)<br>        val addColDataframe = tempDataFrame.withColumn(“col”, tempDataFrame(“id”)*2)<br>        addColDataframe.show(10,false)</p>
<p>+—+——-+—+<br>|id |content|col|<br>+—+——-+—+<br>|1  |asf    |2  |<br>|2  |2143   |4  |<br>|3  |rfds   |6  |<br>+—+——-+—+</p>
<p>然后用udf来实现一个复杂一些的：<br>        import scala.util.Try<br>        import org.apache.spark.sql.functions._<br>        val tempDataFrame = spark.createDataFrame(Seq(<br>            (“1”, “2”),<br>            (“2”, “3”),<br>            (“3”, “1”)<br>        )).toDF(“content1”, “content2”)</p>
<pre><code>val code = (arg1: String, arg2: String) =&gt; {
    Try(if (arg1.toInt &gt; arg2.toInt) &quot;arg1&gt;arg2&quot; else &quot;arg1&lt;=arg2&quot;).getOrElse(&quot;error&quot;)
}
val compareUdf = udf(code)

val addColDataframe = tempDataFrame.withColumn(&quot;compare&quot;, compareUdf(tempDataFrame(&quot;content1&quot;),tempDataFrame(&quot;content2&quot;)))
addColDataframe.show(10, false)
</code></pre><p>+——–+——–+———-+<br>|content1|content2|compare   |<br>+——–+——–+———-+<br>|1       |2       |arg1&lt;=arg2|<br>|2       |3       |arg1&lt;=arg2|<br>|3       |1       |arg1&gt;arg2 |<br>+——–+——–+———-+</p>
<p>这样添加的一列就可以是比较复杂的了。<br>参考资料如下：<br><a href="http://www.cnblogs.com/TTyb/p/7169148.html" target="_blank" rel="external">http://www.cnblogs.com/TTyb/p/7169148.html</a></p>
<p>让我们再深入一些！<br>如何添加唯一的一列，对！就是类似主键的那样<br>两种方式：<br>第一种 使用zipWithUniqueId获取id 并重建 DataFrame.<br>        import spark.implicits._<br>        import org.apache.spark.sql.Row<br>        import org.apache.spark.sql.types.{StructType, StructField, LongType}</p>
<pre><code>val df =Seq((&quot;a&quot;, -1.0), (&quot;b&quot;, -2.0), (&quot;c&quot;, -3.0)).toDF(&quot;foo&quot;, &quot;bar&quot;)
// 获取df 的表头
val s = df.schema

// 将原表转换成带有rdd,
//再转换成带有id的rdd,
//再展开成Seq方便转化成 Dataframe
val rows = df.rdd.zipWithUniqueId.map{case (r: Row, id: Long) =&gt; Row.fromSeq(id +: r.toSeq)}

// 再由 row 根据原表头进行转换
val dfWithPK = spark.createDataFrame( rows, StructType(StructField(&quot;id&quot;, LongType, false) +: s.fields))
dfWithPK.show
</code></pre><p>+—+—+—-+<br>| id|foo| bar|<br>+—+—+—-+<br>|  0|  a|-1.0|<br>|  1|  b|-2.0|<br>|  2|  c|-3.0|<br>+—+—+—-+</p>
<p>以上可以实现自增列。<br>第二种：直接使用spark 自己的api,monotonicallyIncreasingId<br>        val df =Seq((“a”, -1.0), (“b”, -2.0), (“c”, -3.0)).toDF(“foo”, “bar”)<br>        import org.apache.spark.sql.functions.monotonically_increasing_id<br>        df.withColumn(“id”, monotonically_increasing_id).show()</p>
<p>+—+—-+—+<br>|foo| bar| id|<br>+—+—-+—+<br>|  a|-1.0|  0|<br>|  b|-2.0|  1|<br>|  c|-3.0|  2|<br>+—+—-+—+<br>注意这里也是自增列</p>
<p>还有第三种：使用createDateFrame<br>        val input =Seq((“a”, 100.0), (“b”, -2.0), (“c”, -3.0)).toDF(“foo”, “bar”)<br>        var critValueR = 0<br>        var critValueL = -10</p>
<pre><code>val trdd = input.select(&quot;bar&quot;).rdd.map(x=&gt;{
    if (x.get(0).toString().toDouble &gt; critValueR || x.get(0).toString().toDouble &lt; critValueL)
        Row(x.get(0).toString().toDouble,&quot;F&quot;)
    else Row(x.get(0).toString().toDouble,&quot;T&quot;)
})
val schema = input.select(&quot;bar&quot;).schema.add(&quot;flag&quot;, StringType, true)
val sample3 = spark.createDataFrame(trdd, schema).distinct().withColumnRenamed(&quot;flag&quot;, &quot;idx&quot;)
sample3.show
</code></pre><p>+—–+—+<br>|  bar|idx|<br>+—–+—+<br>| -3.0|  T|<br>|100.0|  F|<br>| -2.0|  T|<br>+—–+—+</p>
<p>觉得比较直观且开放性大的还是实用udf。</p>
<p>参考资料：<br><a href="https://www.jianshu.com/p/7e6e406dd15b" target="_blank" rel="external">https://www.jianshu.com/p/7e6e406dd15b</a></p>
<p>。。。。。。。。。。你以为这就完了？。。。。。。<br>那我还写个毛线。。。。。。<br>后来又发现了对列的拆分和合并</p>
<p>1、如何合并列：<br>        val df =Seq((“Ming”,20,”15552211521”), (“hong”,19,”13287994007”), (“zhi”,21,”15552211523”)).toDF(“name”,”age”, “phone”)<br>        val separator = “,”<br>        df.map(<em>.toSeq.foldLeft(“”)(</em> + separator + _).substring(1)).show()<br>+——————-+<br>|              value|<br>+——————-+<br>|Ming,20,15552211521|<br>|hong,19,13287994007|<br>| zhi,21,15552211523|<br>+——————-+<br>也可以使用concat<em>ws:<br>        import org.apache.spark.sql.functions.</em><br>        df.select(concat_ws(separator, $”name”, $”age”, $”phone”).cast(StringType).as(“value”)).show()</p>
<p>结果一样<br>第三种实用udf！<br>       def mergeCols(row: Row): String = {<br>            row.toSeq.foldLeft(“”)(<em> + separator + </em>).substring(1)<br>        }</p>
<pre><code>val mergeColsUDF = udf(mergeCols _)
df.select(mergeColsUDF(struct($&quot;name&quot;, $&quot;age&quot;, $&quot;phone&quot;)).as(&quot;value&quot;)).show()
</code></pre><p>结果也是同上</p>
<p>2、如何拆分列呢？<br>a、实用split<br>    val df =Seq((“Ming,20,15552211521”), (“hong,19,13287994007”), (“zhi,21,15552211523”)).toDF(“value”)<br>    val separator = “,”<br>    lazy val first = df.first()</p>
<pre><code>val numAttrs = first.toString().split(separator).length
val attrs = Array.tabulate(numAttrs)(n =&gt; &quot;col_&quot; + n)
//按指定分隔符拆分value列，生成splitCols列
var newDF = df.withColumn(&quot;splitCols&quot;, split($&quot;value&quot;, separator))
attrs.zipWithIndex.foreach(x =&gt; {
  newDF = newDF.withColumn(x._1, $&quot;splitCols&quot;.getItem(x._2))
})
newDF.show()
</code></pre><p>+——————-+——————–+—–+—–+———–+<br>|              value|           splitCols|col_0|col_1|      col_2|<br>+——————-+——————–+—–+—–+———–+<br>|Ming,20,15552211521|[Ming, 20, 155522…| Ming|   20|15552211521|<br>|hong,19,13287994007|[hong, 19, 132879…| hong|   19|13287994007|<br>| zhi,21,15552211523|[zhi, 21, 1555221…|  zhi|   21|15552211523|<br>+——————-+——————–+—–+—–+———–+</p>
<p>b、实用udf：<br>        import org.apache.spark.ml.attribute.{Attribute, NumericAttribute}</p>
<pre><code>val df =Seq((&quot;Ming,20,15552211521&quot;), (&quot;hong,19,13287994007&quot;), (&quot;zhi,21,15552211523&quot;)).toDF(&quot;value&quot;)
val separator = &quot;,&quot;
lazy val first = df.first()
val attributes: Array[Attribute] = {
    val numAttrs = first.toString().split(separator).length
    //生成attributes
    Array.tabulate(numAttrs)(i =&gt; NumericAttribute.defaultAttr.withName(&quot;value&quot; + &quot;_&quot; + i))
}
//创建多列数据
val fieldCols = attributes.zipWithIndex.map(x =&gt; {
    val assembleFunc = udf {
        str: String =&gt;
            str.split(separator)(x._2)
    }
    assembleFunc(df(&quot;value&quot;).cast(StringType)).as(x._1.name.get, x._1.toMetadata())
})
//合并数据
df.select(col(&quot;*&quot;) +: fieldCols: _*).show()
</code></pre><p>+——————-+——————–+—–+—–+———–+<br>|              value|           splitCols|col_0|col_1|      col_2|<br>+——————-+——————–+—–+—–+———–+<br>|Ming,20,15552211521|[Ming, 20, 155522…| Ming|   20|15552211521|<br>|hong,19,13287994007|[hong, 19, 132879…| hong|   19|13287994007|<br>| zhi,21,15552211523|[zhi, 21, 1555221…|  zhi|   21|15552211523|<br>+——————-+——————–+—–+—–+———–+<br>参考资料：<br><a href="http://www.cnblogs.com/itboys/p/9813934.html" target="_blank" rel="external">http://www.cnblogs.com/itboys/p/9813934.html</a></p>
<p>综上所述，udf还是很牛逼的。</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2018/12/17/how-to-add-col-spark-dataframe/" data-id="cjpwsfigb000497797avd3i34" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>


  
    <article id="post-mysql-HOUR-OF-DAY-2-3" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2018/12/17/mysql-HOUR-OF-DAY-2-3/" class="article-date">
  <time datetime="2018-12-17T15:19:48.000Z" itemprop="datePublished">2018-12-17</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2018/12/17/mysql-HOUR-OF-DAY-2-3/">mysql-HOUR_OF_DAY-2-3</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>今天开发spark应用遇到了一个问题：<br>java.lang.IllegalArgumentException: HOUR_OF_DAY: 2 -&gt; 3</p>
<p>在数据库配置文件中加上 &amp;serverTimezone=Asia/Shanghai 即可~</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2018/12/17/mysql-HOUR-OF-DAY-2-3/" data-id="cjpwsfigl00089779qiiajg1e" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>


  
    <article id="post-spark-sql-count-as-issue" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2018/12/13/spark-sql-count-as-issue/" class="article-date">
  <time datetime="2018-12-13T15:25:33.000Z" itemprop="datePublished">2018-12-13</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2018/12/13/spark-sql-count-as-issue/">spark-sql-count-as-issue</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>今天遇到了一个奇怪的问题，就是用as的时候，如果别名全部为数字，那么会报错，具体说明如下：<br>准备的数据如下：<br><img src="/2018/12/13/spark-sql-count-as-issue/4.png" alt="data" title="data"><br>如果设置成2或者2-2，那么会报如下错误。如果是’2’效果一样<br><img src="/2018/12/13/spark-sql-count-as-issue/2.png" alt="wrong" title="wrong"><br><img src="/2018/12/13/spark-sql-count-as-issue/3.png" alt="exception" title="exception"><br>如果设置成2_2，则查询正常<br><img src="/2018/12/13/spark-sql-count-as-issue/1.png" alt="right" title="right"></p>
<p>用的spark和spark sql为2.3.1，pom如下：</p>
<pre><code>&lt;dependency&gt;
    &lt;groupId&gt;org.apache.spark&lt;/groupId&gt;
    &lt;artifactId&gt;spark-core_2.11&lt;/artifactId&gt;
    &lt;version&gt;2.3.1&lt;/version&gt;
    &lt;scope&gt;${spark.scope}&lt;/scope&gt;
&lt;/dependency&gt;

&lt;dependency&gt;
    &lt;groupId&gt;org.apache.spark&lt;/groupId&gt;
    &lt;artifactId&gt;spark-sql_2.11&lt;/artifactId&gt;
    &lt;version&gt;2.3.1&lt;/version&gt;
    &lt;scope&gt;${spark.scope}&lt;/scope&gt;
&lt;/dependency&gt;
</code></pre><p>暂时先用’_’，还没有具体的解决办法</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2018/12/13/spark-sql-count-as-issue/" data-id="cjpwsfinc000e9779fimrqojp" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>


  
    <article id="post-spark-classloader-cannot-find-classdef-in-the-jar" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2018/12/13/spark-classloader-cannot-find-classdef-in-the-jar/" class="article-date">
  <time datetime="2018-12-13T14:47:35.000Z" itemprop="datePublished">2018-12-13</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2018/12/13/spark-classloader-cannot-find-classdef-in-the-jar/">spark找不到jar里有定义的class</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>今天遇到了一个奇怪的问题：明明打包的时候把class文件打进去了，但是却报classnotfound！<br>相同的程序如果在本地以local形式运行是可以的，但是如果submit到远程就会报classnotfound。<br>开始猜测是classloader的问题。<br>终于在stackoverflow上找到了类似的问题。<br>加上参数–driver-class-path 将jar包自己放到这个参数后面，问题解决。<br>连接如下：<br><a href="https://stackoverflow.com/questions/30426245/apache-spark-classloader-cannot-find-classdef-in-the-jar" target="_blank" rel="external">https://stackoverflow.com/questions/30426245/apache-spark-classloader-cannot-find-classdef-in-the-jar</a></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2018/12/13/spark-classloader-cannot-find-classdef-in-the-jar/" data-id="cjpwsfin6000b9779jwwfw3hh" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/spark-classload-ClassNotFound/">spark classload ClassNotFound</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="tensorflow-1-tensorflow介绍" class="article article-type-tensorflow-1" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2018/10/30/tensorflow介绍/" class="article-date">
  <time datetime="2018-10-30T14:11:59.000Z" itemprop="datePublished">2018-10-30</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2018/10/30/tensorflow介绍/">图像识别与tensorflow</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>1、图像识别基本流程：<br>a、输入:<br>    我们的给定K个类别的N张图片，作为计算机学习的训练集<br>b、学习:<br>    让计算机逐张图片地『观察』和『学习』<br>c、 评估:<br>    就像我们上学学了东西要考试检测一样，我们也得考 考计算机学得如何，于是我们给定一些计算机不知道 类别的图片让它判别，然后再比对我们已知的正确答案。<br>举个例子：<br>教小孩认识水果。</p>
<p><div style="height: 20%;width: 20%;" float="left"><img src="/2018/10/30/tensorflow介绍/boy.jpg" alt="xianxingfenlei" title="xianxingfenlei"></div><br>  <div style="height: 20%;width: 20%;" float="right"><img src="/2018/10/30/tensorflow介绍/juzi.jpg" alt="xianxingfenlei" title="xianxingfenlei"></div><br>看！这是橘子！<br>再看看这个还认识不？</p>
<p><div style="height: 20%;width: 20%;" float="right"><img src="/2018/10/30/tensorflow介绍/chengzi.jpg" alt="xianxingfenlei" title="xianxingfenlei"></div><br>2、tensorflow入门例子：<br>tensorflow是google的亲儿子。<br>从名字可以看出tensorflow=tensor+flow。就是张量加图<br>tensorflow的输入就是张量，以mnist数据集为例：<br>mnist每个图片是28*28像素，将图片按像素展开为数组得到784纬向量空间中的点。<br>而mnist训练集中更有55000张图，就是一个[55000，784]的张量。<br>第一维是图片索引，第二维是像素索引。<br><img src="/2018/10/30/tensorflow介绍/mnist.jpg" alt="xianxingfen" title="xianxingfen"><br><img src="/2018/10/30/tensorflow介绍/tens.jpg" alt="xianxingfen" title="xianxingfen"></p>
<img src="/2018/10/30/tensorflow介绍/demo1.jpg" alt="xianxingfen" title="xianxingfen">
<img src="/2018/10/30/tensorflow介绍/1.jpg" alt="xianxingfen" title="xianxingfen">
<p>tensorflow的处理流程可以这样</p>
<p>从线性分类器开始说起<br>2、线性分类器：<br><img src="/2018/10/30/tensorflow介绍/xianxingfenleiqi.jpg" alt="xianxingfenlei" title="xianxingfenlei"><br>具体计算过程：<br><img src="/2018/10/30/tensorflow介绍/xianxingfenlei1.jpg" alt="xianxingfenlei1" title="xianxingfenlei1"><br>W的每一行可以看做是其中一个类别的模板<br>每类得分，实际上是像素点和模板匹配度<br>模板匹配的方式是内积计算</p>
<p>4、激励函数：<br>常见激励函数：<br>4.1.sigmoid函数<br>S(x)=1/(1+exp(-x))<br>导数为：S(x)*(1-S(x))。<br>4.2 tanh函数<br><img src="/2018/10/30/tensorflow介绍/tanhx.png" alt="softmax" title="softmax"><br>导数为<br><img src="/2018/10/30/tensorflow介绍/tanhx1.png" alt="softmax" title="softmax"><br>4.3 elu函数：<br><img src="/2018/10/30/tensorflow介绍/elu.png" alt="softmax" title="softmax"><br>4.4 softplus函数：<br>f(x)=ln(1+ex)<br>导数为：<br>f′(x)=ex1+ex=11+e−x<br>4.5 softsign函数：<br><img src="/2018/10/30/tensorflow介绍/softsign.png" alt="softmax" title="softmax"><br>4.6 relu函数<br>f(x) = max(0, x)<br>4.7 relu6函数<br>f(x) = max(0,6,x)<br><img src="/2018/10/30/tensorflow介绍/1204043-20170722004859120-1107549120.png" alt="link_fun" title="link_fun"><br>如何选择：<br>神经网络层次很少，可以随意尝试记录函数组合，<br>卷积神经网络用relu<br>循环神经网络推荐relu或者tanh</p>
<p>5、损失函数：<br>损失函数是评价我们训练模型好坏的关键函数，顾名思义就是损失函数的值越小越好。<br>整个训练学习的过程就是最小化损失函数的过程<br>交叉熵:<br><img src="/2018/10/30/tensorflow介绍/cross.jpg" alt="softmax" title="softmax"><br>一般用来求目标与预测值之间的差距。y 是我们预测的概率分布,y′ 是实际的分布<br>激励函数计算方式：<br>以softmax为例<br><img src="/2018/10/30/tensorflow介绍/softmax1.jpg" alt="softmax" title="softmax"><br><img src="/2018/10/30/tensorflow介绍/softmax2.jpg" alt="softmax1" title="softmax1"></p>
<p>4、梯度下降：<br>梯度下降的过程是将损失函数最小化的过程。<br>梯度的负方向是局部下降最快的方向<br><img src="/2018/10/30/tensorflow介绍/gd5.png" alt="softmax1" title="softmax1"><br>tensorflow 优化器：<br><img src="/2018/10/30/tensorflow介绍/youhuaqi.png" alt="softmax1" title="softmax1"><br>梯度下降实际上是tensorflow中最基本的一种优化器。一共有9种。简单总结一下。<br>如果数据是稀疏的，就用自适用方法，即 Adagrad, Adadelta, RMSprop, Adam。</p>
<p>RMSprop, Adadelta, Adam 在很多情况下的效果是相似的。</p>
<p>Adam 就是在 RMSprop 的基础上加了 bias-correction 和 momentum，</p>
<p>随着梯度变的稀疏，Adam 比 RMSprop 效果会好。</p>
<p>整体来讲，Adam 是最好的选择。</p>
<p>5、人工神经网络：<br><img src="/2018/10/30/tensorflow介绍/shenjingwangluo.png" alt="神经网络" title="神经网络"><br>神经网络由神经元构成，每一个神经元都有各自的权重，代表一种特征。<br>神经元的“与或”运算达到了非线性切分目的。<br>“正向传播”求损失，“反向传播”回传误差<br>根据误差信号修正每层的权重<br>神经元：<br><img src="/2018/10/30/tensorflow介绍/shenjingyuan.png" alt="神经元" title="神经元"><br>6、卷积神经网络<br>卷积神经网络是神经网络的一种，他解决了图片处理的一个大问题就是特征向量提取的问题。<br><img src="/2018/10/30/tensorflow介绍/juanji1.png" alt="神经元" title="神经元"><br>卷积神经网络层级分为:输入层、卷积层、激励层、池化层和全联接层<br>卷积层(conv)：<br>固定每个神经元连接权重，可以看做模板 每个神经元只关注一个特性<br>参数共享机制:假设每个神经元连接数据窗的权重是固定的<br>一组固定的权重和不同窗口内数据做内积: 卷积<br><img src="/2018/10/30/tensorflow介绍/juanji2.png" alt="卷积神经网络" title="卷积神经网络"><br>激励层(RELU):<br>把卷积层输出结果做非线性映射<br>卷积神经网络主要用RELU做激励函数<br><img src="/2018/10/30/tensorflow介绍/jili.png" alt="卷积神经网络" title="卷积神经网络"><br>池化层pooling:<br>夹在连续的卷积层中间<br>压缩数据和参数的量，减小过拟合<br>这个与数据量的大小也有关系，过多的池化会把数据压没，不做池化也有可能造成过拟合。<br><img src="/2018/10/30/tensorflow介绍/pooling.png" alt="卷积神经网络" title="卷积神经网络"><br>卷积神经网络的构成<br><img src="/2018/10/30/tensorflow介绍/cnn.png" alt="卷积神经网络" title="卷积神经网络"><br>CNN的优点：<br>共享卷积核，对高维数据处理无压力<br>无需手动选取特征，训练好权重，即得特征<br>分类效果好<br>缺点：<br>需要调参，需要大样本量，训练最好要GPU<br>8、tensorflow卷积神经网络实例<br><img src="/2018/10/30/tensorflow介绍/demo1副本.png" alt="卷积神经网络" title="卷积神经网络"></p>
<img src="/2018/10/30/tensorflow介绍/cnnlogic.png" alt="卷积神经网络" title="卷积神经网络">
<p>9、opencv与tensorflow</p>
<p>参考资料：<br>softmax函数详解 <a href="https://www.cnblogs.com/alexanderkun/p/8098781.html" target="_blank" rel="external">https://www.cnblogs.com/alexanderkun/p/8098781.html</a><br>交叉熵  <a href="https://blog.csdn.net/tsyccnh/article/details/79163834" target="_blank" rel="external">https://blog.csdn.net/tsyccnh/article/details/79163834</a><br>优化器选择  <a href="https://blog.csdn.net/tiankongtiankong01/article/details/80570729" target="_blank" rel="external">https://blog.csdn.net/tiankongtiankong01/article/details/80570729</a><br>tensorflow建议教程 <a href="https://morvanzhou.github.io/tutorials/machine-learning/tensorflow/1-1-A-ANN-and-NN/" target="_blank" rel="external">https://morvanzhou.github.io/tutorials/machine-learning/tensorflow/1-1-A-ANN-and-NN/</a></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2018/10/30/tensorflow介绍/" data-id="cjpwsfj2h000i9779heo4uswv" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/tensroflow/">tensroflow</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="openstack-instroduce-openstack简介" class="article article-type-openstack-instroduce" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2018/09/05/openstack简介/" class="article-date">
  <time datetime="2018-09-05T14:08:38.000Z" itemprop="datePublished">2018-09-05</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2018/09/05/openstack简介/">openstack简介</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <img src="/2018/09/05/openstack简介/introduce.png" alt="introduce" title="introduce">
<p>Openstack由美国国家航空航天局和Rackspace公司合作研发，目前已经占据市场主导地位。它致力于提供规模化、灵活扩展易部署且功能丰富的全开源模式平台，协助运营商、企业、ISP/CP、科研机构等搭建并实现满足自身需求的公共云和私有云服务，有力的推动了云计算的创新发展。<br>  Openstack的主要组件包括：计算（nova）、对象存储（swift）、镜像（glance）、认证（keystone）<br>、web控制台（horizon）以及网络（neutron）。其中Horizon提供了一个Web前端到Openstack其他服务的界面；nova组件主要负责管理虚拟机，存储和检索虚拟磁盘（Image）和Image上相关的元数据（Glance），并提供管理和维护系统镜像服务；网络组件（neutron）提供构建与管理虚拟网路的功能，他将网络连接作为服务；块存储服务服务组件（cinder）提供存储功能；Image（Glance）在对象存储（swift）上能够完成虚拟磁盘文件的存储，该组件提供pb级别、安全可靠的对象存储服务；所有的服务均需要利用keystone进行身份认证。</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2018/09/05/openstack简介/" data-id="cjpwsfihq000a9779srv1cgz9" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>


  
    <article id="post-openstack-neutron" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2017/05/01/openstack-neutron/" class="article-date">
  <time datetime="2017-05-01T08:36:25.000Z" itemprop="datePublished">2017-05-01</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2017/05/01/openstack-neutron/">openstack_neutron</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>本文主要着重介绍openstack neutron vxlan网络模式下的原理和体系架构。<br>名词解释：<br>东西向网络流量：vm内网流量<br>南北向网络流量：vm通过浮动ip与外网产生的流量<br>veth设备：成对出现，往往用于跨namespace通信，如下图所示：<br><img src="/2017/05/01/openstack-neutron/veth.png" alt="veth device" title="veth device"><br>tap设备：虚拟网络接口<br>fdb表：作用于数据链路层（二层网络），mac+vxlan和port的对应关系。<br>arp协议和arp表：<br>Arp协议是三层协议，主机发送网络包的时候会先广播到网络上的所有主机，如果接收到返回，就放到arp表中缓存。<br>Arp表存放ip和mac地址的对应关系<br>vxlan:Vxlan是 一种overlay网络，是一种隧道技术。<br><img src="/2017/05/01/openstack-neutron/vxlan.png" alt="vxlan network" title="vxlan network"></p>
<p>openstack vxlan 网络模式简介</p>
<p>东西向流量(虚拟机之间的网络流向)<br><img src="/2017/05/01/openstack-neutron/eastWest.png" alt="eastwest" title="eastwest"></p>
<p>东西流量示例讲解：<br>计算节点网络：<br><img src="/2017/05/01/openstack-neutron/vm_network.png" alt="eastwest" title="eastwest"></p>
<img src="/2017/05/01/openstack-neutron/port.png" alt="eastwest" title="eastwest">
<p>网桥的命名规则是取networks的id前12位<br><img src="/2017/05/01/openstack-neutron/portid.png" alt="eastwest" title="eastwest"><br>tap设备的命名规则是取portid的前12位<br><img src="/2017/05/01/openstack-neutron/portDB.png" alt="port" title="port"><br>给一个安全组添加ssh（22端口）访问权限后的计算节点iptables：</p>
<img src="/2017/05/01/openstack-neutron/5.png" alt="iptables" title="iptables">
<p>网络节点：<br><img src="/2017/05/01/openstack-neutron/4.png" alt="iptables" title="iptables"></p>
<img src="/2017/05/01/openstack-neutron/2.png" alt="iptables" title="iptables">
<img src="/2017/05/01/openstack-neutron/3.png" alt="iptables" title="iptables">
<img src="/2017/05/01/openstack-neutron/1.png" alt="iptables" title="iptables">
<p>另外负载均衡器也在网络节点上。<br>南北向流量流向（虚拟机与外网之间的网络流向）</p>
<img src="/2017/05/01/openstack-neutron/nanbei.png" alt="northsouth" title="northsouth">
<img src="/2017/05/01/openstack-neutron/6.png" alt="iptables" title="iptables">
<img src="/2017/05/01/openstack-neutron/7.png" alt="iptables" title="iptables">
<img src="/2017/05/01/openstack-neutron/8.png" alt="iptables" title="iptables">

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2017/05/01/openstack-neutron/" data-id="cjpwsfige000597797karsyai" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/openstack-neutron/">openstack neutron</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-spring-cloud" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2017/05/01/spring-cloud/" class="article-date">
  <time datetime="2017-05-01T08:09:13.000Z" itemprop="datePublished">2017-05-01</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2017/05/01/spring-cloud/">spring_cloud</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>spring cloud所有项目总结如下：<br><img src="/2017/05/01/spring-cloud/spring_cloud.png" alt="This is an example image" title="This is an example image"></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2017/05/01/spring-cloud/" data-id="cjpwsfinf000f97793esrdmf8" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/spring/">spring</a></li></ul>

    </footer>
  </div>
  
</article>


  


  <nav id="page-nav">
    <span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><a class="extend next" rel="next" href="/page/2/">__('next') &raquo;</a>
  </nav>
</section>
        
          <aside id="sidebar">
  
    

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tags</h3>
    <div class="widget">
      <ul class="tag-list"><li class="tag-list-item"><a class="tag-list-link" href="/tags/openstack-neutron/">openstack neutron</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/spark-classload-ClassNotFound/">spark classload ClassNotFound</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/spring/">spring</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/tensroflow/">tensroflow</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tag Cloud</h3>
    <div class="widget tagcloud">
      <a href="/tags/openstack-neutron/" style="font-size: 10px;">openstack neutron</a> <a href="/tags/spark-classload-ClassNotFound/" style="font-size: 10px;">spark classload ClassNotFound</a> <a href="/tags/spring/" style="font-size: 20px;">spring</a> <a href="/tags/tensroflow/" style="font-size: 10px;">tensroflow</a>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/12/">December 2018</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/10/">October 2018</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/09/">September 2018</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/05/">May 2017</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/12/">December 2016</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/11/">November 2016</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2018/12/20/hello-world/">如何搭建新版本hexo</a>
          </li>
        
          <li>
            <a href="/2018/12/20/sqoop-huizong/">sqoop问题汇总</a>
          </li>
        
          <li>
            <a href="/2018/12/17/how-to-add-col-spark-dataframe/">如何给spark表添加一列</a>
          </li>
        
          <li>
            <a href="/2018/12/17/mysql-HOUR-OF-DAY-2-3/">mysql-HOUR_OF_DAY-2-3</a>
          </li>
        
          <li>
            <a href="/2018/12/13/spark-sql-count-as-issue/">spark-sql-count-as-issue</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2018 wfliu<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>
    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    

<script src="//ajax.googleapis.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>


  <link rel="stylesheet" href="/fancybox/jquery.fancybox.css">
  <script src="/fancybox/jquery.fancybox.pack.js"></script>


<script src="/js/script.js"></script>

  </div>
</body>
</html>